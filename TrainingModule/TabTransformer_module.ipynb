{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# TabTransformer",
   "id": "63bee39ae7f9f0d9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Setting device and seed",
   "id": "faeeb4159ba17af7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T11:46:32.235468Z",
     "start_time": "2025-01-18T11:46:25.993232Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import copy\n",
    "import itertools\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold, PredefinedSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight, compute_sample_weight"
   ],
   "id": "aad8f447c0f546f8",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T11:46:32.278Z",
     "start_time": "2025-01-18T11:46:32.244450Z"
    }
   },
   "cell_type": "code",
   "source": [
    "SEED = 42\n",
    "\n",
    "\n",
    "def fix_random(seed: int) -> None:\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True  # slower\n",
    "\n",
    "\n",
    "fix_random(SEED)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "# device = torch.device('cpu')\n",
    "\n",
    "print('Using device:', device)\n"
   ],
   "id": "df74ef3a7896fcc1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model definition",
   "id": "5a3edb68937bc46"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T11:46:32.543447Z",
     "start_time": "2025-01-18T11:46:32.531775Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class TabTransformer(nn.Module):\n",
    "    def __init__(self, cat_dims, num_numerical, num_classes, dim_embedding=8, num_heads=2, num_layers=2, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            cat_dims: List of integers, dove ogni elemento rappresenta i valori unici di una colonna categoriale.\n",
    "            num_numerical: Numero di caratteristiche numeriche.\n",
    "            num_classes: Numero di classi per output.\n",
    "            dim_embedding: Dimensione degli embeddings.\n",
    "            num_heads: Numero di \"head\" nel Multi-Head Attention.\n",
    "            num_layers: Numero di livelli Transformer.\n",
    "            dropout: Dropout per prevenire overfitting.\n",
    "        \"\"\"\n",
    "        super(TabTransformer, self).__init__()\n",
    "\n",
    "        # Embeddings per features categoriali\n",
    "        self.embeddings = nn.ModuleList([\n",
    "            nn.Embedding(cat_dim, dim_embedding) for cat_dim in cat_dims\n",
    "        ])\n",
    "\n",
    "        # Layer per le features numeriche\n",
    "        self.numerical_norm = nn.LayerNorm(num_numerical) if num_numerical > 0 else None\n",
    "\n",
    "        # Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=dim_embedding,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=dim_embedding * 4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Classificatore finale\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(len(cat_dims) * dim_embedding + (num_numerical if num_numerical > 0 else 0), 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_cat, x_num):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x_cat: Tensore (batch_size, num_categorical_features), indici per features categoriali.\n",
    "            x_num: Tensore (batch_size, num_numerical_features), valori numerici.\n",
    "        Returns:\n",
    "            Logits (batch_size, num_classes).\n",
    "        \"\"\"\n",
    "        # Embedding per features categoriali\n",
    "        x_cat = x_cat.long()\n",
    "        cat_embeddings = [emb(x_cat[:, i]) for i, emb in enumerate(self.embeddings)]\n",
    "        cat_embeddings = torch.stack(cat_embeddings, dim=1)  # (batch_size, num_categorical_features, dim_embedding)\n",
    "\n",
    "        # Passa attraverso il Transformer\n",
    "        transformed_cat = self.transformer(cat_embeddings)  # (batch_size, num_categorical_features, dim_embedding)\n",
    "        transformed_cat = transformed_cat.view(transformed_cat.size(0), -1)  # Flatten per concatenare\n",
    "\n",
    "        # Normalizzazione delle features numeriche\n",
    "        if x_num is not None and self.numerical_norm is not None:\n",
    "            x_num = self.numerical_norm(x_num)\n",
    "\n",
    "        # Concatenazione\n",
    "        if x_num is not None:\n",
    "            x = torch.cat([transformed_cat, x_num], dim=1)\n",
    "        else:\n",
    "            x = transformed_cat\n",
    "\n",
    "        # Classificatore\n",
    "        logits = self.classifier(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class PyTorchTabTransformer:\n",
    "    def __init__(self, model, cat_idx, num_idx, device='cpu'):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.model.to(self.device)\n",
    "        self.cat_idx = cat_idx\n",
    "        self.num_idx = num_idx\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Esegue le previsioni sul modello PyTorch.\n",
    "        \"\"\"\n",
    "        self.model.eval()  # Modalità di valutazione\n",
    "        with torch.no_grad():\n",
    "            # Controlla se X è un array numpy e convertilo in un tensore PyTorch\n",
    "            if isinstance(X, np.ndarray):\n",
    "                X = torch.tensor(X, dtype=torch.float32).to(self.device)\n",
    "\n",
    "            # Supponi che X sia diviso in categoriale e numerico\n",
    "            y_pred = self.model(X[:, self.cat_idx].long(),\n",
    "                                X[:, self.num_idx])\n",
    "            return torch.argmax(y_pred, dim=1).cpu().numpy()"
   ],
   "id": "3d300e891c0e88e8",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training and test utilities",
   "id": "40fedc19aa807a13"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T11:46:32.568303Z",
     "start_time": "2025-01-18T11:46:32.554976Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import balanced_accuracy_score, f1_score\n",
    "import time\n",
    "\n",
    "\n",
    "def train_model(model, criterion, optimizer, epochs, data_loader, val_loader, device, scheduler, patience):\n",
    "    n_iter = 0\n",
    "\n",
    "    best_model = None\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_since_last_improvement = 0\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    loss_history = []\n",
    "    val_loss_history = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "\n",
    "        start_epoch = time.time()\n",
    "\n",
    "        loss_train = 0\n",
    "\n",
    "        for x_cat, x_num, targets in data_loader:\n",
    "            # print(f'Epoch [{epoch}/{epochs}] - {time.time() - start_epoch:.2f} seconds - Train Loss: {loss_train:.6f}', end='\\r')\n",
    "            x_cat, x_num, targets = x_cat.to(device), x_num.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x_cat, x_num)  # Passa entrambe le componenti\n",
    "            loss = criterion(outputs, targets.long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            n_iter += 1\n",
    "            loss_train += loss.item()\n",
    "\n",
    "        scheduler.step()\n",
    "        loss_train /= len(data_loader)\n",
    "\n",
    "        # Compute Val Loss\n",
    "        val_loss, y_pred, y_true = test_model(model, criterion, val_loader)\n",
    "        # y_true, y_pred, y_pred_probs = test_model(model, val_loader, device)\n",
    "        # val_loss = criterion(y_pred_probs, y_true)\n",
    "\n",
    "\n",
    "        loss_history.append(loss_train)\n",
    "        val_loss_history.append(val_loss)\n",
    "\n",
    "        # Early stopping with threshold\n",
    "        threshold = 0.01\n",
    "        if val_loss < best_val_loss - threshold:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model, f\"{filepath}/models/model_best_tf.save\")\n",
    "            with open(f\"{filepath}/models/model_best_tf_custom.save\", \"wb\") as f:\n",
    "                wrapped_model = PyTorchTabTransformer(model, cat_idxs, num_idxs, device)\n",
    "                pickle.dump(wrapped_model, f)\n",
    "\n",
    "            epochs_since_last_improvement = 0\n",
    "        elif epochs_since_last_improvement >= patience:\n",
    "            break\n",
    "        else:\n",
    "            epochs_since_last_improvement += 1\n",
    "\n",
    "        # print('Epoch [{}/{}] - {:.2f} seconds - train_loss: {:.6f} - val_loss: {:.6f} - patience: {}'.format(epoch ,\n",
    "        #                                                                                                      epochs, time.time() - start_epoch, loss_train, val_loss, epochs_since_last_improvement), end='\\r')\n",
    "        # calculate balanced accuracy\n",
    "        balanced_accuracy = balanced_accuracy_score(\n",
    "            y_true.detach().cpu().numpy(),\n",
    "            y_pred.detach().cpu().numpy()\n",
    "        )\n",
    "        print(f'Epoch [{epoch}/{epochs}] - {time.time() - start_epoch:.2f} seconds - Train Loss: {loss_train:.6f} - Val Loss: {val_loss:.6f} - Val Balanced Accuracy: {balanced_accuracy:.6f}')\n",
    "\n",
    "    print('\\nTraining ended after {:.2f} seconds - Best val_loss: {:.6f}'.format(time.time() - start, best_val_loss))\n",
    "    best_model = torch.load(f\"{filepath}/models/model_best_tf.save\")\n",
    "    return best_model, loss_history, val_loss_history\n",
    "\n",
    "\n",
    "def test_model(model, criterion, loader):\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():  # Turn off gradient tracking\n",
    "        for x_cat, x_num, targets in loader:\n",
    "            x_cat, x_num, targets = x_cat.to(device), x_num.to(device), targets.to(device)\n",
    "            preds = model(x_cat, x_num)  # Outputs logits or probabilities\n",
    "\n",
    "            loss = criterion(preds, targets.long())\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Convert predictions to class labels\n",
    "            predicted_classes = torch.argmax(preds, dim=1)\n",
    "\n",
    "            # Accumulate predictions and targets\n",
    "            y_pred.append(predicted_classes.cpu())\n",
    "            y_true.append(targets.cpu())\n",
    "\n",
    "    # Concatenate tensors only after the loop to minimize memory usage\n",
    "    y_pred = torch.cat(y_pred, dim=0)\n",
    "    y_true = torch.cat(y_true, dim=0)\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    return avg_loss, y_pred, y_true\n",
    "\n",
    "def evaluate_model(y_test, y_pred):\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    bacc = balanced_accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "\n",
    "    perf = {\"acc\": acc, \"bacc\": bacc, \"f1\": f1}\n",
    "\n",
    "    return perf"
   ],
   "id": "12dbd74af7086467",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Define train, validation and test sets",
   "id": "19509bda22360902"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T11:46:37.249697Z",
     "start_time": "2025-01-18T11:46:32.580414Z"
    }
   },
   "cell_type": "code",
   "source": [
    "save_in_test_folder = True\n",
    "if save_in_test_folder:\n",
    "    filepath = \"../TestModule\"\n",
    "else:\n",
    "    filepath = \"..\"\n",
    "\n",
    "seed = 42\n",
    "FILENAME = \"dataset/train_dataset.csv\"\n",
    "\n",
    "#Prepare train data\n",
    "df1 = pd.read_csv(FILENAME, sep=\",\", low_memory=False)\n",
    "\n",
    "# get features names\n",
    "features = list(df1.columns)\n",
    "# features_to_remove = [\"label\", \"ts\", \"src_ip\", \"dst_ip\", \"dns_query\", \"ssl_subject\", \"ssl_issuer\", \"http_uri\", \"type\", \"http_referrer\", \"http_user_agent\"]\n",
    "features_to_remove = [\"label\", \"type\", \"ts\", \"http_referrer\"]\n",
    "features = [feature for feature in features if feature not in features_to_remove]\n",
    "df1 = df1[features + [\"type\"]]\n",
    "\n",
    "# Converte i valori in numeri, sostituendo quelli non validi con NaN\n",
    "df1[\"src_bytes\"] = pd.to_numeric(df1[\"src_bytes\"], errors='coerce')\n",
    "# Filtra le righe con NaN (valori non convertibili)\n",
    "df1 = df1.dropna(subset=[\"src_bytes\"])\n",
    "# Converte i valori rimasti in interi\n",
    "df1.loc[:, \"src_bytes\"] = df1[\"src_bytes\"].astype(int)\n",
    "\n",
    "print(\"#Righe: \" + str(df1.shape[0]) + \" #Colonne: \" + str(df1.shape[1]))\n",
    "df1 = df1.dropna()\n",
    "print(\"#Righe: \" + str(df1.shape[0]) + \" #Colonne: \" + str(df1.shape[1]))\n",
    "\n",
    "X = df1[features]\n",
    "y = df1[\"type\"]\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y)\n",
    "with open(f\"{filepath}/transformer/target_encoder.save\", \"wb\") as f:\n",
    "    pickle.dump(le, f)\n",
    "\n",
    "y = le.transform(y)\n",
    "num_classes = len(le.classes_)\n",
    "\n",
    "indices = np.arange(X.shape[0])\n",
    "train_idx, val_idx = train_test_split(indices, test_size=0.2, stratify=y, random_state=seed)\n",
    "\n",
    "# fold = np.zeros(X.shape[0])\n",
    "# fold[train_idx] = -1\n",
    "\n",
    "n_splits = 10\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "fold = np.full(len(y), -1)  # Inizializza tutto con -1 (default: train)\n",
    "\n",
    "# Assegna i fold ai campioni\n",
    "for fold_number, (_, val_idx) in enumerate(skf.split(X, y)):\n",
    "    fold[val_idx] = fold_number  # Assegna il numero del fold ai campioni di validazione\n",
    "\n",
    "ps = PredefinedSplit(fold)\n",
    "ps.get_n_splits()\n",
    "\n",
    "# for i, (train_index, test_index) in enumerate(ps.split()):\n",
    "#     print(f\"Fold {i}:\")\n",
    "#     print(f\"  Train: index={train_index}\")\n",
    "#     print(f\"  Test:  index={test_index}\")\n",
    "\n",
    "# take only x with index in val_idx\n",
    "X_val = X.iloc[val_idx]\n",
    "y_val = y[val_idx]\n",
    "X_train = X.iloc[train_idx]\n",
    "y_train = y[train_idx]"
   ],
   "id": "6141bad0ff77c3fa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Righe: 616983 #Colonne: 43\n",
      "#Righe: 616983 #Colonne: 43\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Preprocessing",
   "id": "100df547cbfe401c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T11:46:47.403450Z",
     "start_time": "2025-01-18T11:46:37.285633Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "import pickle\n",
    "\n",
    "categorical_columns = X_train.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "numeric_columns = X_train.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "# boolean_columns = X_train.select_dtypes(include=[\"bool\"]).columns.tolist()\n",
    "\n",
    "class CustomOrdinalEncoder(OrdinalEncoder):\n",
    "    def transform(self, X):\n",
    "        encoded = super().transform(X)\n",
    "        # Shift all values by +1 and replace unknown_value (-1) with 0\n",
    "        return np.where(encoded == -1, 0, encoded + 1)\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        # Handle the inverse transform to account for the +1 offset\n",
    "        X = np.where(X == 0, -1, X - 1)\n",
    "        return super().inverse_transform(X)\n",
    "\n",
    "\n",
    "ct = ColumnTransformer(\n",
    "    [\n",
    "        (\"cat\", CustomOrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1), categorical_columns),  # Trasforma le colonne categoriche\n",
    "        # (\"ordinal\", OneHotEncoder(handle_unknown='infrequent_if_exist', sparse_output=False), categorical_columns),  # Trasforma le colonne categoriche\n",
    "        (\"scale\", StandardScaler(), numeric_columns)  # Normalizza le colonne numeriche\n",
    "    ],\n",
    "    remainder=\"passthrough\"  # Mantieni le altre colonne invariate\n",
    ")\n",
    "ct.set_output(transform=\"pandas\")\n",
    "\n",
    "ct = ct.fit(X_train)\n",
    "with open(f\"{filepath}/transformer/transformer_tf.save\", \"wb\") as f:\n",
    "    pickle.dump(ct, f)\n",
    "\n",
    "# train set\n",
    "X_train = ct.transform(X_train)\n",
    "\n",
    "cat_idxs = [i for i, f in enumerate(X_train.columns) if \"cat__\" in f]\n",
    "cat_dims = [len(X_train[f].unique()) + 1 for i, f in enumerate(X_train.columns) if \"cat__\" in f]\n",
    "num_idxs = [i for i, f in enumerate(X_train.columns) if \"scale__\" in f]\n",
    "numeric_columns_number = len(num_idxs)\n",
    "\n",
    "X_train = X_train.to_numpy()\n",
    "# validation set\n",
    "X_val = ct.transform(X_val).to_numpy()\n",
    "\n",
    "# X\n",
    "X = ct.transform(X).to_numpy()"
   ],
   "id": "3417858b58cc5303",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Define weights for unbalanced classes",
   "id": "a80285e3fda23d9d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T11:46:47.596963Z",
     "start_time": "2025-01-18T11:46:47.429345Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "sample_weight = compute_sample_weight(class_weight='balanced', y=y_train)\n",
    "print(class_weights)"
   ],
   "id": "6bdb6c0ca40abad2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 4.069469865611345, 1: 0.3381003918130257, 2: 1.132545546326465, 3: 4.543735616312253, 4: 98.7172, 5: 2.9863625363020327, 6: 1.1966881637007225, 7: 63.85329883570505, 8: 0.28803534018428717, 9: 0.9751965859248429}\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create DataLoader",
   "id": "9b8d57a3f8419975"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T11:46:47.711198Z",
     "start_time": "2025-01-18T11:46:47.629542Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TabDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x_cat, x_num, y):\n",
    "        self.x_cat = x_cat\n",
    "        self.x_num = x_num\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x_cat[idx], self.x_num[idx], self.y[idx]\n",
    "\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "# filter the categorical and numerical features\n",
    "X_cat_train = X_train_tensor[:, cat_idxs]\n",
    "X_num_train = X_train_tensor[:, num_idxs]\n",
    "\n",
    "\n",
    "X_cat_val = X_val_tensor[:, cat_idxs]\n",
    "X_num_val = X_val_tensor[:, num_idxs]\n",
    "\n",
    "# X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "# y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "train_dataset = TabDataset(X_cat_train, X_num_train, y_train_tensor)\n",
    "val_dataset = TabDataset(X_cat_val, X_num_val, y_val_tensor)\n",
    "\n",
    "# val_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_val_tensor, y_val_tensor), batch_size=y_val.shape[0], shuffle=False)\n",
    "# test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_test_tensor, y_test_tensor), batch_size=y_test.shape[0], shuffle=False)"
   ],
   "id": "906c3f2f4dd19812",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Hyperparameters configuration",
   "id": "cb6d99290b7a9591"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T11:46:47.766506Z",
     "start_time": "2025-01-18T11:46:47.759189Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nums_epochs = [1000]\n",
    "batch_sizes = [1024]\n",
    "patience = [20]\n",
    "dim_embedding = [8]\n",
    "num_heads = [8]\n",
    "num_layers = [2]\n",
    "learning_rate = [0.001]\n",
    "hyperparameters = list(itertools.product(nums_epochs, batch_sizes, patience, dim_embedding, num_heads, num_layers, learning_rate))\n",
    "n_comb = len(hyperparameters)\n",
    "print(f'Number of hyperparameter combinations: {n_comb}')"
   ],
   "id": "f0b658a4c013b7d7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of hyperparameter combinations: 1\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training",
   "id": "16212b070609649a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T12:10:13.145727Z",
     "start_time": "2025-01-18T11:46:47.807691Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize the model, loss, and optimizer\n",
    "best_loss = float('inf')\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=torch.tensor(list(class_weights.values()), dtype=torch.float32).to(device))\n",
    "current_iter = 0\n",
    "for epochs, batch_size, patience_, dim_embedding_, num_heads_, num_layers_, lr in hyperparameters:\n",
    "\n",
    "    print(f'Iteration {current_iter + 1}/{n_comb} - Hyperparameters: epochs={epochs}, batch_size={batch_size}, patience={patience_}, dim_embedding={dim_embedding_}, num_heads={num_heads_}, num_layers={num_layers_}, lr={lr}')\n",
    "\n",
    "    # Modello TabTransformer\n",
    "    model = TabTransformer(cat_dims, numeric_columns_number, num_classes, dim_embedding=dim_embedding_, num_heads=num_heads_, num_layers=num_layers_).to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    # Training\n",
    "    model, loss_history, val_loss_history = train_model(\n",
    "        model, criterion, optimizer, epochs, train_loader, val_loader, device, scheduler, patience_\n",
    "    )\n",
    "\n",
    "    # Validation\n",
    "    val_loss, y_pred, y_true = test_model(model, criterion, val_loader)\n",
    "    # y_true, _, y_pred = test_model(model, val_loader, device)\n",
    "    # val_loss = criterion(y_pred, y_true)\n",
    "\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        with open(f\"{filepath}/models/tf.save\", \"wb\") as f:\n",
    "            wrapped_model = PyTorchTabTransformer(model, cat_idxs, num_idxs, device)\n",
    "            pickle.dump(wrapped_model, f)\n",
    "        # best_model = copy.deepcopy(model)\n",
    "        best_hyperparameters = f\"epochs={epochs}, batch_size={batch_size}, patience={patience_}, dim_embedding={dim_embedding_}, num_heads={num_heads_}, num_layers={num_layers_}, lr={lr}\"\n",
    "\n",
    "    print(f'Hyperparameters: epochs={epochs}, batch_size={batch_size}, patience={patience_}, dim_embedding={dim_embedding_}, num_heads={num_heads_}, num_layers={num_layers_}, lr={lr}')\n",
    "    print(f'Validation Loss: {val_loss}')\n",
    "\n",
    "    current_iter += 1"
   ],
   "id": "5e03f4bbceab0bb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/1 - Hyperparameters: epochs=1000, batch_size=1024, patience=20, dim_embedding=8, num_heads=8, num_layers=2, lr=0.001\n",
      "Epoch [0/1000] - 27.46 seconds - Train Loss: 0.465393 - Val Loss: 0.170987 - Val Balanced Accuracy: 0.942187\n",
      "Epoch [1/1000] - 27.92 seconds - Train Loss: 0.167314 - Val Loss: 0.112947 - Val Balanced Accuracy: 0.951410\n",
      "Epoch [2/1000] - 27.39 seconds - Train Loss: 0.132189 - Val Loss: 0.109431 - Val Balanced Accuracy: 0.960977\n",
      "Epoch [3/1000] - 27.05 seconds - Train Loss: 0.110198 - Val Loss: 0.136625 - Val Balanced Accuracy: 0.959941\n",
      "Epoch [4/1000] - 35.43 seconds - Train Loss: 0.102455 - Val Loss: 0.083260 - Val Balanced Accuracy: 0.966621\n",
      "Epoch [5/1000] - 29.68 seconds - Train Loss: 0.091284 - Val Loss: 0.077487 - Val Balanced Accuracy: 0.973264\n",
      "Epoch [6/1000] - 30.22 seconds - Train Loss: 0.086682 - Val Loss: 0.068761 - Val Balanced Accuracy: 0.973750\n",
      "Epoch [7/1000] - 25.72 seconds - Train Loss: 0.078860 - Val Loss: 0.062875 - Val Balanced Accuracy: 0.975266\n",
      "Epoch [8/1000] - 24.98 seconds - Train Loss: 0.076057 - Val Loss: 0.059316 - Val Balanced Accuracy: 0.976015\n",
      "Epoch [9/1000] - 25.88 seconds - Train Loss: 0.071993 - Val Loss: 0.077401 - Val Balanced Accuracy: 0.974879\n",
      "Epoch [10/1000] - 24.92 seconds - Train Loss: 0.065585 - Val Loss: 0.050633 - Val Balanced Accuracy: 0.982041\n",
      "Epoch [11/1000] - 26.05 seconds - Train Loss: 0.061352 - Val Loss: 0.060028 - Val Balanced Accuracy: 0.983587\n",
      "Epoch [12/1000] - 25.07 seconds - Train Loss: 0.057101 - Val Loss: 0.044379 - Val Balanced Accuracy: 0.986558\n",
      "Epoch [13/1000] - 26.72 seconds - Train Loss: 0.054229 - Val Loss: 0.046851 - Val Balanced Accuracy: 0.986202\n",
      "Epoch [14/1000] - 26.47 seconds - Train Loss: 0.052243 - Val Loss: 0.056866 - Val Balanced Accuracy: 0.983652\n",
      "Epoch [15/1000] - 27.51 seconds - Train Loss: 0.049591 - Val Loss: 0.043210 - Val Balanced Accuracy: 0.989725\n",
      "Epoch [16/1000] - 30.48 seconds - Train Loss: 0.046905 - Val Loss: 0.042283 - Val Balanced Accuracy: 0.987998\n",
      "Epoch [17/1000] - 25.81 seconds - Train Loss: 0.045538 - Val Loss: 0.042613 - Val Balanced Accuracy: 0.987518\n",
      "Epoch [18/1000] - 27.02 seconds - Train Loss: 0.042868 - Val Loss: 0.046394 - Val Balanced Accuracy: 0.986066\n",
      "Epoch [19/1000] - 27.64 seconds - Train Loss: 0.041285 - Val Loss: 0.039047 - Val Balanced Accuracy: 0.990048\n",
      "Epoch [20/1000] - 27.63 seconds - Train Loss: 0.039502 - Val Loss: 0.038845 - Val Balanced Accuracy: 0.990515\n",
      "Epoch [21/1000] - 32.66 seconds - Train Loss: 0.039199 - Val Loss: 0.037346 - Val Balanced Accuracy: 0.989781\n",
      "Epoch [22/1000] - 29.59 seconds - Train Loss: 0.037948 - Val Loss: 0.038463 - Val Balanced Accuracy: 0.991436\n",
      "Epoch [23/1000] - 30.29 seconds - Train Loss: 0.035560 - Val Loss: 0.031594 - Val Balanced Accuracy: 0.992412\n",
      "Epoch [24/1000] - 30.01 seconds - Train Loss: 0.035071 - Val Loss: 0.031153 - Val Balanced Accuracy: 0.990103\n",
      "Epoch [25/1000] - 26.71 seconds - Train Loss: 0.034052 - Val Loss: 0.034602 - Val Balanced Accuracy: 0.992305\n",
      "Epoch [26/1000] - 30.20 seconds - Train Loss: 0.033669 - Val Loss: 0.033096 - Val Balanced Accuracy: 0.992690\n",
      "Epoch [27/1000] - 31.29 seconds - Train Loss: 0.033030 - Val Loss: 0.034092 - Val Balanced Accuracy: 0.992666\n",
      "Epoch [28/1000] - 26.15 seconds - Train Loss: 0.031753 - Val Loss: 0.028337 - Val Balanced Accuracy: 0.991259\n",
      "Epoch [29/1000] - 27.09 seconds - Train Loss: 0.030793 - Val Loss: 0.029895 - Val Balanced Accuracy: 0.992883\n",
      "Epoch [30/1000] - 33.27 seconds - Train Loss: 0.030349 - Val Loss: 0.028599 - Val Balanced Accuracy: 0.993896\n",
      "Epoch [31/1000] - 31.57 seconds - Train Loss: 0.029821 - Val Loss: 0.031504 - Val Balanced Accuracy: 0.991814\n",
      "Epoch [32/1000] - 28.95 seconds - Train Loss: 0.029273 - Val Loss: 0.027839 - Val Balanced Accuracy: 0.993794\n",
      "Epoch [33/1000] - 29.99 seconds - Train Loss: 0.029735 - Val Loss: 0.028130 - Val Balanced Accuracy: 0.993720\n",
      "Epoch [34/1000] - 29.33 seconds - Train Loss: 0.027957 - Val Loss: 0.028219 - Val Balanced Accuracy: 0.994105\n",
      "Epoch [35/1000] - 27.94 seconds - Train Loss: 0.027932 - Val Loss: 0.026855 - Val Balanced Accuracy: 0.994064\n",
      "Epoch [36/1000] - 29.68 seconds - Train Loss: 0.027908 - Val Loss: 0.024971 - Val Balanced Accuracy: 0.994293\n",
      "Epoch [37/1000] - 32.22 seconds - Train Loss: 0.026775 - Val Loss: 0.028754 - Val Balanced Accuracy: 0.994530\n",
      "Epoch [38/1000] - 27.30 seconds - Train Loss: 0.026821 - Val Loss: 0.030548 - Val Balanced Accuracy: 0.993851\n",
      "Epoch [39/1000] - 28.26 seconds - Train Loss: 0.027879 - Val Loss: 0.028045 - Val Balanced Accuracy: 0.993768\n",
      "Epoch [40/1000] - 25.52 seconds - Train Loss: 0.025995 - Val Loss: 0.026155 - Val Balanced Accuracy: 0.994936\n",
      "Epoch [41/1000] - 26.88 seconds - Train Loss: 0.025244 - Val Loss: 0.025683 - Val Balanced Accuracy: 0.994507\n",
      "Epoch [42/1000] - 28.41 seconds - Train Loss: 0.024865 - Val Loss: 0.023771 - Val Balanced Accuracy: 0.994698\n",
      "Epoch [43/1000] - 28.57 seconds - Train Loss: 0.025107 - Val Loss: 0.026339 - Val Balanced Accuracy: 0.994447\n",
      "Epoch [44/1000] - 25.07 seconds - Train Loss: 0.024757 - Val Loss: 0.026532 - Val Balanced Accuracy: 0.994034\n",
      "Epoch [45/1000] - 25.54 seconds - Train Loss: 0.024072 - Val Loss: 0.026909 - Val Balanced Accuracy: 0.994574\n",
      "Epoch [46/1000] - 24.94 seconds - Train Loss: 0.023558 - Val Loss: 0.024893 - Val Balanced Accuracy: 0.995038\n",
      "Epoch [47/1000] - 24.58 seconds - Train Loss: 0.024013 - Val Loss: 0.025926 - Val Balanced Accuracy: 0.994021\n",
      "Epoch [48/1000] - 24.74 seconds - Train Loss: 0.023529 - Val Loss: 0.023666 - Val Balanced Accuracy: 0.994730\n",
      "\n",
      "Training ended after 1398.31 seconds - Best val_loss: 0.028337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miaob\\AppData\\Local\\Temp\\ipykernel_3804\\3668389122.py:72: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  best_model = torch.load(f\"{filepath}/models/model_best_tf.save\")\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../TestModule/model/model_tf.save'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[10], line 30\u001B[0m\n\u001B[0;32m     28\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m val_loss \u001B[38;5;241m<\u001B[39m best_loss:\n\u001B[0;32m     29\u001B[0m     best_loss \u001B[38;5;241m=\u001B[39m val_loss\n\u001B[1;32m---> 30\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mfilepath\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m/model/model_tf.save\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mwb\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[0;32m     31\u001B[0m         torch\u001B[38;5;241m.\u001B[39msave(model, f)\n\u001B[0;32m     32\u001B[0m     \u001B[38;5;66;03m# best_model = copy.deepcopy(model)\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\DataAnalyticsProject\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001B[0m, in \u001B[0;36m_modified_open\u001B[1;34m(file, *args, **kwargs)\u001B[0m\n\u001B[0;32m    317\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m file \u001B[38;5;129;01min\u001B[39;00m {\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m}:\n\u001B[0;32m    318\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    319\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIPython won\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt let you open fd=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m by default \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    320\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    321\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myou can use builtins\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m open.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    322\u001B[0m     )\n\u001B[1;32m--> 324\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mio_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '../TestModule/model/model_tf.save'"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Test",
   "id": "f6d478e41e3d5706"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "best_model = torch.load(f\"{filepath}/model/model_tf.save\")\n",
    "# test_loss, y_pred, y_true = test_model(best_model, criterion, test_loader)\n",
    "test_loss, y_pred, y_true = test_model(best_model, criterion, val_loader)\n",
    "y_pred = torch.argmax(y_pred, dim=1)\n",
    "print(f'Best hyperparameters: {best_hyperparameters}')\n",
    "print(f'Test Loss: {test_loss}')\n",
    "print(f'Test Accuracy: {accuracy_score(y_true.detach().numpy(), y_pred.detach().numpy())}')\n",
    "plt.plot(loss_history, label='train_loss')\n",
    "plt.plot(val_loss_history, label='val_loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "58c52c1ee93666c7",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
