{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# TabTransformer",
   "id": "63bee39ae7f9f0d9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Setting device and seed",
   "id": "faeeb4159ba17af7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T09:33:27.374119Z",
     "start_time": "2025-01-18T09:33:07.283463Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import copy\n",
    "import itertools\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold, PredefinedSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight, compute_sample_weight"
   ],
   "id": "aad8f447c0f546f8",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T09:33:27.524186Z",
     "start_time": "2025-01-18T09:33:27.387134Z"
    }
   },
   "cell_type": "code",
   "source": [
    "SEED = 42\n",
    "\n",
    "\n",
    "def fix_random(seed: int) -> None:\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True  # slower\n",
    "\n",
    "\n",
    "fix_random(SEED)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "# device = torch.device('cpu')\n",
    "\n",
    "print('Using device:', device)\n"
   ],
   "id": "df74ef3a7896fcc1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model definition",
   "id": "5a3edb68937bc46"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T09:33:28.085405Z",
     "start_time": "2025-01-18T09:33:28.076519Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# class TabTransformer(torch.nn.Module):\n",
    "#     def __init__(self, num_features, num_classes, dim_embedding=8, num_heads=2, num_layers=2):\n",
    "#         super(TabTransformer, self).__init__()\n",
    "#         self.embedding = torch.nn.Linear(num_features, dim_embedding)\n",
    "#         encoder_layer = torch.nn.TransformerEncoderLayer(d_model=dim_embedding, nhead=num_heads, batch_first=True)\n",
    "#         self.transformer = torch.nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "#         self.classifier = torch.nn.Linear(dim_embedding, num_classes)\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         x = self.embedding(x)\n",
    "#         x = x.unsqueeze(1)  # Adding a sequence length dimension\n",
    "#         x = self.transformer(x)\n",
    "#         x = torch.mean(x, dim=1)  # Pooling\n",
    "#         x = self.classifier(x)\n",
    "#         return x\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class TabTransformer(nn.Module):\n",
    "    def __init__(self, cat_dims, num_numerical, num_classes, dim_embedding=8, num_heads=2, num_layers=2, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            cat_dims: List of integers, dove ogni elemento rappresenta i valori unici di una colonna categoriale.\n",
    "            num_numerical: Numero di caratteristiche numeriche.\n",
    "            num_classes: Numero di classi per output.\n",
    "            dim_embedding: Dimensione degli embeddings.\n",
    "            num_heads: Numero di \"head\" nel Multi-Head Attention.\n",
    "            num_layers: Numero di livelli Transformer.\n",
    "            dropout: Dropout per prevenire overfitting.\n",
    "        \"\"\"\n",
    "        super(TabTransformer, self).__init__()\n",
    "\n",
    "        # Embeddings per features categoriali\n",
    "        self.embeddings = nn.ModuleList([\n",
    "            nn.Embedding(cat_dim, dim_embedding) for cat_dim in cat_dims\n",
    "        ])\n",
    "\n",
    "        # Layer per le features numeriche\n",
    "        self.numerical_norm = nn.LayerNorm(num_numerical) if num_numerical > 0 else None\n",
    "\n",
    "        # Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=dim_embedding,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=dim_embedding * 4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Classificatore finale\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(len(cat_dims) * dim_embedding + (num_numerical if num_numerical > 0 else 0), 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_cat, x_num):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x_cat: Tensore (batch_size, num_categorical_features), indici per features categoriali.\n",
    "            x_num: Tensore (batch_size, num_numerical_features), valori numerici.\n",
    "        Returns:\n",
    "            Logits (batch_size, num_classes).\n",
    "        \"\"\"\n",
    "        # Embedding per features categoriali\n",
    "        x_cat = x_cat.long()\n",
    "        cat_embeddings = [emb(x_cat[:, i]) for i, emb in enumerate(self.embeddings)]\n",
    "        cat_embeddings = torch.stack(cat_embeddings, dim=1)  # (batch_size, num_categorical_features, dim_embedding)\n",
    "\n",
    "        # Passa attraverso il Transformer\n",
    "        transformed_cat = self.transformer(cat_embeddings)  # (batch_size, num_categorical_features, dim_embedding)\n",
    "        transformed_cat = transformed_cat.view(transformed_cat.size(0), -1)  # Flatten per concatenare\n",
    "\n",
    "        # Normalizzazione delle features numeriche\n",
    "        if x_num is not None and self.numerical_norm is not None:\n",
    "            x_num = self.numerical_norm(x_num)\n",
    "\n",
    "        # Concatenazione\n",
    "        if x_num is not None:\n",
    "            x = torch.cat([transformed_cat, x_num], dim=1)\n",
    "        else:\n",
    "            x = transformed_cat\n",
    "\n",
    "        # Classificatore\n",
    "        logits = self.classifier(x)\n",
    "        return logits\n",
    "\n"
   ],
   "id": "3d300e891c0e88e8",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training and test utilities",
   "id": "40fedc19aa807a13"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T09:33:28.107364Z",
     "start_time": "2025-01-18T09:33:28.094034Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import balanced_accuracy_score\n",
    "import time\n",
    "\n",
    "\n",
    "def train_model(model, criterion, optimizer, epochs, data_loader, val_loader, device, scheduler, patience):\n",
    "    n_iter = 0\n",
    "\n",
    "    best_model = None\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_since_last_improvement = 0\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    loss_history = []\n",
    "    val_loss_history = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "\n",
    "        start_epoch = time.time()\n",
    "\n",
    "        loss_train = 0\n",
    "\n",
    "        for x_cat, x_num, targets in data_loader:\n",
    "            print(f'Epoch [{epoch}/{epochs}] - {time.time() - start_epoch:.2f} seconds - Train Loss: {loss_train:.6f}', end='\\r')\n",
    "            x_cat, x_num, targets = x_cat.to(device), x_num.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x_cat, x_num)  # Passa entrambe le componenti\n",
    "            loss = criterion(outputs, targets.long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            n_iter += 1\n",
    "            loss_train += loss.item()\n",
    "\n",
    "        scheduler.step()\n",
    "        loss_train /= len(data_loader)\n",
    "\n",
    "        # Compute Val Loss\n",
    "        val_loss, y_pred, y_true = test_model(model, criterion, val_loader)\n",
    "        # y_true, y_pred, y_pred_probs = test_model(model, val_loader, device)\n",
    "        # val_loss = criterion(y_pred_probs, y_true)\n",
    "\n",
    "\n",
    "        loss_history.append(loss_train)\n",
    "        val_loss_history.append(val_loss)\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            # print('Starting deep copy')\n",
    "            # best_model = copy.deepcopy(model)\n",
    "            epochs_since_last_improvement = 0\n",
    "        elif epochs_since_last_improvement >= patience:\n",
    "            break\n",
    "        else:\n",
    "            epochs_since_last_improvement += 1\n",
    "\n",
    "        # print('Epoch [{}/{}] - {:.2f} seconds - train_loss: {:.6f} - val_loss: {:.6f} - patience: {}'.format(epoch ,\n",
    "        #                                                                                                      epochs, time.time() - start_epoch, loss_train, val_loss, epochs_since_last_improvement), end='\\r')\n",
    "        # calculate balanced accuracy\n",
    "        balanced_accuracy = balanced_accuracy_score(\n",
    "            y_true.detach().cpu().numpy(),\n",
    "            y_pred.detach().cpu().numpy()\n",
    "        )\n",
    "        print(f'Epoch [{epoch}/{epochs}] - {time.time() - start_epoch:.2f} seconds - Train Loss: {loss_train:.6f} - Val Loss: {val_loss:.6f} - Val Balanced Accuracy: {balanced_accuracy:.6f}')\n",
    "\n",
    "    print('\\nTraining ended after {:.2f} seconds - Best val_loss: {:.6f}'.format(time.time() - start, best_val_loss))\n",
    "\n",
    "    return best_model, loss_history, val_loss_history\n",
    "\n",
    "\n",
    "def test_model(model, criterion, loader):\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():  # Turn off gradient tracking\n",
    "        for x_cat, x_num, targets in loader:\n",
    "            x_cat, x_num, targets = x_cat.to(device), x_num.to(device), targets.to(device)\n",
    "            preds = model(x_cat, x_num)  # Outputs logits or probabilities\n",
    "\n",
    "            loss = criterion(preds, targets.long())\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Convert predictions to class labels\n",
    "            predicted_classes = torch.argmax(preds, dim=1)\n",
    "\n",
    "            # Accumulate predictions and targets\n",
    "            y_pred.append(predicted_classes.cpu())\n",
    "            y_true.append(targets.cpu())\n",
    "\n",
    "    # Concatenate tensors only after the loop to minimize memory usage\n",
    "    y_pred = torch.cat(y_pred, dim=0)\n",
    "    y_true = torch.cat(y_true, dim=0)\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    return avg_loss, y_pred, y_true\n",
    "\n",
    "# def test_model(model, loader, device):\n",
    "#     model.eval()\n",
    "#     # y_pred = torch.tensor([], requires_grad=True).to(device)\n",
    "#     # y_true = torch.tensor([], requires_grad=True).to(device)\n",
    "#     y_pred = []\n",
    "#     y_test = []\n",
    "#\n",
    "#     total_loss = 0.0\n",
    "#\n",
    "#\n",
    "#     for x_cat, x_num, targets in loader:\n",
    "#         x_cat, x_num, targets = x_cat.to(device), x_num.to(device), targets.to(device)\n",
    "#         y_pred += model(x_cat, x_num)\n",
    "#         y_test += targets\n",
    "#\n",
    "#     y_pred = torch.stack(y_pred).squeeze()\n",
    "#     y_test = torch.stack(y_test).squeeze()\n",
    "#     # y_pred_c is the class with the highest probability\n",
    "#     y_pred_c = y_pred.argmax(dim=1, keepdim=True).squeeze()\n",
    "#\n",
    "#\n",
    "#     # loss = criterion(preds, targets.long())\n",
    "#     # total_loss += loss.item()\n",
    "#     # y_pred = torch.cat((y_pred, preds.squeeze()))\n",
    "#     # y_true = torch.cat((y_true, targets.detach()))\n",
    "#     # avg_loss = total_loss / len(loader)\n",
    "#     return y_test, y_pred_c, y_pred\n",
    "\n",
    "# def test_model(model, criterion, loader):\n",
    "#     model.eval()\n",
    "#     # y_pred = torch.tensor([], requires_grad=True).to(device)\n",
    "#     # y_true = torch.tensor([], requires_grad=True).to(device)\n",
    "#     y_pred = torch.tensor([]).to(device)\n",
    "#     y_true = torch.tensor([]).to(device)\n",
    "#\n",
    "#     total_loss = 0.0\n",
    "#\n",
    "#     for x_cat, x_num, targets in loader:\n",
    "#         x_cat, x_num, targets = x_cat.to(device), x_num.to(device), targets.to(device)\n",
    "#         preds = model(x_cat, x_num)\n",
    "#         loss = criterion(preds, targets.long())\n",
    "#         total_loss += loss.item()\n",
    "#         y_pred = torch.cat((y_pred, preds.squeeze()))\n",
    "#         y_true = torch.cat((y_true, targets.detach()))\n",
    "#\n",
    "#     avg_loss = total_loss / len(loader)\n",
    "#     return avg_loss, y_pred.squeeze(), y_true.squeeze()"
   ],
   "id": "12dbd74af7086467",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Define train, validation and test sets",
   "id": "19509bda22360902"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T09:33:32.359798Z",
     "start_time": "2025-01-18T09:33:28.184309Z"
    }
   },
   "cell_type": "code",
   "source": [
    "save_in_test_folder = True\n",
    "if save_in_test_folder:\n",
    "    filepath = \"../TestModule\"\n",
    "else:\n",
    "    filepath = \"..\"\n",
    "\n",
    "seed = 42\n",
    "FILENAME = \"dataset/train_dataset.csv\"\n",
    "\n",
    "#Prepare train data\n",
    "df1 = pd.read_csv(FILENAME, sep=\",\", low_memory=False)\n",
    "\n",
    "# get features names\n",
    "features = list(df1.columns)\n",
    "# features_to_remove = [\"label\", \"ts\", \"src_ip\", \"dst_ip\", \"dns_query\", \"ssl_subject\", \"ssl_issuer\", \"http_uri\", \"type\", \"http_referrer\", \"http_user_agent\"]\n",
    "features_to_remove = [\"label\", \"type\", \"ts\", \"http_referrer\"]\n",
    "features = [feature for feature in features if feature not in features_to_remove]\n",
    "df1 = df1[features + [\"type\"]]\n",
    "\n",
    "# Converte i valori in numeri, sostituendo quelli non validi con NaN\n",
    "df1[\"src_bytes\"] = pd.to_numeric(df1[\"src_bytes\"], errors='coerce')\n",
    "# Filtra le righe con NaN (valori non convertibili)\n",
    "df1 = df1.dropna(subset=[\"src_bytes\"])\n",
    "# Converte i valori rimasti in interi\n",
    "df1.loc[:, \"src_bytes\"] = df1[\"src_bytes\"].astype(int)\n",
    "\n",
    "print(\"#Righe: \" + str(df1.shape[0]) + \" #Colonne: \" + str(df1.shape[1]))\n",
    "df1 = df1.dropna()\n",
    "print(\"#Righe: \" + str(df1.shape[0]) + \" #Colonne: \" + str(df1.shape[1]))\n",
    "\n",
    "X = df1[features]\n",
    "y = df1[\"type\"]\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y)\n",
    "with open(f\"{filepath}/transformer/target_encoder.save\", \"wb\") as f:\n",
    "    pickle.dump(le, f)\n",
    "\n",
    "y = le.transform(y)\n",
    "num_classes = len(le.classes_)\n",
    "\n",
    "indices = np.arange(X.shape[0])\n",
    "train_idx, val_idx = train_test_split(indices, test_size=0.2, stratify=y, random_state=seed)\n",
    "\n",
    "# fold = np.zeros(X.shape[0])\n",
    "# fold[train_idx] = -1\n",
    "\n",
    "n_splits = 10\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "fold = np.full(len(y), -1)  # Inizializza tutto con -1 (default: train)\n",
    "\n",
    "# Assegna i fold ai campioni\n",
    "for fold_number, (_, val_idx) in enumerate(skf.split(X, y)):\n",
    "    fold[val_idx] = fold_number  # Assegna il numero del fold ai campioni di validazione\n",
    "\n",
    "ps = PredefinedSplit(fold)\n",
    "ps.get_n_splits()\n",
    "\n",
    "# for i, (train_index, test_index) in enumerate(ps.split()):\n",
    "#     print(f\"Fold {i}:\")\n",
    "#     print(f\"  Train: index={train_index}\")\n",
    "#     print(f\"  Test:  index={test_index}\")\n",
    "\n",
    "# take only x with index in val_idx\n",
    "X_val = X.iloc[val_idx]\n",
    "y_val = y[val_idx]\n",
    "X_train = X.iloc[train_idx]\n",
    "y_train = y[train_idx]"
   ],
   "id": "6141bad0ff77c3fa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Righe: 616983 #Colonne: 43\n",
      "#Righe: 616983 #Colonne: 43\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Preprocessing",
   "id": "100df547cbfe401c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T09:33:41.831471Z",
     "start_time": "2025-01-18T09:33:32.364816Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "import pickle\n",
    "\n",
    "categorical_columns = X_train.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "numeric_columns = X_train.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "# boolean_columns = X_train.select_dtypes(include=[\"bool\"]).columns.tolist()\n",
    "\n",
    "class CustomOrdinalEncoder(OrdinalEncoder):\n",
    "    def transform(self, X):\n",
    "        encoded = super().transform(X)\n",
    "        # Shift all values by +1 and replace unknown_value (-1) with 0\n",
    "        return np.where(encoded == -1, 0, encoded + 1)\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        # Handle the inverse transform to account for the +1 offset\n",
    "        X = np.where(X == 0, -1, X - 1)\n",
    "        return super().inverse_transform(X)\n",
    "\n",
    "\n",
    "ct = ColumnTransformer(\n",
    "    [\n",
    "        (\"cat\", CustomOrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1), categorical_columns),  # Trasforma le colonne categoriche\n",
    "        # (\"ordinal\", OneHotEncoder(handle_unknown='infrequent_if_exist', sparse_output=False), categorical_columns),  # Trasforma le colonne categoriche\n",
    "        (\"scale\", StandardScaler(), numeric_columns)  # Normalizza le colonne numeriche\n",
    "    ],\n",
    "    remainder=\"passthrough\"  # Mantieni le altre colonne invariate\n",
    ")\n",
    "ct.set_output(transform=\"pandas\")\n",
    "\n",
    "ct = ct.fit(X_train)\n",
    "with open(f\"{filepath}/transformer/transformer_tf.save\", \"wb\") as f:\n",
    "    pickle.dump(ct, f)\n",
    "\n",
    "# train set\n",
    "X_train = ct.transform(X_train)\n",
    "\n",
    "cat_idxs = [i for i, f in enumerate(X_train.columns) if \"cat__\" in f]\n",
    "cat_dims = [len(X_train[f].unique()) + 1 for i, f in enumerate(X_train.columns) if \"cat__\" in f]\n",
    "num_idxs = [i for i, f in enumerate(X_train.columns) if \"scale__\" in f]\n",
    "numeric_columns_number = len(num_idxs)\n",
    "\n",
    "X_train = X_train.to_numpy()\n",
    "# validation set\n",
    "X_val = ct.transform(X_val).to_numpy()\n",
    "\n",
    "# X\n",
    "X = ct.transform(X).to_numpy()"
   ],
   "id": "3417858b58cc5303",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Define weights for unbalanced classes",
   "id": "a80285e3fda23d9d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T09:33:42.020095Z",
     "start_time": "2025-01-18T09:33:41.857679Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "sample_weight = compute_sample_weight(class_weight='balanced', y=y_train)\n",
    "print(class_weights)"
   ],
   "id": "6bdb6c0ca40abad2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 4.069469865611345, 1: 0.3381003918130257, 2: 1.132545546326465, 3: 4.543735616312253, 4: 98.7172, 5: 2.9863625363020327, 6: 1.1966881637007225, 7: 63.85329883570505, 8: 0.28803534018428717, 9: 0.9751965859248429}\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create DataLoader",
   "id": "9b8d57a3f8419975"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T09:33:42.170490Z",
     "start_time": "2025-01-18T09:33:42.069530Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TabDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x_cat, x_num, y):\n",
    "        self.x_cat = x_cat\n",
    "        self.x_num = x_num\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x_cat[idx], self.x_num[idx], self.y[idx]\n",
    "\n",
    "\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "# filter the categorical and numerical features\n",
    "X_cat_train = X_train_tensor[:, cat_idxs]\n",
    "X_num_train = X_train_tensor[:, num_idxs]\n",
    "\n",
    "\n",
    "X_cat_val = X_val_tensor[:, cat_idxs]\n",
    "X_num_val = X_val_tensor[:, num_idxs]\n",
    "\n",
    "# X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "# y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "train_dataset = TabDataset(X_cat_train, X_num_train, y_train_tensor)\n",
    "val_dataset = TabDataset(X_cat_val, X_num_val, y_val_tensor)\n",
    "\n",
    "# val_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_val_tensor, y_val_tensor), batch_size=y_val.shape[0], shuffle=False)\n",
    "# test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_test_tensor, y_test_tensor), batch_size=y_test.shape[0], shuffle=False)"
   ],
   "id": "906c3f2f4dd19812",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Hyperparameters configuration",
   "id": "cb6d99290b7a9591"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T09:33:42.181755Z",
     "start_time": "2025-01-18T09:33:42.175499Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nums_epochs = [1000]\n",
    "batch_sizes = [1024]\n",
    "patience = [20]\n",
    "dim_embedding = [8]\n",
    "num_heads = [8]\n",
    "num_layers = [2]\n",
    "learning_rate = [0.001]\n",
    "hyperparameters = list(itertools.product(nums_epochs, batch_sizes, patience, dim_embedding, num_heads, num_layers, learning_rate))\n",
    "n_comb = len(hyperparameters)\n",
    "print(f'Number of hyperparameter combinations: {n_comb}')"
   ],
   "id": "f0b658a4c013b7d7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of hyperparameter combinations: 1\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training",
   "id": "16212b070609649a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T10:49:28.358212Z",
     "start_time": "2025-01-18T09:33:42.291318Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize the model, loss, and optimizer\n",
    "best_loss = float('inf')\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=torch.tensor(list(class_weights.values()), dtype=torch.float32).to(device))\n",
    "current_iter = 0\n",
    "for epochs, batch_size, patience_, dim_embedding_, num_heads_, num_layers_, lr in hyperparameters:\n",
    "\n",
    "    print(f'Iteration {current_iter + 1}/{n_comb} - Hyperparameters: epochs={epochs}, batch_size={batch_size}, patience={patience_}, dim_embedding={dim_embedding_}, num_heads={num_heads_}, num_layers={num_layers_}, lr={lr}')\n",
    "\n",
    "    # Modello TabTransformer\n",
    "    model = TabTransformer(cat_dims, numeric_columns_number, num_classes, dim_embedding=dim_embedding_, num_heads=num_heads_, num_layers=num_layers_).to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    # Training\n",
    "    model, loss_history, val_loss_history = train_model(\n",
    "        model, criterion, optimizer, epochs, train_loader, val_loader, device, scheduler, patience_\n",
    "    )\n",
    "\n",
    "    # Validation\n",
    "    val_loss, y_pred, y_true = test_model(model, criterion, val_loader)\n",
    "    # y_true, _, y_pred = test_model(model, val_loader, device)\n",
    "    # val_loss = criterion(y_pred, y_true)\n",
    "\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        with open(f\"{filepath}/model/model_tf.save\", \"wb\") as f:\n",
    "            torch.save(model, f)\n",
    "        # best_model = copy.deepcopy(model)\n",
    "        best_hyperparameters = f\"epochs={epochs}, batch_size={batch_size}, patience={patience_}, dim_embedding={dim_embedding_}, num_heads={num_heads_}, num_layers={num_layers_}, lr={lr}\"\n",
    "\n",
    "    print(f'Hyperparameters: epochs={epochs}, batch_size={batch_size}, patience={patience_}, dim_embedding={dim_embedding_}, num_heads={num_heads_}, num_layers={num_layers_}, lr={lr}')\n",
    "    print(f'Validation Loss: {val_loss}')\n",
    "\n",
    "    current_iter += 1"
   ],
   "id": "5e03f4bbceab0bb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/1 - Hyperparameters: epochs=1000, batch_size=1024, patience=20, dim_embedding=8, num_heads=8, num_layers=2, lr=0.001\n",
      "Epoch [0/1000] - 23.49 seconds - Train Loss: 0.465393 - Val Loss: 0.170987 - Val Balanced Accuracy: 0.942187\n",
      "Epoch [1/1000] - 23.10 seconds - Train Loss: 0.167314 - Val Loss: 0.112947 - Val Balanced Accuracy: 0.951410\n",
      "Epoch [2/1000] - 23.77 seconds - Train Loss: 0.132189 - Val Loss: 0.109431 - Val Balanced Accuracy: 0.960977\n",
      "Epoch [3/1000] - 23.74 seconds - Train Loss: 0.110198 - Val Loss: 0.136625 - Val Balanced Accuracy: 0.959941\n",
      "Epoch [4/1000] - 23.31 seconds - Train Loss: 0.102455 - Val Loss: 0.083260 - Val Balanced Accuracy: 0.966621\n",
      "Epoch [5/1000] - 23.39 seconds - Train Loss: 0.091284 - Val Loss: 0.077487 - Val Balanced Accuracy: 0.973264\n",
      "Epoch [6/1000] - 23.28 seconds - Train Loss: 0.086682 - Val Loss: 0.068761 - Val Balanced Accuracy: 0.973750\n",
      "Epoch [7/1000] - 23.14 seconds - Train Loss: 0.078860 - Val Loss: 0.062875 - Val Balanced Accuracy: 0.975266\n",
      "Epoch [8/1000] - 23.23 seconds - Train Loss: 0.076057 - Val Loss: 0.059316 - Val Balanced Accuracy: 0.976015\n",
      "Epoch [9/1000] - 23.24 seconds - Train Loss: 0.071993 - Val Loss: 0.077401 - Val Balanced Accuracy: 0.974879\n",
      "Epoch [10/1000] - 23.35 seconds - Train Loss: 0.065585 - Val Loss: 0.050633 - Val Balanced Accuracy: 0.982041\n",
      "Epoch [11/1000] - 23.22 seconds - Train Loss: 0.061352 - Val Loss: 0.060028 - Val Balanced Accuracy: 0.983587\n",
      "Epoch [12/1000] - 23.31 seconds - Train Loss: 0.057101 - Val Loss: 0.044379 - Val Balanced Accuracy: 0.986558\n",
      "Epoch [13/1000] - 23.56 seconds - Train Loss: 0.054229 - Val Loss: 0.046851 - Val Balanced Accuracy: 0.986202\n",
      "Epoch [14/1000] - 23.49 seconds - Train Loss: 0.052243 - Val Loss: 0.056866 - Val Balanced Accuracy: 0.983652\n",
      "Epoch [15/1000] - 23.50 seconds - Train Loss: 0.049591 - Val Loss: 0.043210 - Val Balanced Accuracy: 0.989725\n",
      "Epoch [16/1000] - 23.21 seconds - Train Loss: 0.046905 - Val Loss: 0.042283 - Val Balanced Accuracy: 0.987998\n",
      "Epoch [17/1000] - 23.36 seconds - Train Loss: 0.045538 - Val Loss: 0.042613 - Val Balanced Accuracy: 0.987518\n",
      "Epoch [18/1000] - 23.45 seconds - Train Loss: 0.042868 - Val Loss: 0.046394 - Val Balanced Accuracy: 0.986066\n",
      "Epoch [19/1000] - 23.53 seconds - Train Loss: 0.041285 - Val Loss: 0.039047 - Val Balanced Accuracy: 0.990048\n",
      "Epoch [20/1000] - 23.30 seconds - Train Loss: 0.039502 - Val Loss: 0.038845 - Val Balanced Accuracy: 0.990515\n",
      "Epoch [21/1000] - 23.59 seconds - Train Loss: 0.039199 - Val Loss: 0.037346 - Val Balanced Accuracy: 0.989781\n",
      "Epoch [22/1000] - 23.38 seconds - Train Loss: 0.037948 - Val Loss: 0.038463 - Val Balanced Accuracy: 0.991436\n",
      "Epoch [23/1000] - 23.39 seconds - Train Loss: 0.035560 - Val Loss: 0.031594 - Val Balanced Accuracy: 0.992412\n",
      "Epoch [24/1000] - 23.57 seconds - Train Loss: 0.035071 - Val Loss: 0.031153 - Val Balanced Accuracy: 0.990103\n",
      "Epoch [25/1000] - 23.13 seconds - Train Loss: 0.034052 - Val Loss: 0.034602 - Val Balanced Accuracy: 0.992305\n",
      "Epoch [26/1000] - 24.15 seconds - Train Loss: 0.033669 - Val Loss: 0.033096 - Val Balanced Accuracy: 0.992690\n",
      "Epoch [27/1000] - 23.59 seconds - Train Loss: 0.033030 - Val Loss: 0.034092 - Val Balanced Accuracy: 0.992666\n",
      "Epoch [28/1000] - 23.45 seconds - Train Loss: 0.031753 - Val Loss: 0.028337 - Val Balanced Accuracy: 0.991259\n",
      "Epoch [29/1000] - 23.14 seconds - Train Loss: 0.030793 - Val Loss: 0.029895 - Val Balanced Accuracy: 0.992883\n",
      "Epoch [30/1000] - 23.13 seconds - Train Loss: 0.030349 - Val Loss: 0.028599 - Val Balanced Accuracy: 0.993896\n",
      "Epoch [31/1000] - 23.48 seconds - Train Loss: 0.029821 - Val Loss: 0.031504 - Val Balanced Accuracy: 0.991814\n",
      "Epoch [32/1000] - 23.63 seconds - Train Loss: 0.029273 - Val Loss: 0.027839 - Val Balanced Accuracy: 0.993794\n",
      "Epoch [33/1000] - 23.49 seconds - Train Loss: 0.029735 - Val Loss: 0.028130 - Val Balanced Accuracy: 0.993720\n",
      "Epoch [34/1000] - 23.16 seconds - Train Loss: 0.027957 - Val Loss: 0.028219 - Val Balanced Accuracy: 0.994105\n",
      "Epoch [35/1000] - 23.25 seconds - Train Loss: 0.027932 - Val Loss: 0.026855 - Val Balanced Accuracy: 0.994064\n",
      "Epoch [36/1000] - 23.07 seconds - Train Loss: 0.027908 - Val Loss: 0.024971 - Val Balanced Accuracy: 0.994293\n",
      "Epoch [37/1000] - 23.09 seconds - Train Loss: 0.026775 - Val Loss: 0.028754 - Val Balanced Accuracy: 0.994530\n",
      "Epoch [38/1000] - 23.10 seconds - Train Loss: 0.026821 - Val Loss: 0.030548 - Val Balanced Accuracy: 0.993851\n",
      "Epoch [39/1000] - 23.16 seconds - Train Loss: 0.027879 - Val Loss: 0.028045 - Val Balanced Accuracy: 0.993768\n",
      "Epoch [40/1000] - 23.12 seconds - Train Loss: 0.025995 - Val Loss: 0.026155 - Val Balanced Accuracy: 0.994936\n",
      "Epoch [41/1000] - 23.10 seconds - Train Loss: 0.025244 - Val Loss: 0.025683 - Val Balanced Accuracy: 0.994507\n",
      "Epoch [42/1000] - 23.13 seconds - Train Loss: 0.024865 - Val Loss: 0.023771 - Val Balanced Accuracy: 0.994698\n",
      "Epoch [43/1000] - 23.10 seconds - Train Loss: 0.025107 - Val Loss: 0.026339 - Val Balanced Accuracy: 0.994447\n",
      "Epoch [44/1000] - 23.15 seconds - Train Loss: 0.024757 - Val Loss: 0.026532 - Val Balanced Accuracy: 0.994034\n",
      "Epoch [45/1000] - 23.01 seconds - Train Loss: 0.024072 - Val Loss: 0.026909 - Val Balanced Accuracy: 0.994574\n",
      "Epoch [46/1000] - 23.15 seconds - Train Loss: 0.023558 - Val Loss: 0.024893 - Val Balanced Accuracy: 0.995038\n",
      "Epoch [47/1000] - 23.09 seconds - Train Loss: 0.024013 - Val Loss: 0.025926 - Val Balanced Accuracy: 0.994021\n",
      "Epoch [48/1000] - 23.14 seconds - Train Loss: 0.023529 - Val Loss: 0.023666 - Val Balanced Accuracy: 0.994730\n",
      "Epoch [49/1000] - 23.23 seconds - Train Loss: 0.024023 - Val Loss: 0.022099 - Val Balanced Accuracy: 0.994928\n",
      "Epoch [50/1000] - 23.15 seconds - Train Loss: 0.022690 - Val Loss: 0.024064 - Val Balanced Accuracy: 0.993862\n",
      "Epoch [51/1000] - 23.18 seconds - Train Loss: 0.023072 - Val Loss: 0.024605 - Val Balanced Accuracy: 0.994367\n",
      "Epoch [52/1000] - 23.22 seconds - Train Loss: 0.022912 - Val Loss: 0.022895 - Val Balanced Accuracy: 0.995600\n",
      "Epoch [53/1000] - 23.16 seconds - Train Loss: 0.022107 - Val Loss: 0.024792 - Val Balanced Accuracy: 0.995438\n",
      "Epoch [54/1000] - 23.20 seconds - Train Loss: 0.021822 - Val Loss: 0.021961 - Val Balanced Accuracy: 0.995544\n",
      "Epoch [55/1000] - 23.17 seconds - Train Loss: 0.022012 - Val Loss: 0.021446 - Val Balanced Accuracy: 0.995082\n",
      "Epoch [56/1000] - 23.18 seconds - Train Loss: 0.022097 - Val Loss: 0.021440 - Val Balanced Accuracy: 0.994729\n",
      "Epoch [57/1000] - 23.20 seconds - Train Loss: 0.022035 - Val Loss: 0.021785 - Val Balanced Accuracy: 0.995338\n",
      "Epoch [58/1000] - 23.18 seconds - Train Loss: 0.021543 - Val Loss: 0.021459 - Val Balanced Accuracy: 0.995257\n",
      "Epoch [59/1000] - 23.17 seconds - Train Loss: 0.021749 - Val Loss: 0.021388 - Val Balanced Accuracy: 0.995788\n",
      "Epoch [60/1000] - 23.08 seconds - Train Loss: 0.021145 - Val Loss: 0.023249 - Val Balanced Accuracy: 0.995156\n",
      "Epoch [61/1000] - 23.23 seconds - Train Loss: 0.022227 - Val Loss: 0.022344 - Val Balanced Accuracy: 0.994444\n",
      "Epoch [62/1000] - 23.19 seconds - Train Loss: 0.020476 - Val Loss: 0.021126 - Val Balanced Accuracy: 0.995937\n",
      "Epoch [63/1000] - 23.17 seconds - Train Loss: 0.020639 - Val Loss: 0.021621 - Val Balanced Accuracy: 0.995358\n",
      "Epoch [64/1000] - 23.16 seconds - Train Loss: 0.020915 - Val Loss: 0.024300 - Val Balanced Accuracy: 0.994248\n",
      "Epoch [65/1000] - 23.14 seconds - Train Loss: 0.020296 - Val Loss: 0.023273 - Val Balanced Accuracy: 0.995461\n",
      "Epoch [66/1000] - 23.14 seconds - Train Loss: 0.020075 - Val Loss: 0.022766 - Val Balanced Accuracy: 0.995690\n",
      "Epoch [67/1000] - 23.18 seconds - Train Loss: 0.020710 - Val Loss: 0.027052 - Val Balanced Accuracy: 0.994819\n",
      "Epoch [68/1000] - 23.15 seconds - Train Loss: 0.020309 - Val Loss: 0.022301 - Val Balanced Accuracy: 0.995646\n",
      "Epoch [69/1000] - 23.15 seconds - Train Loss: 0.020073 - Val Loss: 0.022369 - Val Balanced Accuracy: 0.995411\n",
      "Epoch [70/1000] - 23.20 seconds - Train Loss: 0.019687 - Val Loss: 0.021465 - Val Balanced Accuracy: 0.995365\n",
      "Epoch [71/1000] - 23.43 seconds - Train Loss: 0.019460 - Val Loss: 0.020408 - Val Balanced Accuracy: 0.995216\n",
      "Epoch [72/1000] - 23.49 seconds - Train Loss: 0.019445 - Val Loss: 0.022243 - Val Balanced Accuracy: 0.994612\n",
      "Epoch [73/1000] - 23.30 seconds - Train Loss: 0.019391 - Val Loss: 0.023302 - Val Balanced Accuracy: 0.995265\n",
      "Epoch [74/1000] - 23.46 seconds - Train Loss: 0.020050 - Val Loss: 0.021780 - Val Balanced Accuracy: 0.995537\n",
      "Epoch [75/1000] - 23.06 seconds - Train Loss: 0.019376 - Val Loss: 0.020577 - Val Balanced Accuracy: 0.995677\n",
      "Epoch [76/1000] - 23.20 seconds - Train Loss: 0.019062 - Val Loss: 0.021969 - Val Balanced Accuracy: 0.995590\n",
      "Epoch [77/1000] - 23.24 seconds - Train Loss: 0.019062 - Val Loss: 0.021286 - Val Balanced Accuracy: 0.995204\n",
      "Epoch [78/1000] - 23.88 seconds - Train Loss: 0.019382 - Val Loss: 0.020247 - Val Balanced Accuracy: 0.995676\n",
      "Epoch [79/1000] - 28.77 seconds - Train Loss: 0.018832 - Val Loss: 0.020546 - Val Balanced Accuracy: 0.995413\n",
      "Epoch [80/1000] - 24.90 seconds - Train Loss: 0.018489 - Val Loss: 0.019756 - Val Balanced Accuracy: 0.995571\n",
      "Epoch [81/1000] - 23.92 seconds - Train Loss: 0.018718 - Val Loss: 0.020707 - Val Balanced Accuracy: 0.995603\n",
      "Epoch [82/1000] - 25.48 seconds - Train Loss: 0.018272 - Val Loss: 0.020145 - Val Balanced Accuracy: 0.995695\n",
      "Epoch [83/1000] - 25.12 seconds - Train Loss: 0.018527 - Val Loss: 0.021281 - Val Balanced Accuracy: 0.995743\n",
      "Epoch [84/1000] - 23.98 seconds - Train Loss: 0.018652 - Val Loss: 0.021133 - Val Balanced Accuracy: 0.995353\n",
      "Epoch [85/1000] - 23.64 seconds - Train Loss: 0.018044 - Val Loss: 0.019070 - Val Balanced Accuracy: 0.995598\n",
      "Epoch [86/1000] - 23.77 seconds - Train Loss: 0.018038 - Val Loss: 0.019269 - Val Balanced Accuracy: 0.995619\n",
      "Epoch [87/1000] - 23.75 seconds - Train Loss: 0.017769 - Val Loss: 0.021303 - Val Balanced Accuracy: 0.994689\n",
      "Epoch [88/1000] - 23.76 seconds - Train Loss: 0.017926 - Val Loss: 0.020548 - Val Balanced Accuracy: 0.995741\n",
      "Epoch [89/1000] - 23.61 seconds - Train Loss: 0.017946 - Val Loss: 0.020962 - Val Balanced Accuracy: 0.996153\n",
      "Epoch [90/1000] - 23.57 seconds - Train Loss: 0.019437 - Val Loss: 0.021386 - Val Balanced Accuracy: 0.995735\n",
      "Epoch [91/1000] - 23.72 seconds - Train Loss: 0.017776 - Val Loss: 0.019115 - Val Balanced Accuracy: 0.995376\n",
      "Epoch [92/1000] - 23.92 seconds - Train Loss: 0.017619 - Val Loss: 0.019497 - Val Balanced Accuracy: 0.995968\n",
      "Epoch [93/1000] - 23.80 seconds - Train Loss: 0.017572 - Val Loss: 0.020491 - Val Balanced Accuracy: 0.995712\n",
      "Epoch [94/1000] - 24.41 seconds - Train Loss: 0.017488 - Val Loss: 0.019946 - Val Balanced Accuracy: 0.995747\n",
      "Epoch [95/1000] - 24.09 seconds - Train Loss: 0.017327 - Val Loss: 0.019964 - Val Balanced Accuracy: 0.995646\n",
      "Epoch [96/1000] - 23.85 seconds - Train Loss: 0.017409 - Val Loss: 0.019597 - Val Balanced Accuracy: 0.996084\n",
      "Epoch [97/1000] - 23.89 seconds - Train Loss: 0.017092 - Val Loss: 0.021760 - Val Balanced Accuracy: 0.995630\n",
      "Epoch [98/1000] - 27.80 seconds - Train Loss: 0.017187 - Val Loss: 0.018664 - Val Balanced Accuracy: 0.995926\n",
      "Epoch [99/1000] - 28.36 seconds - Train Loss: 0.016871 - Val Loss: 0.020702 - Val Balanced Accuracy: 0.995143\n",
      "Epoch [100/1000] - 25.04 seconds - Train Loss: 0.016990 - Val Loss: 0.020227 - Val Balanced Accuracy: 0.995926\n",
      "Epoch [101/1000] - 25.37 seconds - Train Loss: 0.016674 - Val Loss: 0.020832 - Val Balanced Accuracy: 0.995770\n",
      "Epoch [102/1000] - 24.81 seconds - Train Loss: 0.017382 - Val Loss: 0.019317 - Val Balanced Accuracy: 0.995942\n",
      "Epoch [103/1000] - 24.08 seconds - Train Loss: 0.017138 - Val Loss: 0.019616 - Val Balanced Accuracy: 0.996084\n",
      "Epoch [104/1000] - 25.31 seconds - Train Loss: 0.016459 - Val Loss: 0.019325 - Val Balanced Accuracy: 0.996007\n",
      "Epoch [105/1000] - 24.82 seconds - Train Loss: 0.016572 - Val Loss: 0.020108 - Val Balanced Accuracy: 0.995976\n",
      "Epoch [106/1000] - 25.11 seconds - Train Loss: 0.016517 - Val Loss: 0.020528 - Val Balanced Accuracy: 0.995818\n",
      "Epoch [107/1000] - 24.88 seconds - Train Loss: 0.016532 - Val Loss: 0.018417 - Val Balanced Accuracy: 0.995812\n",
      "Epoch [108/1000] - 24.14 seconds - Train Loss: 0.016807 - Val Loss: 0.020631 - Val Balanced Accuracy: 0.995136\n",
      "Epoch [109/1000] - 23.64 seconds - Train Loss: 0.016602 - Val Loss: 0.018821 - Val Balanced Accuracy: 0.995560\n",
      "Epoch [110/1000] - 23.97 seconds - Train Loss: 0.016059 - Val Loss: 0.019510 - Val Balanced Accuracy: 0.996279\n",
      "Epoch [111/1000] - 23.62 seconds - Train Loss: 0.016470 - Val Loss: 0.020198 - Val Balanced Accuracy: 0.996277\n",
      "Epoch [112/1000] - 24.22 seconds - Train Loss: 0.016031 - Val Loss: 0.018551 - Val Balanced Accuracy: 0.996016\n",
      "Epoch [113/1000] - 25.06 seconds - Train Loss: 0.016051 - Val Loss: 0.019095 - Val Balanced Accuracy: 0.996026\n",
      "Epoch [114/1000] - 24.78 seconds - Train Loss: 0.016037 - Val Loss: 0.018431 - Val Balanced Accuracy: 0.996202\n",
      "Epoch [115/1000] - 25.51 seconds - Train Loss: 0.016223 - Val Loss: 0.019714 - Val Balanced Accuracy: 0.995939\n",
      "Epoch [116/1000] - 25.19 seconds - Train Loss: 0.016201 - Val Loss: 0.019413 - Val Balanced Accuracy: 0.996015\n",
      "Epoch [117/1000] - 24.98 seconds - Train Loss: 0.016180 - Val Loss: 0.020335 - Val Balanced Accuracy: 0.995785\n",
      "Epoch [118/1000] - 24.07 seconds - Train Loss: 0.015874 - Val Loss: 0.017767 - Val Balanced Accuracy: 0.995675\n",
      "Epoch [119/1000] - 25.21 seconds - Train Loss: 0.016117 - Val Loss: 0.021823 - Val Balanced Accuracy: 0.995617\n",
      "Epoch [120/1000] - 24.32 seconds - Train Loss: 0.015821 - Val Loss: 0.018836 - Val Balanced Accuracy: 0.995893\n",
      "Epoch [121/1000] - 24.88 seconds - Train Loss: 0.015542 - Val Loss: 0.018884 - Val Balanced Accuracy: 0.996236\n",
      "Epoch [122/1000] - 24.67 seconds - Train Loss: 0.015464 - Val Loss: 0.018780 - Val Balanced Accuracy: 0.995876\n",
      "Epoch [123/1000] - 24.22 seconds - Train Loss: 0.015573 - Val Loss: 0.019363 - Val Balanced Accuracy: 0.996469\n",
      "Epoch [124/1000] - 23.69 seconds - Train Loss: 0.015368 - Val Loss: 0.018186 - Val Balanced Accuracy: 0.995849\n",
      "Epoch [125/1000] - 23.39 seconds - Train Loss: 0.015750 - Val Loss: 0.018437 - Val Balanced Accuracy: 0.996231\n",
      "Epoch [126/1000] - 23.67 seconds - Train Loss: 0.015471 - Val Loss: 0.017546 - Val Balanced Accuracy: 0.996215\n",
      "Epoch [127/1000] - 23.56 seconds - Train Loss: 0.015276 - Val Loss: 0.019489 - Val Balanced Accuracy: 0.996309\n",
      "Epoch [128/1000] - 23.88 seconds - Train Loss: 0.015454 - Val Loss: 0.017382 - Val Balanced Accuracy: 0.996248\n",
      "Epoch [129/1000] - 23.39 seconds - Train Loss: 0.015241 - Val Loss: 0.018060 - Val Balanced Accuracy: 0.996349\n",
      "Epoch [130/1000] - 23.36 seconds - Train Loss: 0.015485 - Val Loss: 0.018599 - Val Balanced Accuracy: 0.996090\n",
      "Epoch [131/1000] - 23.65 seconds - Train Loss: 0.015432 - Val Loss: 0.018476 - Val Balanced Accuracy: 0.996021\n",
      "Epoch [132/1000] - 23.51 seconds - Train Loss: 0.015121 - Val Loss: 0.018042 - Val Balanced Accuracy: 0.996589\n",
      "Epoch [133/1000] - 23.94 seconds - Train Loss: 0.015207 - Val Loss: 0.018446 - Val Balanced Accuracy: 0.996381\n",
      "Epoch [134/1000] - 23.59 seconds - Train Loss: 0.015159 - Val Loss: 0.018700 - Val Balanced Accuracy: 0.996273\n",
      "Epoch [135/1000] - 23.60 seconds - Train Loss: 0.015303 - Val Loss: 0.020280 - Val Balanced Accuracy: 0.995554\n",
      "Epoch [136/1000] - 24.12 seconds - Train Loss: 0.015297 - Val Loss: 0.017277 - Val Balanced Accuracy: 0.996063\n",
      "Epoch [137/1000] - 23.79 seconds - Train Loss: 0.015148 - Val Loss: 0.018235 - Val Balanced Accuracy: 0.996227\n",
      "Epoch [138/1000] - 24.45 seconds - Train Loss: 0.015001 - Val Loss: 0.018519 - Val Balanced Accuracy: 0.995809\n",
      "Epoch [139/1000] - 23.65 seconds - Train Loss: 0.014987 - Val Loss: 0.018299 - Val Balanced Accuracy: 0.995990\n",
      "Epoch [140/1000] - 23.52 seconds - Train Loss: 0.015127 - Val Loss: 0.018626 - Val Balanced Accuracy: 0.995987\n",
      "Epoch [141/1000] - 23.46 seconds - Train Loss: 0.014816 - Val Loss: 0.017784 - Val Balanced Accuracy: 0.996460\n",
      "Epoch [142/1000] - 23.45 seconds - Train Loss: 0.014727 - Val Loss: 0.017360 - Val Balanced Accuracy: 0.996428\n",
      "Epoch [143/1000] - 23.44 seconds - Train Loss: 0.014719 - Val Loss: 0.018536 - Val Balanced Accuracy: 0.996159\n",
      "Epoch [144/1000] - 23.70 seconds - Train Loss: 0.014976 - Val Loss: 0.017690 - Val Balanced Accuracy: 0.996013\n",
      "Epoch [145/1000] - 23.84 seconds - Train Loss: 0.015060 - Val Loss: 0.016824 - Val Balanced Accuracy: 0.996391\n",
      "Epoch [146/1000] - 23.71 seconds - Train Loss: 0.014833 - Val Loss: 0.018382 - Val Balanced Accuracy: 0.996102\n",
      "Epoch [147/1000] - 23.59 seconds - Train Loss: 0.014794 - Val Loss: 0.017096 - Val Balanced Accuracy: 0.996293\n",
      "Epoch [148/1000] - 23.65 seconds - Train Loss: 0.014841 - Val Loss: 0.017723 - Val Balanced Accuracy: 0.996368\n",
      "Epoch [149/1000] - 23.63 seconds - Train Loss: 0.014749 - Val Loss: 0.017281 - Val Balanced Accuracy: 0.996245\n",
      "Epoch [150/1000] - 23.94 seconds - Train Loss: 0.014562 - Val Loss: 0.018557 - Val Balanced Accuracy: 0.996221\n",
      "Epoch [151/1000] - 23.94 seconds - Train Loss: 0.014373 - Val Loss: 0.017041 - Val Balanced Accuracy: 0.996404\n",
      "Epoch [152/1000] - 24.80 seconds - Train Loss: 0.014651 - Val Loss: 0.016658 - Val Balanced Accuracy: 0.996327\n",
      "Epoch [153/1000] - 24.05 seconds - Train Loss: 0.014573 - Val Loss: 0.017584 - Val Balanced Accuracy: 0.996708\n",
      "Epoch [154/1000] - 25.67 seconds - Train Loss: 0.014373 - Val Loss: 0.018881 - Val Balanced Accuracy: 0.996134\n",
      "Epoch [155/1000] - 25.52 seconds - Train Loss: 0.014493 - Val Loss: 0.018198 - Val Balanced Accuracy: 0.996494\n",
      "Epoch [156/1000] - 26.17 seconds - Train Loss: 0.014498 - Val Loss: 0.017025 - Val Balanced Accuracy: 0.996510\n",
      "Epoch [157/1000] - 25.76 seconds - Train Loss: 0.014458 - Val Loss: 0.018241 - Val Balanced Accuracy: 0.996229\n",
      "Epoch [158/1000] - 24.68 seconds - Train Loss: 0.014298 - Val Loss: 0.018283 - Val Balanced Accuracy: 0.996278\n",
      "Epoch [159/1000] - 24.56 seconds - Train Loss: 0.014344 - Val Loss: 0.017103 - Val Balanced Accuracy: 0.996224\n",
      "Epoch [160/1000] - 24.52 seconds - Train Loss: 0.014172 - Val Loss: 0.017813 - Val Balanced Accuracy: 0.996494\n",
      "Epoch [161/1000] - 24.92 seconds - Train Loss: 0.014086 - Val Loss: 0.017086 - Val Balanced Accuracy: 0.996031\n",
      "Epoch [162/1000] - 24.55 seconds - Train Loss: 0.014206 - Val Loss: 0.017620 - Val Balanced Accuracy: 0.996241\n",
      "Epoch [163/1000] - 24.38 seconds - Train Loss: 0.014379 - Val Loss: 0.017243 - Val Balanced Accuracy: 0.996159\n",
      "Epoch [164/1000] - 25.47 seconds - Train Loss: 0.014239 - Val Loss: 0.017516 - Val Balanced Accuracy: 0.996631\n",
      "Epoch [165/1000] - 25.11 seconds - Train Loss: 0.014322 - Val Loss: 0.017217 - Val Balanced Accuracy: 0.996587\n",
      "Epoch [166/1000] - 25.49 seconds - Train Loss: 0.014345 - Val Loss: 0.017066 - Val Balanced Accuracy: 0.996541\n",
      "Epoch [167/1000] - 25.43 seconds - Train Loss: 0.014266 - Val Loss: 0.016207 - Val Balanced Accuracy: 0.996131\n",
      "Epoch [168/1000] - 24.99 seconds - Train Loss: 0.014236 - Val Loss: 0.017479 - Val Balanced Accuracy: 0.996413\n",
      "Epoch [169/1000] - 24.39 seconds - Train Loss: 0.014114 - Val Loss: 0.017959 - Val Balanced Accuracy: 0.996499\n",
      "Epoch [170/1000] - 24.66 seconds - Train Loss: 0.014138 - Val Loss: 0.017945 - Val Balanced Accuracy: 0.996379\n",
      "Epoch [171/1000] - 25.62 seconds - Train Loss: 0.014248 - Val Loss: 0.017598 - Val Balanced Accuracy: 0.996462\n",
      "Epoch [172/1000] - 24.91 seconds - Train Loss: 0.014110 - Val Loss: 0.017064 - Val Balanced Accuracy: 0.996767\n",
      "Epoch [173/1000] - 24.67 seconds - Train Loss: 0.013852 - Val Loss: 0.016322 - Val Balanced Accuracy: 0.996219\n",
      "Epoch [174/1000] - 25.22 seconds - Train Loss: 0.013856 - Val Loss: 0.017521 - Val Balanced Accuracy: 0.996557\n",
      "Epoch [175/1000] - 25.29 seconds - Train Loss: 0.013756 - Val Loss: 0.017215 - Val Balanced Accuracy: 0.996494\n",
      "Epoch [176/1000] - 25.46 seconds - Train Loss: 0.013909 - Val Loss: 0.016366 - Val Balanced Accuracy: 0.996444\n",
      "Epoch [177/1000] - 24.55 seconds - Train Loss: 0.014098 - Val Loss: 0.016980 - Val Balanced Accuracy: 0.996388\n",
      "Epoch [178/1000] - 25.24 seconds - Train Loss: 0.013993 - Val Loss: 0.018043 - Val Balanced Accuracy: 0.996293\n",
      "Epoch [179/1000] - 25.21 seconds - Train Loss: 0.013967 - Val Loss: 0.017262 - Val Balanced Accuracy: 0.996765\n",
      "Epoch [180/1000] - 25.43 seconds - Train Loss: 0.013527 - Val Loss: 0.017525 - Val Balanced Accuracy: 0.996457\n",
      "Epoch [181/1000] - 24.65 seconds - Train Loss: 0.013787 - Val Loss: 0.017270 - Val Balanced Accuracy: 0.996483\n",
      "Epoch [182/1000] - 24.96 seconds - Train Loss: 0.013810 - Val Loss: 0.016995 - Val Balanced Accuracy: 0.996627\n",
      "Epoch [183/1000] - 28.09 seconds - Train Loss: 0.013899 - Val Loss: 0.017224 - Val Balanced Accuracy: 0.996581\n",
      "Epoch [184/1000] - 26.96 seconds - Train Loss: 0.013644 - Val Loss: 0.017605 - Val Balanced Accuracy: 0.996465\n",
      "Epoch [185/1000] - 25.18 seconds - Train Loss: 0.013874 - Val Loss: 0.016662 - Val Balanced Accuracy: 0.996619\n",
      "Epoch [186/1000] - 24.66 seconds - Train Loss: 0.013624 - Val Loss: 0.016795 - Val Balanced Accuracy: 0.996657\n",
      "Epoch [187/1000] - 24.51 seconds - Train Loss: 0.013692 - Val Loss: 0.017893 - Val Balanced Accuracy: 0.996491\n",
      "Epoch [188/1000] - 23.13 seconds - Train Loss: 6.643738\r\n",
      "Training ended after 4543.47 seconds - Best val_loss: 0.016207\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'eval'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[10], line 24\u001B[0m\n\u001B[0;32m     19\u001B[0m model, loss_history, val_loss_history \u001B[38;5;241m=\u001B[39m train_model(\n\u001B[0;32m     20\u001B[0m     model, criterion, optimizer, epochs, train_loader, val_loader, device, scheduler, patience_\n\u001B[0;32m     21\u001B[0m )\n\u001B[0;32m     23\u001B[0m \u001B[38;5;66;03m# Validation\u001B[39;00m\n\u001B[1;32m---> 24\u001B[0m val_loss, y_pred, y_true \u001B[38;5;241m=\u001B[39m \u001B[43mtest_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     25\u001B[0m \u001B[38;5;66;03m# y_true, _, y_pred = test_model(model, val_loader, device)\u001B[39;00m\n\u001B[0;32m     26\u001B[0m \u001B[38;5;66;03m# val_loss = criterion(y_pred, y_true)\u001B[39;00m\n\u001B[0;32m     28\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m val_loss \u001B[38;5;241m<\u001B[39m best_loss:\n",
      "Cell \u001B[1;32mIn[4], line 73\u001B[0m, in \u001B[0;36mtest_model\u001B[1;34m(model, criterion, loader)\u001B[0m\n\u001B[0;32m     72\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mtest_model\u001B[39m(model, criterion, loader):\n\u001B[1;32m---> 73\u001B[0m     \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meval\u001B[49m()\n\u001B[0;32m     74\u001B[0m     y_pred \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m     75\u001B[0m     y_true \u001B[38;5;241m=\u001B[39m []\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'eval'"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Test",
   "id": "f6d478e41e3d5706"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "best_model = torch.load(f\"{filepath}/model/model_tf.save\")\n",
    "# test_loss, y_pred, y_true = test_model(best_model, criterion, test_loader)\n",
    "test_loss, y_pred, y_true = test_model(best_model, criterion, val_loader)\n",
    "y_pred = torch.argmax(y_pred, dim=1)\n",
    "print(f'Best hyperparameters: {best_hyperparameters}')\n",
    "print(f'Test Loss: {test_loss}')\n",
    "print(f'Test Accuracy: {accuracy_score(y_true.detach().numpy(), y_pred.detach().numpy())}')\n",
    "plt.plot(loss_history, label='train_loss')\n",
    "plt.plot(val_loss_history, label='val_loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "58c52c1ee93666c7",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
