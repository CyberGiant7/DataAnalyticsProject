{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# TabTransformer",
   "id": "63bee39ae7f9f0d9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Setting device and seed",
   "id": "faeeb4159ba17af7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "import itertools\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold, PredefinedSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight, compute_sample_weight\n"
   ],
   "id": "aad8f447c0f546f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "SEED = 42\n",
    "\n",
    "def fix_random(seed: int) -> None:\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True  # slower\n",
    "\n",
    "\n",
    "fix_random(SEED)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "# device = torch.device('cpu')\n",
    "\n",
    "print('Using device:', device)\n"
   ],
   "id": "df74ef3a7896fcc1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model definition",
   "id": "5a3edb68937bc46"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class TabTransformer(nn.Module):\n",
    "    def __init__(self, cat_dims, num_numerical, num_classes, dim_embedding=8, num_heads=2, num_layers=2, dropout=0.1, hidden_size=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            cat_dims: List of integers, dove ogni elemento rappresenta i valori unici di una colonna categoriale.\n",
    "            num_numerical: Numero di caratteristiche numeriche.\n",
    "            num_classes: Numero di classi per output.\n",
    "            dim_embedding: Dimensione degli embeddings.\n",
    "            num_heads: Numero di \"head\" nel Multi-Head Attention.\n",
    "            num_layers: Numero di livelli Transformer.\n",
    "            dropout: Dropout per prevenire overfitting.\n",
    "        \"\"\"\n",
    "        super(TabTransformer, self).__init__()\n",
    "\n",
    "        # Embeddings per features categoriali\n",
    "        self.embeddings = nn.ModuleList([\n",
    "            nn.Embedding(cat_dim, dim_embedding) for cat_dim in cat_dims\n",
    "        ])\n",
    "\n",
    "        # Layer per le features numeriche\n",
    "        self.numerical_norm = nn.LayerNorm(num_numerical) if num_numerical > 0 else None\n",
    "\n",
    "        # Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=dim_embedding,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=dim_embedding * 4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Classificatore finale\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(len(cat_dims) * dim_embedding + (num_numerical if num_numerical > 0 else 0), hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "\n",
    "            nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_cat, x_num):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x_cat: Tensore (batch_size, num_categorical_features), indici per features categoriali.\n",
    "            x_num: Tensore (batch_size, num_numerical_features), valori numerici.\n",
    "        Returns:\n",
    "            Logits (batch_size, num_classes).\n",
    "        \"\"\"\n",
    "        # Embedding per features categoriali\n",
    "        x_cat = x_cat.long()\n",
    "        cat_embeddings = [emb(x_cat[:, i]) for i, emb in enumerate(self.embeddings)]\n",
    "        cat_embeddings = torch.stack(cat_embeddings, dim=1)  # (batch_size, num_categorical_features, dim_embedding)\n",
    "\n",
    "        # Passa attraverso il Transformer\n",
    "        transformed_cat = self.transformer(cat_embeddings)  # (batch_size, num_categorical_features, dim_embedding)\n",
    "        transformed_cat = transformed_cat.view(transformed_cat.size(0), -1)  # Flatten per concatenare\n",
    "\n",
    "        # Normalizzazione delle features numeriche\n",
    "        if x_num is not None and self.numerical_norm is not None:\n",
    "            x_num = self.numerical_norm(x_num)\n",
    "\n",
    "        # Concatenazione\n",
    "        if x_num is not None:\n",
    "            x = torch.cat([transformed_cat, x_num], dim=1)\n",
    "        else:\n",
    "            x = transformed_cat\n",
    "\n",
    "        # Classificatore\n",
    "        logits = self.classifier(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class PyTorchTabTransformer:\n",
    "    def __init__(self, model, cat_idx, num_idx, device='cpu'):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.model.to(self.device)\n",
    "        self.cat_idx = cat_idx\n",
    "        self.num_idx = num_idx\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Esegue le previsioni sul modello PyTorch.\n",
    "        \"\"\"\n",
    "        self.model.eval()  # Modalità di valutazione\n",
    "        with torch.no_grad():\n",
    "            # Controlla se X è un array numpy e convertilo in un tensore PyTorch\n",
    "            if isinstance(X, np.ndarray):\n",
    "                X = torch.tensor(X, dtype=torch.float32).to(self.device)\n",
    "\n",
    "            # Supponi che X sia diviso in categoriale e numerico\n",
    "            y_pred = self.model(X[:, self.cat_idx].long(),\n",
    "                                X[:, self.num_idx])\n",
    "            return torch.argmax(y_pred, dim=1).cpu().numpy()"
   ],
   "id": "3d300e891c0e88e8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training and test utilities",
   "id": "40fedc19aa807a13"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import balanced_accuracy_score, f1_score\n",
    "import time\n",
    "\n",
    "\n",
    "def train_model(model, criterion, optimizer, epochs, data_loader, val_loader, device, scheduler, patience, cat_idxs, num_idxs):\n",
    "    n_iter = 0\n",
    "\n",
    "    best_model = None\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_since_last_improvement = 0\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    loss_history = []\n",
    "    val_loss_history = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "\n",
    "        start_epoch = time.time()\n",
    "\n",
    "        loss_train = 0\n",
    "\n",
    "        for x_cat, x_num, targets in data_loader:\n",
    "            # print(f'Epoch [{epoch}/{epochs}] - {time.time() - start_epoch:.2f} seconds - Train Loss: {loss_train:.6f}', end='\\r')\n",
    "            x_cat, x_num, targets = x_cat.to(device), x_num.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x_cat, x_num)  # Passa entrambe le componenti\n",
    "            loss = criterion(outputs, targets.long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            n_iter += 1\n",
    "            loss_train += loss.item()\n",
    "\n",
    "        scheduler.step()\n",
    "        loss_train /= len(data_loader)\n",
    "\n",
    "        # Compute Val Loss\n",
    "        val_loss, y_pred, y_true = test_model(model, criterion, val_loader)\n",
    "        # perf = evaluate_model(y_true.detach().cpu().numpy(), y_pred.detach().cpu().numpy())\n",
    "        loss_history.append(loss_train)\n",
    "\n",
    "        # Early stopping with threshold\n",
    "        threshold = 0.01\n",
    "        if val_loss < best_val_loss - threshold:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model, f\"{filepath}/models/model_best_tf.save\")\n",
    "            with open(f\"{filepath}/models/model_best_tf_custom.save\", \"wb\") as f:\n",
    "                wrapped_model = PyTorchTabTransformer(model, cat_idxs, num_idxs, device)\n",
    "                pickle.dump(wrapped_model, f)\n",
    "\n",
    "            epochs_since_last_improvement = 0\n",
    "        elif epochs_since_last_improvement >= patience:\n",
    "            break\n",
    "        else:\n",
    "            epochs_since_last_improvement += 1\n",
    "\n",
    "        # print('Epoch [{}/{}] - {:.2f} seconds - train_loss: {:.6f} - val_loss: {:.6f} - patience: {}'.format(epoch ,\n",
    "        #                                                                                                      epochs, time.time() - start_epoch, loss_train, val_loss, epochs_since_last_improvement), end='\\r')\n",
    "        # calculate balanced accuracy\n",
    "        balanced_accuracy = balanced_accuracy_score(\n",
    "            y_true.detach().cpu().numpy(),\n",
    "            y_pred.detach().cpu().numpy()\n",
    "        )\n",
    "        print(f'Epoch [{epoch}/{epochs}] - {time.time() - start_epoch:.2f} seconds - Train Loss: {loss_train:.6f} - Val Loss: {val_loss:.6f} - Val Balanced Accuracy: {balanced_accuracy:.6f}')\n",
    "\n",
    "    print('\\nTraining ended after {:.2f} seconds - Best val_loss: {:.6f}'.format(time.time() - start, best_val_loss))\n",
    "    best_model = torch.load(f\"{filepath}/models/model_best_tf.save\")\n",
    "    return best_model, loss_history, val_loss_history, best_val_loss\n",
    "\n",
    "\n",
    "def test_model(model, criterion, loader):\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():  # Turn off gradient tracking\n",
    "        for x_cat, x_num, targets in loader:\n",
    "            x_cat, x_num, targets = x_cat.to(device), x_num.to(device), targets.to(device)\n",
    "            preds = model(x_cat, x_num)  # Outputs logits or probabilities\n",
    "\n",
    "            loss = criterion(preds, targets.long())\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Convert predictions to class labels\n",
    "            predicted_classes = torch.argmax(preds, dim=1)\n",
    "\n",
    "            # Accumulate predictions and targets\n",
    "            y_pred.append(predicted_classes.cpu())\n",
    "            y_true.append(targets.cpu())\n",
    "\n",
    "    # Concatenate tensors only after the loop to minimize memory usage\n",
    "    y_pred = torch.cat(y_pred, dim=0)\n",
    "    y_true = torch.cat(y_true, dim=0)\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    return avg_loss, y_pred, y_true\n",
    "\n",
    "\n",
    "def evaluate_model(y_test, y_pred):\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    bacc = balanced_accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "\n",
    "    return {\"acc\": acc, \"bacc\": bacc, \"f1\": f1}\n"
   ],
   "id": "12dbd74af7086467",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Define train, validation and test sets",
   "id": "19509bda22360902"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "save_in_test_folder = True\n",
    "if save_in_test_folder:\n",
    "    filepath = \"../TestModule\"\n",
    "else:\n",
    "    filepath = \"..\"\n",
    "\n",
    "seed = 42\n",
    "FILENAME = \"dataset/train_dataset.csv\"\n",
    "\n",
    "#Prepare train data\n",
    "df1 = pd.read_csv(FILENAME, sep=\",\", low_memory=False)\n",
    "\n",
    "# get features names\n",
    "features = list(df1.columns)\n",
    "# features_to_remove = [\"label\", \"ts\", \"src_ip\", \"dst_ip\", \"dns_query\", \"ssl_subject\", \"ssl_issuer\", \"http_uri\", \"type\", \"http_referrer\", \"http_user_agent\"]\n",
    "features_to_remove = [\"label\", \"type\", \"ts\", \"http_referrer\"]\n",
    "features = [feature for feature in features if feature not in features_to_remove]\n",
    "df1 = df1[features + [\"type\"]]\n",
    "\n",
    "# Converte i valori in numeri, sostituendo quelli non validi con NaN\n",
    "df1[\"src_bytes\"] = pd.to_numeric(df1[\"src_bytes\"], errors='coerce')\n",
    "# Filtra le righe con NaN (valori non convertibili)\n",
    "df1 = df1.dropna(subset=[\"src_bytes\"])\n",
    "# Converte i valori rimasti in interi\n",
    "df1.loc[:, \"src_bytes\"] = df1[\"src_bytes\"].astype(int)\n",
    "\n",
    "print(\"#Righe: \" + str(df1.shape[0]) + \" #Colonne: \" + str(df1.shape[1]))\n",
    "df1 = df1.dropna()\n",
    "print(\"#Righe: \" + str(df1.shape[0]) + \" #Colonne: \" + str(df1.shape[1]))\n",
    "\n",
    "X = df1[features]\n",
    "y = df1[\"type\"]\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y)\n",
    "with open(f\"{filepath}/transformer/target_encoder.save\", \"wb\") as f:\n",
    "    pickle.dump(le, f)\n",
    "\n",
    "y = le.transform(y)\n",
    "num_classes = len(le.classes_)\n",
    "\n",
    "indices = np.arange(X.shape[0])\n",
    "train_idx, val_idx = train_test_split(indices, test_size=0.2, stratify=y, random_state=seed)\n",
    "\n",
    "# fold = np.zeros(X.shape[0])\n",
    "# fold[train_idx] = -1\n",
    "\n",
    "n_splits = 10\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "fold = np.full(len(y), -1)  # Inizializza tutto con -1 (default: train)\n",
    "\n",
    "# Assegna i fold ai campioni\n",
    "for fold_number, (_, val_idx) in enumerate(skf.split(X, y)):\n",
    "    fold[val_idx] = fold_number  # Assegna il numero del fold ai campioni di validazione\n",
    "\n",
    "ps = PredefinedSplit(fold)\n",
    "ps.get_n_splits()\n",
    "\n",
    "# for i, (train_index, test_index) in enumerate(ps.split()):\n",
    "#     print(f\"Fold {i}:\")\n",
    "#     print(f\"  Train: index={train_index}\")\n",
    "#     print(f\"  Test:  index={test_index}\")\n",
    "\n",
    "# take only x with index in val_idx\n",
    "X_val = X.iloc[val_idx]\n",
    "y_val = y[val_idx]\n",
    "X_train = X.iloc[train_idx]\n",
    "y_train = y[train_idx]"
   ],
   "id": "6141bad0ff77c3fa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Preprocessing",
   "id": "100df547cbfe401c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "import pickle\n",
    "\n",
    "class CustomOrdinalEncoder(OrdinalEncoder):\n",
    "    def transform(self, X):\n",
    "        encoded = super().transform(X)\n",
    "        # Shift all values by +1 and replace unknown_value (-1) with 0\n",
    "        return np.where(encoded == -1, 0, encoded + 1)\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        # Handle the inverse transform to account for the +1 offset\n",
    "        X = np.where(X == 0, -1, X - 1)\n",
    "        return super().inverse_transform(X)\n",
    "\n",
    "def preprocess(X_train,X_val):\n",
    "    categorical_columns = X_train.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "    numeric_columns = X_train.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "\n",
    "    ct = ColumnTransformer(\n",
    "        [\n",
    "            (\"cat\", CustomOrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1), categorical_columns),  # Trasforma le colonne categoriche\n",
    "            (\"scale\", StandardScaler(), numeric_columns)  # Normalizza le colonne numeriche\n",
    "        ],\n",
    "        remainder=\"passthrough\"  # Mantieni le altre colonne invariate\n",
    "    )\n",
    "    ct.set_output(transform=\"pandas\")\n",
    "\n",
    "    ct = ct.fit(X_train)\n",
    "    with open(f\"{filepath}/transformer/transformer_tf.save\", \"wb\") as f:\n",
    "        pickle.dump(ct, f)\n",
    "\n",
    "    # train set\n",
    "    X_train = ct.transform(X_train)\n",
    "\n",
    "    cat_idxs = [i for i, f in enumerate(X_train.columns) if \"cat__\" in f]\n",
    "    cat_dims = [len(X_train[f].unique()) + 1 for i, f in enumerate(X_train.columns) if \"cat__\" in f]\n",
    "    num_idxs = [i for i, f in enumerate(X_train.columns) if \"scale__\" in f]\n",
    "    numeric_columns_number = len(num_idxs)\n",
    "\n",
    "    X_train = X_train.to_numpy()\n",
    "\n",
    "    # validation set\n",
    "    X_val = ct.transform(X_val).to_numpy()\n",
    "    return X_train, X_val, cat_idxs, cat_dims, num_idxs, numeric_columns_number"
   ],
   "id": "3417858b58cc5303",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Define weights for unbalanced classes",
   "id": "a80285e3fda23d9d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "sample_weight = compute_sample_weight(class_weight='balanced', y=y_train)\n",
    "print(class_weights)"
   ],
   "id": "6bdb6c0ca40abad2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create DataLoader",
   "id": "9b8d57a3f8419975"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class TabDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x_cat, x_num, y):\n",
    "        self.x_cat = x_cat\n",
    "        self.x_num = x_num\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x_cat[idx], self.x_num[idx], self.y[idx]\n",
    "\n",
    "\n",
    "# X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "# y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "#\n",
    "# X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "# y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "#\n",
    "# # filter the categorical and numerical features\n",
    "# X_cat_train = X_train_tensor[:, cat_idxs]\n",
    "# X_num_train = X_train_tensor[:, num_idxs]\n",
    "#\n",
    "# X_cat_val = X_val_tensor[:, cat_idxs]\n",
    "# X_num_val = X_val_tensor[:, num_idxs]\n",
    "#\n",
    "# # X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "# # y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "#\n",
    "# train_dataset = TabDataset(X_cat_train, X_num_train, y_train_tensor)\n",
    "# val_dataset = TabDataset(X_cat_val, X_num_val, y_val_tensor)\n",
    "#\n",
    "# # val_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_val_tensor, y_val_tensor), batch_size=y_val.shape[0], shuffle=False)\n",
    "# # test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_test_tensor, y_test_tensor), batch_size=y_test.shape[0], shuffle=False)"
   ],
   "id": "906c3f2f4dd19812",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Hyperparameters configuration",
   "id": "cb6d99290b7a9591"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-19T18:20:40.964089Z",
     "start_time": "2025-01-19T18:20:40.957477Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nums_epochs = [1000]\n",
    "batch_sizes = [512, 1024]\n",
    "patience = [20]\n",
    "dim_embedding = [8, 16]\n",
    "num_heads = [4, 8]\n",
    "num_layers = [2, 4]\n",
    "hidden_sizes = [64, 128]\n",
    "learning_rate = [0.01, 0.001]\n",
    "dropout = [0, 0.3]\n",
    "hyperparameters = list(itertools.product(nums_epochs, batch_sizes, hidden_sizes, patience, dim_embedding, num_heads, num_layers, learning_rate, dropout))\n",
    "n_comb = len(hyperparameters)\n",
    "print(f'Number of hyperparameter combinations: {n_comb}')"
   ],
   "id": "f0b658a4c013b7d7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of hyperparameter combinations: 128\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training",
   "id": "16212b070609649a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "kf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "# Initialize the model, loss, and optimizer\n",
    "best_loss = float('inf')\n",
    "current_iter = 0\n",
    "for epochs, batch_size, hidden_size, patience_, dim_embedding_, num_heads_, num_layers_, lr, dropout in hyperparameters:\n",
    "    fix_random(seed)\n",
    "    start = time.time()\n",
    "\n",
    "    print(f'Iteration {current_iter + 1}/{n_comb} - Hyperparameters: epochs={epochs}, batch_size={batch_size}, hidden_size={hidden_size}, patience={patience_}, dim_embedding={dim_embedding_}, num_heads={num_heads_}, num_layers={num_layers_}, lr={lr}, dropout={dropout}')\n",
    "\n",
    "    log_name = f\"B{batch_size}-hidden{hidden_size}-pat{patience_}-dim{dim_embedding_}-heads{num_heads_}-layers{num_layers_}-lr{lr}-drop{dropout}\"\n",
    "\n",
    "    writer = SummaryWriter('runs/tab_transformer/' + log_name)\n",
    "    accuracy_per_fold = []\n",
    "    balanced_accuracy_score_per_fold = []\n",
    "    f1_score_per_fold = []\n",
    "    best_loss_per_fold = []\n",
    "\n",
    "    fold = 1\n",
    "\n",
    "    for train_index, val_index in kf.split(X, y):\n",
    "        X_train_fold, X_val_fold = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train_fold, y_val_fold = y[train_index], y[val_index]\n",
    "\n",
    "        X_train_fold, X_val_fold, cat_idxs_fold, cat_dims_fold, num_idxs_fold, numeric_columns_number_fold = preprocess(X_train_fold, X_val_fold)\n",
    "\n",
    "        X_train_tensor_fold = torch.tensor(X_train_fold, dtype=torch.float32)\n",
    "        y_train_tensor_fold = torch.tensor(y_train_fold, dtype=torch.long)\n",
    "\n",
    "        X_val_tensor_fold = torch.tensor(X_val_fold, dtype=torch.float32)\n",
    "        y_val_tensor_fold = torch.tensor(y_val_fold, dtype=torch.long)\n",
    "\n",
    "        # filter the categorical and numerical features\n",
    "        X_cat_train_fold = X_train_tensor_fold[:, cat_idxs_fold]\n",
    "        X_num_train_fold = X_train_tensor_fold[:, num_idxs_fold]\n",
    "\n",
    "        X_cat_val_fold = X_val_tensor_fold[:, cat_idxs_fold].long()\n",
    "        X_num_val_fold = X_val_tensor_fold[:, num_idxs_fold]\n",
    "\n",
    "        train_dataset_fold = TabDataset(X_cat_train_fold, X_num_train_fold, y_train_tensor_fold)\n",
    "        val_dataset_fold = TabDataset(X_cat_val_fold, X_num_val_fold, y_val_tensor_fold)\n",
    "\n",
    "        train_loader_fold = DataLoader(train_dataset_fold, batch_size=batch_size, shuffle=True)\n",
    "        val_loader_fold = DataLoader(val_dataset_fold, batch_size=batch_size)\n",
    "\n",
    "        # Modello TabTransformer\n",
    "\n",
    "        model = TabTransformer(cat_dims_fold, numeric_columns_number_fold, num_classes, dim_embedding=dim_embedding_, num_heads=num_heads_, num_layers=num_layers_, dropout=dropout, hidden_size=hidden_size).to(device)\n",
    "\n",
    "        class_weights_fold = compute_class_weight(class_weight='balanced', classes=np.unique(y_train_fold), y=y_train_fold)\n",
    "        class_weights_fold = dict(enumerate(class_weights_fold))\n",
    "\n",
    "        criterion = torch.nn.CrossEntropyLoss(weight=torch.tensor(list(class_weights_fold.values()), dtype=torch.float32).to(device))\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)\n",
    "\n",
    "        # Training\n",
    "        model, loss_history, val_loss_history, best_loss_per_fold = train_model(\n",
    "            model, criterion, optimizer, epochs, train_loader_fold, val_loader_fold, device, scheduler, patience_, cat_idxs_fold, num_idxs_fold\n",
    "        )\n",
    "\n",
    "        # Validation\n",
    "        val_loss, y_pred, y_true = test_model(model, criterion, val_loader_fold)\n",
    "        perf = evaluate_model(y_true.detach().cpu().numpy(), y_pred.detach().cpu().numpy())\n",
    "\n",
    "        print(f\"Fold {fold} - Accuracy: {perf['acc']:.2f}%\")\n",
    "        print(f\"Fold {fold} - Balanced Accuracy: {perf['bacc']:.2f}%\")\n",
    "        print(f\"Fold {fold} - F1 Score: {perf['f1']:.2f}%\")\n",
    "\n",
    "        accuracy_per_fold.append(perf[\"acc\"])\n",
    "        balanced_accuracy_score_per_fold.append(perf[\"bacc\"])\n",
    "        f1_score_per_fold.append(perf[\"f1\"])\n",
    "        best_loss_per_fold.append(best_loss)\n",
    "        fold += 1\n",
    "\n",
    "\n",
    "        # y_true, _, y_pred = test_model(model, val_loader, device)\n",
    "        # val_loss = criterion(y_pred, y_true)\n",
    "\n",
    "        # if val_loss < best_loss:\n",
    "        #     best_loss = val_loss\n",
    "        #     with open(f\"{filepath}/models/tf.save\", \"wb\") as f:\n",
    "        #         wrapped_model = PyTorchTabTransformer(model, cat_idxs_fold, num_idxs_fold, device)\n",
    "        #         pickle.dump(wrapped_model, f)\n",
    "        #     # best_model = copy.deepcopy(model)\n",
    "        #     best_hyperparameters = f\"epochs={epochs}, batch_size={batch_size}, patience={patience_}, dim_embedding={dim_embedding_}, num_heads={num_heads_}, num_layers={num_layers_}, lr={lr}\"\n",
    "\n",
    "        # print(f'Hyperparameters: epochs={epochs}, batch_size={batch_size}, patience={patience_}, dim_embedding={dim_embedding_}, num_heads={num_heads_}, num_layers={num_layers_}, lr={lr}')\n",
    "        # print(f'Validation Loss: {val_loss}')\n",
    "    # Riassunto dei risultati\n",
    "    print(\"Cross-Validation Results:\")\n",
    "    print(f\"Average Accuracy: {np.mean(accuracy_per_fold) * 100:.2f}%\")\n",
    "    print(f\"Standard Deviation of Accuracy: {np.std(accuracy_per_fold) * 100:.2f}%\")\n",
    "    print(f\"Average Balanced Accuracy: {np.mean(balanced_accuracy_score_per_fold) * 100:.2f}%\")\n",
    "    print(f\"Standard Deviation of Balanced Accuracy: {np.std(balanced_accuracy_score_per_fold) * 100:.2f}%\")\n",
    "    print(f\"Average F1 Score: {np.mean(f1_score_per_fold) * 100:.2f}%\")\n",
    "    print(f\"Standard Deviation of F1 Score: {np.std(f1_score_per_fold) * 100:.2f}%\")\n",
    "\n",
    "    # Close tensorboard writer after a training\n",
    "    # Log hyperparameters and metrics to TensorBoard\n",
    "    writer.add_hparams(\n",
    "        {\n",
    "            'hparam/bsize': batch_size,\n",
    "            'hparam/hidden_size': hidden_size,\n",
    "            'hparam/epochs': epochs,\n",
    "            'hparam/patience': patience_,\n",
    "            'hparam/dim_embedding': dim_embedding_,\n",
    "            'hparam/num_heads': num_heads_,\n",
    "            'hparam/num_layers': num_layers_,\n",
    "            'hparam/lr': lr,\n",
    "            'hparam/dropout': dropout\n",
    "        },\n",
    "        {\n",
    "            'Best Loss': np.mean(best_loss_per_fold),\n",
    "            'Avg Accuracy': np.mean(accuracy_per_fold),\n",
    "            'Std Accuracy': np.std(accuracy_per_fold),\n",
    "            'Avg Balanced Accuracy': np.mean(balanced_accuracy_score_per_fold),\n",
    "            'Std Balanced Accuracy': np.std(balanced_accuracy_score_per_fold),\n",
    "            'Avg F1 score': np.mean(f1_score_per_fold),\n",
    "            'Std F1 score': np.std(f1_score_per_fold)\n",
    "        }\n",
    "    )\n",
    "    writer.flush()\n",
    "    current_iter += 1\n",
    "    writer.close()"
   ],
   "id": "5e03f4bbceab0bb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Test",
   "id": "f6d478e41e3d5706"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# best_model = torch.load(f\"{filepath}/model/model_tf.save\")\n",
    "# # test_loss, y_pred, y_true = test_model(best_model, criterion, test_loader)\n",
    "# test_loss, y_pred, y_true = test_model(best_model, criterion, val_loader)\n",
    "# y_pred = torch.argmax(y_pred, dim=1)\n",
    "# print(f'Best hyperparameters: {best_hyperparameters}')\n",
    "# print(f'Test Loss: {test_loss}')\n",
    "# print(f'Test Accuracy: {accuracy_score(y_true.detach().numpy(), y_pred.detach().numpy())}')\n",
    "# plt.plot(loss_history, label='train_loss')\n",
    "# plt.plot(val_loss_history, label='val_loss')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ],
   "id": "58c52c1ee93666c7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Hyperparameter tuning",
   "id": "4361a2f0c8e134a7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "\n"
   ],
   "id": "ca29e6d4addfac6a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
