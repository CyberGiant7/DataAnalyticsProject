{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedforward Neural Network\n",
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-31T12:33:19.040981Z",
     "start_time": "2025-01-31T12:33:15.247850Z"
    }
   },
   "source": [
    "import itertools\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn import preprocessing\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import balanced_accuracy_score, f1_score, accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "from sklearn.utils import compute_class_weight\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Setting device and seed"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-31T12:33:20.134655Z",
     "start_time": "2025-01-31T12:33:20.130343Z"
    }
   },
   "source": [
    "seed = 42\n",
    "\n",
    "# For reproducibility\n",
    "def fix_random(seed: int) -> None:\n",
    "    \"\"\"Fix all the possible sources of randomness.\n",
    "\n",
    "    Args:\n",
    "        seed: the seed to use. \n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True  # slower\n",
    "\n",
    "fix_random(seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print('Using device:', device)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create DataLoader"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-31T12:33:21.567591Z",
     "start_time": "2025-01-31T12:33:21.558088Z"
    }
   },
   "source": [
    "# Define the Data Layer\n",
    "class TabDataset(torch.utils.data.Dataset):\n",
    "    # Define a custom dataset class for handling categorical and numerical data\n",
    "    def __init__(self, x_cat, x_num, y):\n",
    "        self.x_cat = x_cat\n",
    "        self.x_num = x_num\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x_cat[idx], self.x_num[idx], self.y[idx]\n",
    "\n",
    "\n",
    "def create_datasets(X_train, y_train, X_val, y_val, cat_idxs, num_idxs):\n",
    "    # Convert training data to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "\n",
    "    # Convert validation data to PyTorch tensors\n",
    "    X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "    y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "    # Filter the categorical and numerical features for training data\n",
    "    X_cat_train = X_train_tensor[:, cat_idxs]\n",
    "    X_num_train = X_train_tensor[:, num_idxs]\n",
    "\n",
    "    # Filter the categorical and numerical features for validation data\n",
    "    X_cat_val = X_val_tensor[:, cat_idxs].long()\n",
    "    X_num_val = X_val_tensor[:, num_idxs]\n",
    "\n",
    "    # Create the dataset for training and validation\n",
    "    train_dataset = TabDataset(X_cat_train, X_num_train, y_train_tensor)\n",
    "    val_dataset = TabDataset(X_cat_val, X_num_val, y_val_tensor)\n",
    "\n",
    "    return train_dataset, val_dataset"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Define preprocessing utilities"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-31T12:54:34.574716Z",
     "start_time": "2025-01-31T12:54:34.564412Z"
    }
   },
   "source": [
    "class CustomOrdinalEncoder(OrdinalEncoder):\n",
    "    def transform(self, X):\n",
    "        encoded = super().transform(X)\n",
    "        # Shift all values by +1 and replace unknown_value (-1) with 0\n",
    "        return np.where(encoded == -1, 0, encoded + 1)\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        # Handle the inverse transform to account for the +1 offset\n",
    "        X = np.where(X == 0, -1, X - 1)\n",
    "        return super().inverse_transform(X)\n",
    "\n",
    "\n",
    "# define a function with different normalization and scaling techniques\n",
    "def preprocess(X_train, X_val, save=False):\n",
    "    categorical_columns = X_train.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "    numeric_columns = X_train.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "\n",
    "    ct = ColumnTransformer(\n",
    "        [\n",
    "            (\"cat\", CustomOrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1), categorical_columns),  # Trasforma le colonne categoriche\n",
    "            (\"scale\", StandardScaler(), numeric_columns)  # Normalizza le colonne numeriche\n",
    "        ],\n",
    "        remainder=\"passthrough\"  # Mantieni le altre colonne invariate\n",
    "    )\n",
    "    ct.set_output(transform=\"pandas\")\n",
    "\n",
    "    ct = ct.fit(X_train)\n",
    "    if save:\n",
    "        with open(f\"{filepath}/transformer/transformer_ff.save\", \"wb\") as f:\n",
    "            pickle.dump(ct, f)\n",
    "\n",
    "    # train set\n",
    "    X_train = ct.transform(X_train)\n",
    "\n",
    "    cat_idxs = [i for i, f in enumerate(X_train.columns) if \"cat__\" in f]\n",
    "    cat_dims = [len(X_train[f].unique()) + 1 for i, f in enumerate(X_train.columns) if \"cat__\" in f]\n",
    "    num_idxs = [i for i, f in enumerate(X_train.columns) if \"scale__\" in f]\n",
    "    numeric_columns_number = len(num_idxs)\n",
    "\n",
    "    X_train = X_train.to_numpy()\n",
    "    # validation set\n",
    "    X_val = ct.transform(X_val).to_numpy()\n",
    "\n",
    "    return X_train, X_val, cat_idxs, cat_dims, num_idxs, numeric_columns_number"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Define the model"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-31T12:36:32.283883Z",
     "start_time": "2025-01-31T12:36:32.272161Z"
    }
   },
   "source": [
    "# Architecture\n",
    "class FeedForwardPlus(nn.Module):\n",
    "    def __init__(self, cat_dims, num_numerical, num_categorical, num_classes, hidden_size, depth=1, batch_norm=False, drop=0, dim_embedding=8, ):\n",
    "        super(FeedForwardPlus, self).__init__()\n",
    "\n",
    "        # Initialize embeddings for categorical features\n",
    "        self.embeddings = nn.ModuleList([\n",
    "            nn.Embedding(cat_dim, dim_embedding) for cat_dim in cat_dims\n",
    "        ])\n",
    "\n",
    "        # Layer normalization for numerical features\n",
    "        self.numerical_norm = nn.LayerNorm(num_numerical)\n",
    "\n",
    "        model = []\n",
    "        # Input layer\n",
    "        model += [nn.Linear(num_categorical * dim_embedding + num_numerical, hidden_size)]\n",
    "        if batch_norm:\n",
    "            model += [nn.BatchNorm1d(hidden_size)]\n",
    "        model += [nn.ReLU()]\n",
    "\n",
    "        # Define blocks for hidden layers\n",
    "        block = [\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU()\n",
    "        ]\n",
    "\n",
    "        block_batch_norm = [\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.ReLU()\n",
    "        ]\n",
    "\n",
    "        block_dropout = [\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU()\n",
    "        ]\n",
    "\n",
    "        # Add hidden layers based on depth, batch normalization, and dropout\n",
    "        for i in range(depth):\n",
    "            if not batch_norm and drop == 0:\n",
    "                model += block\n",
    "            elif batch_norm and drop == 0:\n",
    "                model += block_batch_norm\n",
    "            elif drop > 0 and not batch_norm:\n",
    "                model += block_dropout\n",
    "\n",
    "        # Sequential model\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "        # Output layer\n",
    "        self.output = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x_cat, x_num):\n",
    "        # Convert categorical features to long type\n",
    "        x_cat = x_cat.long()\n",
    "        # Get embeddings for categorical features\n",
    "        cat_embeddings = [emb(x_cat[:, i]) for i, emb in enumerate(self.embeddings)]\n",
    "        cat_embeddings = torch.stack(cat_embeddings, dim=1)  # (batch_size, num_categorical_features, dim_embedding)\n",
    "        cat_embeddings = cat_embeddings.view(cat_embeddings.size(0), -1)  # Flatten to concatenate\n",
    "\n",
    "        # Normalize numerical features\n",
    "        x_num = self.numerical_norm(x_num)\n",
    "        # Concatenate categorical embeddings and numerical features\n",
    "        x = torch.cat([cat_embeddings, x_num], dim=1)\n",
    "        # Pass through the model\n",
    "        h = self.model(x)\n",
    "        # Get the output\n",
    "        out = self.output(h)\n",
    "        return out\n"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Define the wrapper"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-31T12:33:29.622881Z",
     "start_time": "2025-01-31T12:33:29.617079Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PyTorchFeedForwardWrapper:\n",
    "    def __init__(self, model, cat_idx, num_idx, device='cpu'):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.model.to(self.device)\n",
    "        self.cat_idx = cat_idx\n",
    "        self.num_idx = num_idx\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Esegue le previsioni sul modello PyTorch.\n",
    "        \"\"\"\n",
    "        self.model.eval()  # Modalità di valutazione\n",
    "        with torch.no_grad():\n",
    "            # Controlla se X è un array numpy e convertilo in un tensore PyTorch\n",
    "            if isinstance(X, np.ndarray):\n",
    "                X = torch.tensor(X, dtype=torch.float32).to(self.device)\n",
    "\n",
    "            # Supponi che X sia diviso in categoriale e numerico\n",
    "            y_pred = self.model(X[:, self.cat_idx].long(),\n",
    "                                X[:, self.num_idx])\n",
    "            return torch.argmax(y_pred, dim=1).cpu().numpy()"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Define training function"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-31T12:33:33.050126Z",
     "start_time": "2025-01-31T12:33:33.041136Z"
    }
   },
   "source": [
    "# Define a function for the training process\n",
    "\n",
    "def train_model(model: FeedForwardPlus, criterion, optimizer, epoch, scheduler, train_loader, val_loader, device, writer, log_name=\"model\"):\n",
    "    # Initialize variables for tracking training progress\n",
    "    n_iter = 0\n",
    "    best_valid_loss = float('inf')\n",
    "    patience = 20\n",
    "    epochs_since_last_improvement = 0\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Training loop for each epoch\n",
    "    for epoch in range(epoch):\n",
    "        model.train()  # Set model to training mode\n",
    "        loss_train = 0\n",
    "\n",
    "        # Iterate over batches in the training loader\n",
    "        for x_cat, x_num, targets in train_loader:\n",
    "            x_cat, x_num, targets = x_cat.to(device), x_num.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            y_pred = model(x_cat, x_num)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(y_pred, targets)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            n_iter += 1\n",
    "            loss_train += loss.item()\n",
    "\n",
    "        # Calculate average training loss\n",
    "        loss_train /= len(train_loader)\n",
    "        writer.add_scalar(\"Metrics/Loss/train\", loss_train, epoch)\n",
    "\n",
    "        # Validate the model\n",
    "        y_true, y_pred_c, y_pred = test_model(model, val_loader, device)\n",
    "        loss_val = criterion(y_pred, y_true).item()\n",
    "        writer.add_scalar(\"Metrics/Loss/val\", loss_val, epoch)\n",
    "\n",
    "        # Evaluate performance metrics\n",
    "        perf = evaluate_model(y_true.detach().cpu().numpy(), y_pred_c.detach().cpu().numpy())\n",
    "\n",
    "        # Print epoch summary\n",
    "        print(f'Epoch [{epoch}] - {time.time() - start:.2f} seconds - Train Loss: {loss_train:.6f} - Val Loss: {loss_val:.6f} - Accuracy: {perf[\"acc\"]:.3f} - Balanced Accuracy: {perf[\"bacc\"]:.3f} - Val F1 Score: {perf[\"f1\"]:.3f}')\n",
    "        writer.add_scalar(\"Metrics/Accuracy/val\", perf[\"acc\"], epoch)\n",
    "        writer.add_scalar(\"Metrics/Balanced Accuracy/val\", perf[\"bacc\"], epoch)\n",
    "        writer.add_scalar(\"Metrics/F1 Score/val\", perf[\"f1\"], epoch)\n",
    "\n",
    "        threshold = 0.01\n",
    "        # Save the best model if validation loss improves\n",
    "        if loss_val < best_valid_loss - threshold:\n",
    "            best_valid_loss = loss_val\n",
    "            torch.save(model, f\"{filepath}/models/best_model_ff.save\")\n",
    "            if not os.path.exists('models'):\n",
    "                os.makedirs('models')\n",
    "            epochs_since_last_improvement = 0\n",
    "        elif epochs_since_last_improvement >= patience:\n",
    "            break\n",
    "        else:\n",
    "            epochs_since_last_improvement += 1\n",
    "\n",
    "        writer.add_scalar(\"hparam/Learning Rate\", scheduler.get_last_lr()[0], epoch)\n",
    "\n",
    "        # Step the learning rate scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "    # Load the best model\n",
    "    best_model = torch.load(f\"{filepath}/models/best_model_ff.save\")\n",
    "    return best_model, best_valid_loss"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Define test function"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-31T12:33:40.811158Z",
     "start_time": "2025-01-31T12:33:40.802805Z"
    }
   },
   "source": [
    "# Define a function to evaluate the performance on validation and test sets\n",
    "def test_model(model, data_loader, device):\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    y_test = []\n",
    "\n",
    "    # Iterate over batches in the data loader\n",
    "    for x_cat, x_num, targets in data_loader:\n",
    "        # Move data to the specified device (CPU or GPU)\n",
    "        x_cat, x_num, targets = x_cat.to(device), x_num.to(device), targets.to(device)\n",
    "        # Perform forward pass and collect predictions\n",
    "        y_pred += model(x_cat, x_num)\n",
    "        # Collect true targets\n",
    "        y_test += targets\n",
    "\n",
    "    # Stack the collected targets and predictions into tensors\n",
    "    y_test = torch.stack(y_test).squeeze()\n",
    "    y_pred = torch.stack(y_pred).squeeze()\n",
    "    # Get the class predictions by taking the argmax\n",
    "    y_pred_c = y_pred.argmax(dim=1, keepdim=True).squeeze()\n",
    "\n",
    "    # Return the true targets, class predictions, and raw predictions\n",
    "    return y_test, y_pred_c, y_pred\n",
    "\n",
    "\n",
    "def evaluate_model(y_test, y_pred):\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    bacc = balanced_accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "    return {\"acc\": acc, \"bacc\": bacc, \"f1\": f1}"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Define train, validation"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-31T12:55:07.367806Z",
     "start_time": "2025-01-31T12:54:55.907499Z"
    }
   },
   "source": [
    "filepath = \".\"\n",
    "\n",
    "FILENAME = \"dataset/train_dataset.csv\"\n",
    "\n",
    "#Prepare train data\n",
    "data = pd.read_csv(FILENAME, sep=\",\", low_memory=False)\n",
    "\n",
    "# Extract relevant features excluding specified columns\n",
    "features_to_retain = data.columns.difference([\"label\", \"ts\", \"type\", \"http_referrer\"]).tolist()\n",
    "data = data[features_to_retain + [\"type\"]]\n",
    "\n",
    "print(\"#Righe: \" + str(data.shape[0]) + \" #Colonne: \" + str(data.shape[1]))\n",
    "\n",
    "# Converte i valori in numeri, sostituendo quelli non validi con NaN\n",
    "data[\"src_bytes\"] = pd.to_numeric(data[\"src_bytes\"], errors='coerce')\n",
    "# Filtra le righe con NaN (valori non convertibili)\n",
    "data = data.dropna(subset=[\"src_bytes\"])\n",
    "# Converte i valori rimasti in interi\n",
    "data.loc[:, \"src_bytes\"] = data[\"src_bytes\"].astype(int)\n",
    "\n",
    "data = data.dropna()\n",
    "print(\"Dopo drop NaN\")\n",
    "print(\"#Righe: \" + str(data.shape[0]) + \" #Colonne: \" + str(data.shape[1]))\n",
    "\n",
    "features = data.columns.difference([\"type\"]).tolist()\n",
    "X = data[features]\n",
    "y = data[\"type\"]\n",
    "\n",
    "with open(f\"{filepath}/transformer/target_encoder.save\", \"rb\") as f:\n",
    "    le: preprocessing.LabelEncoder = pickle.load(f)\n",
    "\n",
    "num_classes = len(le.classes_)\n",
    "y = le.transform(y)\n",
    "\n",
    "# Separate indices\n",
    "indices = np.arange(X.shape[0])\n",
    "train_idx, val_idx = train_test_split(indices, test_size=0.2, stratify=y, random_state=seed)\n",
    "\n",
    "# Take only X with index in val_idx\n",
    "X_val = X.iloc[val_idx]\n",
    "y_val = y[val_idx]\n",
    "\n",
    "# Take only X with index in train_idx\n",
    "X_train = X.iloc[train_idx]\n",
    "y_train = y[train_idx]\n",
    "\n",
    "X_train, X_val, cat_idxs, cat_dims, num_idxs, numeric_columns_number = preprocess(X_train, X_val, save=True)\n",
    "\n",
    "print(\"X_train: #Righe: \" + str(X_train.shape[0]) + \" #Colonne: \" + str(X_train.shape[1]))\n",
    "print(\"X_val: #Righe: \" + str(X_val.shape[0]) + \" #Colonne: \" + str(X_val.shape[1]))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Righe: 617002 #Colonne: 43\n",
      "Dopo drop NaN\n",
      "#Righe: 616983 #Colonne: 43\n",
      "X_train: #Righe: 493586 #Colonne: 42\n",
      "X_val: #Righe: 123397 #Colonne: 42\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Cross-validation and hyperparameter tuning"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-01-19T12:29:16.859578Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of hyperparameter combinations: 64\n",
      "Iteration 1/64 B512-dim64-dp2-ep100-lr0.01-steplr10-gamma0.5-BNFalse-drop0\n",
      "Epoch [0] - 22.78 seconds - Train Loss: 0.270018 - Val Loss: 0.110735 - Accuracy: 0.960 - Balanced Accuracy: 0.955 - Val F1 Score: 0.963\n",
      "Epoch [1] - 44.69 seconds - Train Loss: 0.141636 - Val Loss: 0.140529 - Accuracy: 0.963 - Balanced Accuracy: 0.953 - Val F1 Score: 0.966\n",
      "Epoch [2] - 67.04 seconds - Train Loss: 0.154846 - Val Loss: 0.139303 - Accuracy: 0.958 - Balanced Accuracy: 0.960 - Val F1 Score: 0.960\n",
      "Epoch [3] - 87.47 seconds - Train Loss: 0.172256 - Val Loss: 0.275106 - Accuracy: 0.958 - Balanced Accuracy: 0.951 - Val F1 Score: 0.961\n",
      "Epoch [4] - 108.43 seconds - Train Loss: 0.137846 - Val Loss: 0.209730 - Accuracy: 0.971 - Balanced Accuracy: 0.966 - Val F1 Score: 0.972\n",
      "Epoch [5] - 130.29 seconds - Train Loss: 0.164581 - Val Loss: 0.103233 - Accuracy: 0.959 - Balanced Accuracy: 0.964 - Val F1 Score: 0.964\n",
      "Epoch [6] - 151.15 seconds - Train Loss: 0.074198 - Val Loss: 0.069991 - Accuracy: 0.972 - Balanced Accuracy: 0.972 - Val F1 Score: 0.974\n",
      "Epoch [7] - 172.86 seconds - Train Loss: 0.059547 - Val Loss: 0.064341 - Accuracy: 0.983 - Balanced Accuracy: 0.977 - Val F1 Score: 0.983\n",
      "Epoch [8] - 196.03 seconds - Train Loss: 0.056855 - Val Loss: 0.044555 - Accuracy: 0.987 - Balanced Accuracy: 0.988 - Val F1 Score: 0.987\n",
      "Epoch [9] - 216.60 seconds - Train Loss: 0.123494 - Val Loss: 0.115026 - Accuracy: 0.979 - Balanced Accuracy: 0.963 - Val F1 Score: 0.980\n"
     ]
    }
   ],
   "execution_count": null,
   "source": [
    "# Cross-validation\n",
    "kf = StratifiedKFold(n_splits=5, random_state=seed, shuffle=True)\n",
    "\n",
    "filepath = \".\"\n",
    "\n",
    "# Hyperparameters\n",
    "batch_sizes = [512, 1024]\n",
    "hidden_sizes = [64, 128]\n",
    "batch_norm_list = [False, True]\n",
    "drop = [0, 1]\n",
    "depths = [2, 4]\n",
    "num_epochs = 100\n",
    "learning_rate = 0.01\n",
    "gammas = [0.5, 0.9]\n",
    "step_size = 10\n",
    "\n",
    "hyperparameters = list(itertools.product(batch_sizes, hidden_sizes, depths, gammas, batch_norm_list, drop))\n",
    "n_comb = len(hyperparameters)\n",
    "print(f'Number of hyperparameter combinations: {n_comb}')\n",
    "current_iter = 0\n",
    "\n",
    "# grid search loop\n",
    "for batch_size, hidden_size, depth, gamma, batch_norm, drop in hyperparameters:\n",
    "    # Fix the random seed for reproducibility\n",
    "    fix_random(seed)\n",
    "    start = time.time()\n",
    "\n",
    "    # Create a log name for TensorBoard\n",
    "    log_name = \"B\" + str(batch_size) + \"-dim\" + str(hidden_size) + \"-dp\" + str(depth) + \"-ep\" + str(num_epochs) + \"-lr\" + str(learning_rate) + \"-steplr\" + str(step_size) + \"-gamma\" + str(gamma) + \"-BN\" + str(batch_norm) + \"-drop\" + str(drop)\n",
    "    print(f'Iteration {current_iter + 1}/{n_comb}', log_name)\n",
    "\n",
    "    # Start TensorBoard writer\n",
    "    writer = SummaryWriter('runs/' + log_name)\n",
    "    accuracy_per_fold = []\n",
    "    balanced_accuracy_score_per_fold = []\n",
    "    f1_score_per_fold = []\n",
    "    best_loss_per_fold = []\n",
    "\n",
    "    fold = 1\n",
    "    for train_index, val_index in kf.split(X, y):\n",
    "        # Split the data into training and validation sets for the current fold\n",
    "        X_train_fold, X_val_fold = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train_fold, y_val_fold = y[train_index], y[val_index]\n",
    "\n",
    "        # Preprocess the data\n",
    "        X_train_fold, X_val_fold, cat_idxs_fold, cat_dims_fold, num_idxs_fold, numeric_columns_number_fold = preprocess(X_train_fold, X_val_fold)\n",
    "\n",
    "        # Create datasets for the current fold\n",
    "        train_dataset_fold, val_dataset_fold = create_datasets(X_train_fold, y_train_fold, X_val_fold, y_val_fold, cat_idxs_fold, num_idxs_fold)\n",
    "\n",
    "        # Create data loaders for the current fold\n",
    "        train_loader_fold = DataLoader(train_dataset_fold, batch_size=batch_size, shuffle=True)\n",
    "        val_loader_fold = DataLoader(val_dataset_fold, batch_size=batch_size)\n",
    "\n",
    "        # Define the model architecture\n",
    "        model = FeedForwardPlus(cat_dims_fold, numeric_columns_number_fold, len(cat_dims_fold), num_classes, hidden_size, depth, batch_norm=batch_norm, drop=drop)\n",
    "        model.to(device)\n",
    "\n",
    "        # Compute class weights for the current fold\n",
    "        class_weights_fold = compute_class_weight(class_weight='balanced', classes=np.unique(y_train_fold), y=y_train_fold)\n",
    "        class_weights_fold = dict(enumerate(class_weights_fold))\n",
    "\n",
    "        # Define the loss function and optimizer\n",
    "        criterion = torch.nn.CrossEntropyLoss(weight=torch.tensor(list(class_weights_fold.values()), dtype=torch.float32).to(device))\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "        # Train the model\n",
    "        model, best_valid_loss = train_model(model, criterion, optimizer, num_epochs, scheduler, train_loader_fold, val_loader_fold, device, writer, log_name)\n",
    "\n",
    "        # Evaluate the model on the validation set\n",
    "        y_true, y_pred_c, y_pred = test_model(model, val_loader_fold, device)\n",
    "        perf = evaluate_model(y_true.detach().cpu().numpy(), y_pred_c.detach().cpu().numpy())\n",
    "\n",
    "        print(f\"Fold {fold} - Accuracy: {perf['acc']:.2f}%\")\n",
    "        print(f\"Fold {fold} - Balanced Accuracy: {perf['bacc']:.2f}%\")\n",
    "        print(f\"Fold {fold} - F1 Score: {perf['f1']:.2f}%\")\n",
    "\n",
    "        # Store performance metrics for the current fold\n",
    "        accuracy_per_fold.append(perf[\"acc\"])\n",
    "        balanced_accuracy_score_per_fold.append(perf[\"bacc\"])\n",
    "        f1_score_per_fold.append(perf[\"f1\"])\n",
    "        best_loss_per_fold.append(best_valid_loss)\n",
    "        fold += 1\n",
    "\n",
    "    # Print cross-validation results\n",
    "    print(\"Cross-Validation Results:\")\n",
    "    print(f\"Average Accuracy: {np.mean(accuracy_per_fold) * 100:.2f}%\")\n",
    "    print(f\"Standard Deviation of Accuracy: {np.std(accuracy_per_fold) * 100:.2f}%\")\n",
    "    print(f\"Average Balanced Accuracy: {np.mean(balanced_accuracy_score_per_fold) * 100:.2f}%\")\n",
    "    print(f\"Standard Deviation of Balanced Accuracy: {np.std(balanced_accuracy_score_per_fold) * 100:.2f}%\")\n",
    "    print(f\"Average F1 Score: {np.mean(f1_score_per_fold) * 100:.2f}%\")\n",
    "    print(f\"Standard Deviation of F1 Score: {np.std(f1_score_per_fold) * 100:.2f}%\")\n",
    "\n",
    "    # Log hyperparameters and metrics to TensorBoard\n",
    "    writer.add_hparams(\n",
    "        {\n",
    "            'hparam/bsize': batch_size,\n",
    "            'hparam/hidden size': hidden_size,\n",
    "            'hparam/depth': depth + 2,\n",
    "            'hparam/scheduler': gamma,\n",
    "            'hparam/batch norm': batch_norm,\n",
    "            'hparam/dropout': drop\n",
    "        },\n",
    "        {\n",
    "            'Best Loss': np.mean(best_loss_per_fold),\n",
    "            'Avg Accuracy': np.mean(accuracy_per_fold),\n",
    "            'Std Accuracy': np.std(accuracy_per_fold),\n",
    "            'Avg Balanced Accuracy': np.mean(balanced_accuracy_score_per_fold),\n",
    "            'Std Balanced Accuracy': np.std(balanced_accuracy_score_per_fold),\n",
    "            'Avg F1 score': np.mean(f1_score_per_fold),\n",
    "            'Std F1 score': np.std(f1_score_per_fold)\n",
    "        }\n",
    "    )\n",
    "    writer.flush()\n",
    "    print(\"Time elapsed:\", time.time() - start)\n",
    "    current_iter += 1\n",
    "    writer.close()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Run Tensorboard from the command line:\n",
    "\n",
    "\"tensorboard --logdir runs/\""
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training final model"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Define weights for unbalanced classes"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-31T12:35:08.166358Z",
     "start_time": "2025-01-31T12:35:08.091220Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "print(class_weights)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 4.069469865611345, 1: 0.3381003918130257, 2: 1.132545546326465, 3: 4.543735616312253, 4: 98.7172, 5: 2.9863625363020327, 6: 1.1966881637007225, 7: 63.85329883570505, 8: 0.28803534018428717, 9: 0.9751965859248429}\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Train"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-31T12:48:06.418819Z",
     "start_time": "2025-01-31T12:36:37.090147Z"
    }
   },
   "cell_type": "code",
   "source": [
    "start = time.time()\n",
    "# Hyperparatemeters\n",
    "batch_size = 512\n",
    "depth = 2\n",
    "hidden_size = 128\n",
    "batch_norm = True\n",
    "drop = 1\n",
    "num_epochs = 100\n",
    "learning_rate = 0.01\n",
    "gamma = 0.5\n",
    "step_size = 10\n",
    "\n",
    "log_name = \"B\" + str(batch_size) + \"-dim\" + str(hidden_size) + \"-dp\" + str(depth) + \"-ep\" + str(num_epochs) + \"-lr\" + str(learning_rate) + \"-steplr\" + str(step_size) + \"-gamma\" + str(gamma) + \"-BN\" + str(batch_norm) + \"-drop\" + str(drop)\n",
    "\n",
    "# fix the seed for reproducibility\n",
    "fix_random(seed)\n",
    "\n",
    "# Create the dataset for training and validation\n",
    "train_dataset, val_dataset = create_datasets(X_train, y_train, X_val, y_val, cat_idxs, num_idxs)\n",
    "\n",
    "# Create relative dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# Define the architecture, loss and optimizer\n",
    "model = FeedForwardPlus(cat_dims, numeric_columns_number, len(cat_dims), num_classes, hidden_size, depth, batch_norm=batch_norm, drop=drop)\n",
    "print(model)\n",
    "model.to(device)\n",
    "\n",
    "# Define the training elements\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=torch.tensor(list(class_weights.values()), dtype=torch.float32).to(device))\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "# Start tensorboard\n",
    "writer = SummaryWriter('runs/' + log_name)\n",
    "\n",
    "# Train the model\n",
    "model, best_valid_loss = train_model(model, criterion, optimizer, num_epochs, scheduler, train_loader, val_loader, device, writer, log_name)\n",
    "\n",
    "y_true, y_pred_c, y_pred = test_model(model, val_loader, device)\n",
    "perf = evaluate_model(y_true.detach().cpu().numpy(), y_pred_c.detach().cpu().numpy())\n",
    "\n",
    "print(f\"Accuracy: {perf['acc']:.2f}%\")\n",
    "print(f\"Balanced Accuracy: {perf['bacc']:.2f}%\")\n",
    "print(f\"F1 Score: {perf['f1']:.2f}%\")\n",
    "\n",
    "wrapped_model = PyTorchFeedForwardWrapper(model, cat_idxs, num_idxs)\n",
    "\n",
    "# Save the model\n",
    "with open(f\"{filepath}/models/ff.save\", 'wb') as f:\n",
    "    pickle.dump(wrapped_model, f)\n",
    "\n",
    "writer.add_hparams(\n",
    "    {\n",
    "        'hparam/bsize': batch_size,\n",
    "        'hparam/hidden size': hidden_size,\n",
    "        'hparam/depth': depth + 2,\n",
    "        'hparam/scheduler': gamma,\n",
    "        'hparam/batch norm': batch_norm\n",
    "    },\n",
    "    {\n",
    "        'Best loss': best_valid_loss,\n",
    "        'Accuracy': perf['acc'],\n",
    "        'Balanced accuracy': perf['bacc'],\n",
    "        'F1 score': perf['f1']\n",
    "    }\n",
    ")\n",
    "\n",
    "# Close tensorboard writer after a training\n",
    "writer.flush()\n",
    "writer.close()\n",
    "\n",
    "# Save timestamp\n",
    "end = time.time()\n",
    "print(\"Time elapsed:\", end - start)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeedForwardPlus(\n",
      "  (embeddings): ModuleList(\n",
      "    (0): Embedding(14, 8)\n",
      "    (1-3): 3 x Embedding(4, 8)\n",
      "    (4): Embedding(1875, 8)\n",
      "    (5): Embedding(4, 8)\n",
      "    (6): Embedding(1493, 8)\n",
      "    (7-8): 2 x Embedding(4, 8)\n",
      "    (9): Embedding(6, 8)\n",
      "    (10): Embedding(3, 8)\n",
      "    (11): Embedding(49, 8)\n",
      "    (12): Embedding(103, 8)\n",
      "    (13): Embedding(3, 8)\n",
      "    (14): Embedding(4, 8)\n",
      "    (15): Embedding(21, 8)\n",
      "    (16): Embedding(675, 8)\n",
      "    (17): Embedding(7, 8)\n",
      "    (18): Embedding(4, 8)\n",
      "    (19): Embedding(3, 8)\n",
      "    (20): Embedding(4, 8)\n",
      "    (21): Embedding(3, 8)\n",
      "    (22): Embedding(6, 8)\n",
      "    (23): Embedding(5, 8)\n",
      "    (24): Embedding(12, 8)\n",
      "    (25): Embedding(3, 8)\n",
      "  )\n",
      "  (numerical_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=224, out_features=128, bias=True)\n",
      "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (output): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n",
      "Epoch [0] - 20.06 seconds - Train Loss: 0.140304 - Val Loss: 0.102848 - Accuracy: 0.977 - Balanced Accuracy: 0.961 - Val F1 Score: 0.977\n",
      "Epoch [1] - 39.10 seconds - Train Loss: 0.071241 - Val Loss: 0.154350 - Accuracy: 0.967 - Balanced Accuracy: 0.953 - Val F1 Score: 0.966\n",
      "Epoch [2] - 58.09 seconds - Train Loss: 0.046864 - Val Loss: 0.163595 - Accuracy: 0.952 - Balanced Accuracy: 0.945 - Val F1 Score: 0.950\n",
      "Epoch [3] - 77.67 seconds - Train Loss: 0.056064 - Val Loss: 0.063587 - Accuracy: 0.976 - Balanced Accuracy: 0.976 - Val F1 Score: 0.977\n",
      "Epoch [4] - 96.80 seconds - Train Loss: 0.044867 - Val Loss: 0.042912 - Accuracy: 0.986 - Balanced Accuracy: 0.988 - Val F1 Score: 0.987\n",
      "Epoch [5] - 117.00 seconds - Train Loss: 0.034564 - Val Loss: 0.034844 - Accuracy: 0.988 - Balanced Accuracy: 0.989 - Val F1 Score: 0.988\n",
      "Epoch [6] - 137.90 seconds - Train Loss: 0.036710 - Val Loss: 0.097654 - Accuracy: 0.965 - Balanced Accuracy: 0.971 - Val F1 Score: 0.966\n",
      "Epoch [7] - 158.98 seconds - Train Loss: 0.033058 - Val Loss: 0.038992 - Accuracy: 0.990 - Balanced Accuracy: 0.988 - Val F1 Score: 0.990\n",
      "Epoch [8] - 181.66 seconds - Train Loss: 0.034130 - Val Loss: 0.087516 - Accuracy: 0.983 - Balanced Accuracy: 0.972 - Val F1 Score: 0.983\n",
      "Epoch [9] - 205.61 seconds - Train Loss: 0.030963 - Val Loss: 0.143812 - Accuracy: 0.965 - Balanced Accuracy: 0.958 - Val F1 Score: 0.966\n",
      "Epoch [10] - 226.63 seconds - Train Loss: 0.032485 - Val Loss: 0.102683 - Accuracy: 0.974 - Balanced Accuracy: 0.966 - Val F1 Score: 0.975\n",
      "Epoch [11] - 248.36 seconds - Train Loss: 0.026827 - Val Loss: 0.031828 - Accuracy: 0.990 - Balanced Accuracy: 0.990 - Val F1 Score: 0.990\n",
      "Epoch [12] - 268.02 seconds - Train Loss: 0.022179 - Val Loss: 0.029759 - Accuracy: 0.989 - Balanced Accuracy: 0.992 - Val F1 Score: 0.989\n",
      "Epoch [13] - 288.59 seconds - Train Loss: 0.022342 - Val Loss: 0.032808 - Accuracy: 0.991 - Balanced Accuracy: 0.992 - Val F1 Score: 0.991\n",
      "Epoch [14] - 314.21 seconds - Train Loss: 0.022402 - Val Loss: 0.073693 - Accuracy: 0.981 - Balanced Accuracy: 0.979 - Val F1 Score: 0.981\n",
      "Epoch [15] - 334.80 seconds - Train Loss: 0.025235 - Val Loss: 0.036445 - Accuracy: 0.993 - Balanced Accuracy: 0.990 - Val F1 Score: 0.993\n",
      "Epoch [16] - 356.30 seconds - Train Loss: 0.020866 - Val Loss: 0.035295 - Accuracy: 0.992 - Balanced Accuracy: 0.989 - Val F1 Score: 0.992\n",
      "Epoch [17] - 376.13 seconds - Train Loss: 0.022376 - Val Loss: 0.038852 - Accuracy: 0.984 - Balanced Accuracy: 0.990 - Val F1 Score: 0.985\n",
      "Epoch [18] - 397.88 seconds - Train Loss: 0.020582 - Val Loss: 0.030498 - Accuracy: 0.990 - Balanced Accuracy: 0.990 - Val F1 Score: 0.990\n",
      "Epoch [19] - 418.47 seconds - Train Loss: 0.021031 - Val Loss: 0.035751 - Accuracy: 0.988 - Balanced Accuracy: 0.989 - Val F1 Score: 0.989\n",
      "Epoch [20] - 438.51 seconds - Train Loss: 0.017832 - Val Loss: 0.029779 - Accuracy: 0.991 - Balanced Accuracy: 0.992 - Val F1 Score: 0.991\n",
      "Epoch [21] - 461.66 seconds - Train Loss: 0.016310 - Val Loss: 0.030980 - Accuracy: 0.994 - Balanced Accuracy: 0.992 - Val F1 Score: 0.994\n",
      "Epoch [22] - 482.19 seconds - Train Loss: 0.016812 - Val Loss: 0.043034 - Accuracy: 0.985 - Balanced Accuracy: 0.983 - Val F1 Score: 0.986\n",
      "Epoch [23] - 501.51 seconds - Train Loss: 0.016729 - Val Loss: 0.036207 - Accuracy: 0.992 - Balanced Accuracy: 0.991 - Val F1 Score: 0.992\n",
      "Epoch [24] - 521.15 seconds - Train Loss: 0.016058 - Val Loss: 0.037029 - Accuracy: 0.992 - Balanced Accuracy: 0.990 - Val F1 Score: 0.992\n",
      "Epoch [25] - 541.72 seconds - Train Loss: 0.015604 - Val Loss: 0.036055 - Accuracy: 0.990 - Balanced Accuracy: 0.991 - Val F1 Score: 0.990\n",
      "Epoch [26] - 562.90 seconds - Train Loss: 0.017184 - Val Loss: 0.035510 - Accuracy: 0.993 - Balanced Accuracy: 0.991 - Val F1 Score: 0.993\n",
      "Epoch [27] - 583.21 seconds - Train Loss: 0.015743 - Val Loss: 0.040146 - Accuracy: 0.992 - Balanced Accuracy: 0.991 - Val F1 Score: 0.992\n",
      "Epoch [28] - 604.54 seconds - Train Loss: 0.015795 - Val Loss: 0.038689 - Accuracy: 0.990 - Balanced Accuracy: 0.991 - Val F1 Score: 0.990\n",
      "Epoch [29] - 625.90 seconds - Train Loss: 0.015217 - Val Loss: 0.042971 - Accuracy: 0.993 - Balanced Accuracy: 0.992 - Val F1 Score: 0.993\n",
      "Epoch [30] - 646.02 seconds - Train Loss: 0.014524 - Val Loss: 0.037697 - Accuracy: 0.992 - Balanced Accuracy: 0.992 - Val F1 Score: 0.992\n",
      "Epoch [31] - 665.21 seconds - Train Loss: 0.014117 - Val Loss: 0.036070 - Accuracy: 0.993 - Balanced Accuracy: 0.992 - Val F1 Score: 0.993\n",
      "Epoch [32] - 685.12 seconds - Train Loss: 0.013532 - Val Loss: 0.035358 - Accuracy: 0.993 - Balanced Accuracy: 0.992 - Val F1 Score: 0.993\n",
      "Time elapsed: 689.3198843002319\n"
     ]
    }
   ],
   "execution_count": 16
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "da",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
