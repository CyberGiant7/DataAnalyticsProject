{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to install some packages (pip install \"package\"):\n",
    "- matplotlib\n",
    "- numpy\n",
    "- scikit-learn\n",
    "- tensorboard\n",
    "- torch"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-19T12:27:58.873276Z",
     "start_time": "2025-01-19T12:27:58.868611Z"
    }
   },
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "import time"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-19T12:27:19.784403Z",
     "start_time": "2025-01-19T12:27:19.778884Z"
    }
   },
   "source": [
    "# For reproducibility\n",
    "def fix_random(seed: int) -> None:\n",
    "    \"\"\"Fix all the possible sources of randomness.\n",
    "\n",
    "    Args:\n",
    "        seed: the seed to use. \n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True  # slower\n",
    "\n",
    "\n",
    "seed = 42"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-19T12:27:20.450627Z",
     "start_time": "2025-01-19T12:27:20.446117Z"
    }
   },
   "source": [
    "# Define the Data Layer\n",
    "class TabDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x_cat, x_num, y):\n",
    "        self.x_cat = x_cat\n",
    "        self.x_num = x_num\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x_cat[idx], self.x_num[idx], self.y[idx]\n"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-19T12:27:26.603100Z",
     "start_time": "2025-01-19T12:27:20.462068Z"
    }
   },
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "class CustomOrdinalEncoder(OrdinalEncoder):\n",
    "    def transform(self, X):\n",
    "        encoded = super().transform(X)\n",
    "        # Shift all values by +1 and replace unknown_value (-1) with 0\n",
    "        return np.where(encoded == -1, 0, encoded + 1)\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        # Handle the inverse transform to account for the +1 offset\n",
    "        X = np.where(X == 0, -1, X - 1)\n",
    "        return super().inverse_transform(X)\n",
    "\n",
    "\n",
    "# define a function with different normalization and scaling techniques\n",
    "def preprocess(X_train, X_val):\n",
    "    categorical_columns = X_train.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "    # print(\"categorical_columns len: \", len(categorical_columns))\n",
    "    numeric_columns = X_train.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "    # print(\"numeric_columns len: \", len(numeric_columns))\n",
    "\n",
    "    ct = ColumnTransformer(\n",
    "        [\n",
    "            (\"cat\", CustomOrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1), categorical_columns),  # Trasforma le colonne categoriche\n",
    "            # (\"ordinal\", OneHotEncoder(handle_unknown='infrequent_if_exist', sparse_output=False), categorical_columns),  # Trasforma le colonne categoriche\n",
    "            (\"scale\", StandardScaler(), numeric_columns)  # Normalizza le colonne numeriche\n",
    "        ],\n",
    "        remainder=\"passthrough\"  # Mantieni le altre colonne invariate\n",
    "    )\n",
    "    ct.set_output(transform=\"pandas\")\n",
    "\n",
    "    ct = ct.fit(X_train)\n",
    "    with open(f\"{filepath}/transformer/transformer_ff.save\", \"wb\") as f:\n",
    "        pickle.dump(ct, f)\n",
    "\n",
    "    # train set\n",
    "    X_train = ct.transform(X_train)\n",
    "\n",
    "    cat_idxs = [i for i, f in enumerate(X_train.columns) if \"cat__\" in f]\n",
    "    cat_dims = [len(X_train[f].unique()) + 1 for i, f in enumerate(X_train.columns) if \"cat__\" in f]\n",
    "    num_idxs = [i for i, f in enumerate(X_train.columns) if \"scale__\" in f]\n",
    "    numeric_columns_number = len(num_idxs)\n",
    "\n",
    "    X_train = X_train.to_numpy()\n",
    "    # validation set\n",
    "    X_val = ct.transform(X_val).to_numpy()\n",
    "\n",
    "    return X_train, X_val, cat_idxs, cat_dims, num_idxs, numeric_columns_number"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-19T12:27:26.623324Z",
     "start_time": "2025-01-19T12:27:26.611623Z"
    }
   },
   "source": [
    "# Architecture\n",
    "\n",
    "class FeedForwardPlus(nn.Module):\n",
    "    def __init__(self, cat_dims, num_numerical, num_categorical, num_classes, hidden_size, depth=1, batch_norm=False, drop=0, dim_embedding=8, ):\n",
    "        super(FeedForwardPlus, self).__init__()\n",
    "\n",
    "        self.embeddings = nn.ModuleList([\n",
    "            nn.Embedding(cat_dim, dim_embedding) for cat_dim in cat_dims\n",
    "        ])\n",
    "\n",
    "        self.numerical_norm = nn.LayerNorm(num_numerical)\n",
    "\n",
    "        model = []\n",
    "        model += [nn.Linear(num_categorical * dim_embedding + num_numerical, hidden_size)]\n",
    "        if batch_norm:\n",
    "            model += [nn.BatchNorm1d(hidden_size)]\n",
    "        model += [nn.ReLU()]\n",
    "\n",
    "        block = [\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU()\n",
    "        ]\n",
    "\n",
    "        block_batch_norm = [\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.ReLU()\n",
    "        ]\n",
    "\n",
    "        block_dropout = [\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU()\n",
    "        ]\n",
    "\n",
    "        for i in range(depth):\n",
    "            if not batch_norm and drop == 0:\n",
    "                model += block\n",
    "            elif batch_norm and drop == 0:\n",
    "                model += block_batch_norm\n",
    "            elif drop > 0 and not batch_norm:\n",
    "                model += block_dropout\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "        self.output = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x_cat, x_num):\n",
    "        x_cat = x_cat.long()\n",
    "        cat_embeddings = [emb(x_cat[:, i]) for i, emb in enumerate(self.embeddings)]\n",
    "        cat_embeddings = torch.stack(cat_embeddings, dim=1)  # (batch_size, num_categorical_features, dim_embedding)\n",
    "        cat_embeddings = cat_embeddings.view(cat_embeddings.size(0), -1)  # Flatten per concatenare\n",
    "\n",
    "        x_num = self.numerical_norm(x_num)\n",
    "        x = torch.cat([cat_embeddings, x_num], dim=1)\n",
    "        h = self.model(x)\n",
    "        out = self.output(h)\n",
    "        return out\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class PyTorchTabTransformer:\n",
    "    def __init__(self, model, cat_idx, num_idx, device='cpu'):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.model.to(self.device)\n",
    "        self.cat_idx = cat_idx\n",
    "        self.num_idx = num_idx\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Esegue le previsioni sul modello PyTorch.\n",
    "        \"\"\"\n",
    "        self.model.eval()  # Modalità di valutazione\n",
    "        with torch.no_grad():\n",
    "            # Controlla se X è un array numpy e convertilo in un tensore PyTorch\n",
    "            if isinstance(X, np.ndarray):\n",
    "                X = torch.tensor(X, dtype=torch.float32).to(self.device)\n",
    "\n",
    "            # Supponi che X sia diviso in categoriale e numerico\n",
    "            y_pred = self.model(X[:, self.cat_idx].long(),\n",
    "                                X[:, self.num_idx])\n",
    "            return torch.argmax(y_pred, dim=1).cpu().numpy()"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-19T12:27:26.640583Z",
     "start_time": "2025-01-19T12:27:26.631584Z"
    }
   },
   "source": [
    "# Define a function for the training process\n",
    "\n",
    "def train_model(model: FeedForwardPlus, criterion, optimizer, epoch, scheduler, train_loader, val_loader, device, writer, log_name=\"model\"):\n",
    "    n_iter = 0\n",
    "    best_valid_loss = float('inf')\n",
    "    patience = 20\n",
    "    epochs_since_last_improvement = 0\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    for epoch in range(epoch):\n",
    "        model.train()\n",
    "\n",
    "        start_epoch = time.time()\n",
    "        loss_train = 0\n",
    "\n",
    "        for x_cat, x_num, targets in train_loader:\n",
    "            x_cat, x_num, targets = x_cat.to(device), x_num.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            y_pred = model(x_cat, x_num)\n",
    "\n",
    "            # Compute Loss\n",
    "            loss = criterion(y_pred, targets)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            n_iter += 1\n",
    "            loss_train += loss.item()\n",
    "\n",
    "        loss_train /= len(train_loader)\n",
    "        writer.add_scalar(\"Metrics/Loss/train\", loss_train, epoch)\n",
    "\n",
    "        y_true, y_pred_c, y_pred = test_model(model, val_loader, device)\n",
    "        loss_val = criterion(y_pred, y_true).item()\n",
    "        writer.add_scalar(\"Metrics/Loss/val\", loss_val, epoch)\n",
    "\n",
    "        perf = evaluate_model(y_true.detach().cpu().numpy(), y_pred_c.detach().cpu().numpy())\n",
    "\n",
    "        print(f'Epoch [{epoch}] - {time.time() - start:.2f} seconds - Train Loss: {loss_train:.6f} - Val Loss: {loss_val:.6f} - Accuracy: {perf[\"acc\"]:.3f} - Balanced Accuracy: {perf[\"bacc\"]:.3f} - Val F1 Score: {perf[\"f1\"]:.3f}')\n",
    "        writer.add_scalar(\"Metrics/Accuracy/val\", perf[\"acc\"], epoch)\n",
    "        writer.add_scalar(\"Metrics/Balanced Accuracy/val\", perf[\"bacc\"], epoch)\n",
    "        writer.add_scalar(\"Metrics/F1 Score/val\", perf[\"f1\"], epoch)\n",
    "\n",
    "        threshold = 0.01\n",
    "        # save best model\n",
    "        if loss_val < best_valid_loss - threshold:\n",
    "            best_valid_loss = loss_val\n",
    "            torch.save(model, f\"{filepath}/models/best_model_ff.save\")\n",
    "            if not os.path.exists('models'):\n",
    "                os.makedirs('models')\n",
    "            # with open(f\"{filepath}/models/model_best_tf_custom.save\", \"wb\") as f:\n",
    "            #     wrapped_model = PyTorchTabTransformer(model, cat_idxs, num_idxs, device)\n",
    "            #     pickle.dump(wrapped_model, f)\n",
    "            epochs_since_last_improvement = 0\n",
    "        elif epochs_since_last_improvement >= patience:\n",
    "            break\n",
    "        else:\n",
    "            epochs_since_last_improvement += 1\n",
    "\n",
    "        writer.add_scalar(\"hparam/Learning Rate\", scheduler.get_last_lr()[0], epoch)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    best_model = torch.load(f\"{filepath}/models/best_model_ff.save\")\n",
    "    return best_model, best_valid_loss"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-19T12:27:26.655432Z",
     "start_time": "2025-01-19T12:27:26.648279Z"
    }
   },
   "source": [
    "from sklearn.metrics import balanced_accuracy_score, f1_score, accuracy_score\n",
    "\n",
    "\n",
    "# Define a function to evaluate the performance on validation and test sets\n",
    "\n",
    "def test_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    y_test = []\n",
    "\n",
    "    for x_cat, x_num, targets in data_loader:\n",
    "        x_cat, x_num, targets = x_cat.to(device), x_num.to(device), targets.to(device)\n",
    "        y_pred += model(x_cat, x_num)\n",
    "        #print(y_pred)\n",
    "        y_test += targets\n",
    "        #print(targets)\n",
    "\n",
    "    y_test = torch.stack(y_test).squeeze()\n",
    "    y_pred = torch.stack(y_pred).squeeze()\n",
    "    y_pred_c = y_pred.argmax(dim=1, keepdim=True).squeeze()\n",
    "\n",
    "    return y_test, y_pred_c, y_pred\n",
    "\n",
    "\n",
    "def evaluate_model(y_test, y_pred):\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    bacc = balanced_accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "    return {\"acc\": acc, \"bacc\": bacc, \"f1\": f1}"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-19T12:27:26.717663Z",
     "start_time": "2025-01-19T12:27:26.663305Z"
    }
   },
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print('Using device:', device)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-19T12:27:37.323655Z",
     "start_time": "2025-01-19T12:27:26.727785Z"
    }
   },
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, StratifiedGroupKFold\n",
    "\n",
    "save_in_test_folder = True\n",
    "if save_in_test_folder:\n",
    "    filepath = \"../TestModule\"\n",
    "else:\n",
    "    filepath = \".\"\n",
    "\n",
    "FILENAME = \"dataset/train_dataset.csv\"\n",
    "\n",
    "#Prepare train data\n",
    "data = pd.read_csv(FILENAME, sep=\",\", low_memory=False)\n",
    "\n",
    "# get features names\n",
    "features = list(data.columns)\n",
    "# features_to_remove = [\"label\", \"ts\", \"src_ip\", \"dst_ip\", \"dns_query\", \"ssl_subject\", \"ssl_issuer\", \"http_uri\", \"type\", \"http_referrer\", \"http_user_agent\"]\n",
    "features_to_remove = [\"type\", \"label\", \"ts\", \"http_referrer\"]\n",
    "features = [feature for feature in features if feature not in features_to_remove]\n",
    "data = data[features + [\"type\"]]\n",
    "\n",
    "# Converte i valori in numeri, sostituendo quelli non validi con NaN\n",
    "data[\"src_bytes\"] = pd.to_numeric(data[\"src_bytes\"], errors='coerce')\n",
    "# Filtra le righe con NaN (valori non convertibili)\n",
    "data = data.dropna(subset=[\"src_bytes\"])\n",
    "# Converte i valori rimasti in interi\n",
    "data.loc[:, \"src_bytes\"] = data[\"src_bytes\"].astype(int)\n",
    "\n",
    "print(\"#Righe: \" + str(data.shape[0]) + \" #Colonne: \" + str(data.shape[1]))\n",
    "df1 = data.dropna()\n",
    "print(\"#Righe: \" + str(df1.shape[0]) + \" #Colonne: \" + str(data.shape[1]))\n",
    "\n",
    "# data = data.sample(n=1000, random_state=5)\n",
    "\n",
    "X = data[features]\n",
    "y = data[\"type\"]\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y)\n",
    "with open(f\"{filepath}/transformer/target_encoder.save\", \"wb\") as f:\n",
    "    pickle.dump(le, f)\n",
    "\n",
    "num_classes = len(le.classes_)\n",
    "\n",
    "y = le.transform(y)\n",
    "\n",
    "# Separate indices\n",
    "indices = np.arange(X.shape[0])\n",
    "train_idx, val_idx = train_test_split(indices, test_size=0.2, stratify=y, random_state=seed)\n",
    "\n",
    "X_val = X.iloc[val_idx]\n",
    "y_val = y[val_idx]\n",
    "X_train = X.iloc[train_idx]\n",
    "y_train = y[train_idx]\n",
    "\n",
    "X_train, X_val, cat_idxs, cat_dims, num_idxs, numeric_columns_number = preprocess(X_train, X_val)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Righe: 616983 #Colonne: 43\n",
      "#Righe: 616983 #Colonne: 43\n",
      "(493586, 42)\n",
      "(123397, 42)\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Define weights for unbalanced classes"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-19T12:27:37.591645Z",
     "start_time": "2025-01-19T12:27:37.403285Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.utils import compute_class_weight, compute_sample_weight\n",
    "\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "sample_weight = compute_sample_weight(class_weight='balanced', y=y_train)\n",
    "print(class_weights)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 4.069469865611345, 1: 0.3381003918130257, 2: 1.132545546326465, 3: 4.543735616312253, 4: 98.7172, 5: 2.9863625363020327, 6: 1.1966881637007225, 7: 63.85329883570505, 8: 0.28803534018428717, 9: 0.9751965859248429}\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-19T12:27:37.699711Z",
     "start_time": "2025-01-19T12:27:37.599004Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "# filter the categorical and numerical features\n",
    "X_cat_train = X_train_tensor[:, cat_idxs]\n",
    "X_num_train = X_train_tensor[:, num_idxs]\n",
    "\n",
    "X_cat_val = X_val_tensor[:, cat_idxs].long()\n",
    "X_num_val = X_val_tensor[:, num_idxs]\n",
    "\n",
    "# Create the dataset\n",
    "train_dataset = TabDataset(X_cat_train, X_num_train, y_train_tensor)\n",
    "val_dataset = TabDataset(X_cat_val, X_num_val, y_val_tensor)"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Run Tensorboard from the command line:\n",
    "\n",
    "\"tensorboard --logdir runs/\""
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-19T12:18:38.302302Z",
     "start_time": "2025-01-19T12:17:57.922969Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "# Hyperparatemeters\n",
    "batch_size = 1024\n",
    "depth = 4\n",
    "hidden_size = 128\n",
    "batch_norm = True\n",
    "drop = 1\n",
    "num_epochs = 100\n",
    "learning_rate = 0.01\n",
    "gamma = 0.5\n",
    "step_size = 10\n",
    "\n",
    "log_name = \"B\" + str(batch_size) + \"-dim\" + str(hidden_size) + \"-dp\" + str(depth) + \"-ep\" + str(num_epochs) + \"-lr\" + str(learning_rate) + \"-steplr\" + str(step_size) + \"-gamma\" + str(gamma) + \"-BN\" + str(batch_norm) + \"-drop\" + str(drop)\n",
    "\n",
    "# fix the seed for reproducibility\n",
    "fix_random(seed)\n",
    "\n",
    "# Create relative dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# Define the architecture, loss and optimizer\n",
    "model = FeedForwardPlus(cat_dims, numeric_columns_number, len(cat_dims), num_classes, hidden_size, depth, batch_norm=batch_norm, drop=drop)\n",
    "print(model)\n",
    "model.to(device)\n",
    "\n",
    "# Define the training elements\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=torch.tensor(list(class_weights.values()), dtype=torch.float32).to(device))\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "# Start tensorboard\n",
    "writer = SummaryWriter('runs/' + log_name)\n",
    "\n",
    "# Train the model\n",
    "model, best_valid_loss = train_model(model, criterion, optimizer, num_epochs, scheduler, train_loader, val_loader, device, writer, log_name)\n",
    "\n",
    "y_true, y_pred_c, y_pred = test_model(model, val_loader, device)\n",
    "perf = evaluate_model(y_true.detach().cpu().numpy(), y_pred_c.detach().cpu().numpy())\n",
    "\n",
    "writer.add_hparams(\n",
    "    {\n",
    "        'hparam/bsize': batch_size,\n",
    "        'hparam/hidden size': hidden_size,\n",
    "        'hparam/depth': depth + 2,\n",
    "        'hparam/scheduler': gamma,\n",
    "        'hparam/batch norm': batch_norm\n",
    "    },\n",
    "    {\n",
    "        'Best loss': best_valid_loss,\n",
    "        'Accuracy': perf['acc'],\n",
    "        'Balanced accuracy': perf['bacc'],\n",
    "        'F1 score': perf['f1']\n",
    "    }\n",
    ")\n",
    "\n",
    "# Close tensorboard writer after a training\n",
    "writer.flush()\n",
    "writer.close()\n",
    "\n",
    "# Save timestamp\n",
    "end = time.time()\n",
    "print(\"Time elapsed:\", end - start)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeedForwardPlus(\n",
      "  (embeddings): ModuleList(\n",
      "    (0): Embedding(669, 8)\n",
      "    (1): Embedding(1499, 8)\n",
      "    (2): Embedding(4, 8)\n",
      "    (3): Embedding(23, 8)\n",
      "    (4): Embedding(14, 8)\n",
      "    (5): Embedding(1888, 8)\n",
      "    (6-9): 4 x Embedding(4, 8)\n",
      "    (10): Embedding(6, 8)\n",
      "    (11): Embedding(8, 8)\n",
      "    (12-13): 2 x Embedding(4, 8)\n",
      "    (14-15): 2 x Embedding(5, 8)\n",
      "    (16): Embedding(3, 8)\n",
      "    (17): Embedding(4, 8)\n",
      "    (18): Embedding(47, 8)\n",
      "    (19): Embedding(3, 8)\n",
      "    (20): Embedding(102, 8)\n",
      "    (21): Embedding(4, 8)\n",
      "    (22): Embedding(6, 8)\n",
      "    (23): Embedding(13, 8)\n",
      "    (24): Embedding(5, 8)\n",
      "    (25): Embedding(3, 8)\n",
      "  )\n",
      "  (numerical_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=224, out_features=128, bias=True)\n",
      "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (output): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n",
      "Epoch [0] - 16.23 seconds - Train Loss: 0.152607 - Val Loss: 0.096601 - Accuracy: 0.977 - Balanced Accuracy: 0.966 - Val F1 Score: 0.977\n",
      "Epoch [1] - 32.45 seconds - Train Loss: 0.067699 - Val Loss: 0.569083 - Accuracy: 0.908 - Balanced Accuracy: 0.886 - Val F1 Score: 0.879\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[28], line 41\u001B[0m\n\u001B[0;32m     38\u001B[0m writer \u001B[38;5;241m=\u001B[39m SummaryWriter(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mruns/\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m+\u001B[39m log_name)\n\u001B[0;32m     40\u001B[0m \u001B[38;5;66;03m# Train the model\u001B[39;00m\n\u001B[1;32m---> 41\u001B[0m model, best_valid_loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscheduler\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwriter\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlog_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     43\u001B[0m y_true, y_pred_c, y_pred \u001B[38;5;241m=\u001B[39m test_model(model, val_loader, device)\n\u001B[0;32m     44\u001B[0m perf \u001B[38;5;241m=\u001B[39m evaluate_model(y_true\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mnumpy(), y_pred_c\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mnumpy())\n",
      "Cell \u001B[1;32mIn[6], line 22\u001B[0m, in \u001B[0;36mtrain_model\u001B[1;34m(model, criterion, optimizer, epoch, scheduler, train_loader, val_loader, device, writer, log_name)\u001B[0m\n\u001B[0;32m     19\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m     21\u001B[0m \u001B[38;5;66;03m# Forward pass\u001B[39;00m\n\u001B[1;32m---> 22\u001B[0m y_pred \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_cat\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx_num\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     24\u001B[0m \u001B[38;5;66;03m# Compute Loss\u001B[39;00m\n\u001B[0;32m     25\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(y_pred, targets)\n",
      "File \u001B[1;32m~\\PycharmProjects\\DataAnalyticsProject\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\DataAnalyticsProject\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[5], line 50\u001B[0m, in \u001B[0;36mFeedForwardPlus.forward\u001B[1;34m(self, x_cat, x_num)\u001B[0m\n\u001B[0;32m     48\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x_cat, x_num):\n\u001B[0;32m     49\u001B[0m     x_cat \u001B[38;5;241m=\u001B[39m x_cat\u001B[38;5;241m.\u001B[39mlong()\n\u001B[1;32m---> 50\u001B[0m     cat_embeddings \u001B[38;5;241m=\u001B[39m [\u001B[43memb\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_cat\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m i, emb \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membeddings)]\n\u001B[0;32m     51\u001B[0m     cat_embeddings \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mstack(cat_embeddings, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)  \u001B[38;5;66;03m# (batch_size, num_categorical_features, dim_embedding)\u001B[39;00m\n\u001B[0;32m     52\u001B[0m     cat_embeddings \u001B[38;5;241m=\u001B[39m cat_embeddings\u001B[38;5;241m.\u001B[39mview(cat_embeddings\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)  \u001B[38;5;66;03m# Flatten per concatenare\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\DataAnalyticsProject\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\DataAnalyticsProject\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\DataAnalyticsProject\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:163\u001B[0m, in \u001B[0;36mEmbedding.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    162\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 163\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membedding\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    164\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_norm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    165\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnorm_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscale_grad_by_freq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msparse\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\DataAnalyticsProject\\Lib\\site-packages\\torch\\nn\\functional.py:2237\u001B[0m, in \u001B[0;36membedding\u001B[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001B[0m\n\u001B[0;32m   2231\u001B[0m     \u001B[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001B[39;00m\n\u001B[0;32m   2232\u001B[0m     \u001B[38;5;66;03m# XXX: equivalent to\u001B[39;00m\n\u001B[0;32m   2233\u001B[0m     \u001B[38;5;66;03m# with torch.no_grad():\u001B[39;00m\n\u001B[0;32m   2234\u001B[0m     \u001B[38;5;66;03m#   torch.embedding_renorm_\u001B[39;00m\n\u001B[0;32m   2235\u001B[0m     \u001B[38;5;66;03m# remove once script supports set_grad_enabled\u001B[39;00m\n\u001B[0;32m   2236\u001B[0m     _no_grad_embedding_renorm_(weight, \u001B[38;5;28minput\u001B[39m, max_norm, norm_type)\n\u001B[1;32m-> 2237\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membedding\u001B[49m\u001B[43m(\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpadding_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscale_grad_by_freq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msparse\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-01-19T12:29:16.859578Z"
    }
   },
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "\n",
    "# Cross-validation\n",
    "kf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "# Grid Search\n",
    "\n",
    "# Hyperparameters\n",
    "seed = 42\n",
    "batch_sizes = [512, 1024]\n",
    "hidden_sizes = [64, 128]\n",
    "batch_norm_list = [False,True]\n",
    "drop = [0, 1]\n",
    "depths = [2, 4]\n",
    "num_epochs = 100\n",
    "learning_rate = 0.01\n",
    "gammas = [0.5, 0.9]\n",
    "step_size = 10\n",
    "\n",
    "import itertools\n",
    "\n",
    "hyperparameters = list(itertools.product(batch_sizes, hidden_sizes, depths, gammas, batch_norm_list, drop))\n",
    "n_comb = len(hyperparameters)\n",
    "print(f'Number of hyperparameter combinations: {n_comb}')\n",
    "current_iter = 0\n",
    "\n",
    "# grid search loop\n",
    "for batch_size, hidden_size, depth, gamma, batch_norm, drop in hyperparameters:\n",
    "    fix_random(seed)\n",
    "    start = time.time()\n",
    "\n",
    "    log_name = \"B\" + str(batch_size) + \"-dim\" + str(hidden_size) + \"-dp\" + str(depth) + \"-ep\" + str(num_epochs) + \"-lr\" + str(learning_rate) + \"-steplr\" + str(step_size) + \"-gamma\" + str(gamma) + \"-BN\" + str(batch_norm) + \"-drop\" + str(drop)\n",
    "    print(f'Iteration {current_iter + 1}/{n_comb}', log_name)\n",
    "    #start tensorboard\n",
    "    writer = SummaryWriter('runs/' + log_name)\n",
    "    accuracy_per_fold = []\n",
    "    balanced_accuracy_score_per_fold = []\n",
    "    f1_score_per_fold = []\n",
    "    best_loss_per_fold = []\n",
    "\n",
    "    fold = 1\n",
    "    for train_index, val_index in kf.split(X, y):\n",
    "        X_train_fold, X_val_fold = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train_fold, y_val_fold = y[train_index], y[val_index]\n",
    "\n",
    "        X_train_fold, X_val_fold, cat_idxs_fold, cat_dims_fold, num_idxs_fold, numeric_columns_number_fold = preprocess(X_train_fold, X_val_fold)\n",
    "\n",
    "        X_train_tensor_fold = torch.tensor(X_train_fold, dtype=torch.float32)\n",
    "        y_train_tensor_fold = torch.tensor(y_train_fold, dtype=torch.long)\n",
    "\n",
    "        X_val_tensor_fold = torch.tensor(X_val_fold, dtype=torch.float32)\n",
    "        y_val_tensor_fold = torch.tensor(y_val_fold, dtype=torch.long)\n",
    "\n",
    "        # filter the categorical and numerical features\n",
    "        X_cat_train_fold = X_train_tensor_fold[:, cat_idxs_fold]\n",
    "        X_num_train_fold = X_train_tensor_fold[:, num_idxs_fold]\n",
    "\n",
    "        X_cat_val_fold = X_val_tensor_fold[:, cat_idxs_fold].long()\n",
    "        X_num_val_fold = X_val_tensor_fold[:, num_idxs_fold]\n",
    "\n",
    "        train_dataset_fold = TabDataset(X_cat_train_fold, X_num_train_fold, y_train_tensor_fold)\n",
    "        val_dataset_fold = TabDataset(X_cat_val_fold, X_num_val_fold, y_val_tensor_fold)\n",
    "\n",
    "        train_loader_fold = DataLoader(train_dataset_fold, batch_size=batch_size, shuffle=True)\n",
    "        val_loader_fold = DataLoader(val_dataset_fold, batch_size=batch_size)\n",
    "\n",
    "        # define architecture, loss and optimizer\n",
    "        model = FeedForwardPlus(cat_dims_fold, numeric_columns_number_fold, len(cat_dims_fold), num_classes, hidden_size, depth, batch_norm=batch_norm, drop=drop)\n",
    "        model.to(device)\n",
    "\n",
    "        class_weights_fold = compute_class_weight(class_weight='balanced', classes=np.unique(y_train_fold), y=y_train_fold)\n",
    "        class_weights_fold = dict(enumerate(class_weights_fold))\n",
    "\n",
    "        # train\n",
    "        criterion = torch.nn.CrossEntropyLoss(weight=torch.tensor(list(class_weights_fold.values()), dtype=torch.float32).to(device))\n",
    "        # optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "        model, best_valid_loss = train_model(model, criterion, optimizer, num_epochs, scheduler, train_loader_fold, val_loader_fold, device, writer, log_name)\n",
    "\n",
    "        # Valuta il modello sul validation set\n",
    "        y_true, y_pred_c, y_pred = test_model(model, val_loader_fold, device)\n",
    "        perf = evaluate_model(y_true.detach().cpu().numpy(), y_pred_c.detach().cpu().numpy())\n",
    "\n",
    "        print(f\"Fold {fold} - Accuracy: {perf['acc']:.2f}%\")\n",
    "        print(f\"Fold {fold} - Balanced Accuracy: {perf['bacc']:.2f}%\")\n",
    "        print(f\"Fold {fold} - F1 Score: {perf['f1']:.2f}%\")\n",
    "\n",
    "        accuracy_per_fold.append(perf[\"acc\"])\n",
    "        balanced_accuracy_score_per_fold.append(perf[\"bacc\"])\n",
    "        f1_score_per_fold.append(perf[\"f1\"])\n",
    "        best_loss_per_fold.append(best_valid_loss)\n",
    "        fold += 1\n",
    "\n",
    "    # Riassunto dei risultati\n",
    "    print(\"Cross-Validation Results:\")\n",
    "    print(f\"Average Accuracy: {np.mean(accuracy_per_fold) * 100:.2f}%\")\n",
    "    print(f\"Standard Deviation of Accuracy: {np.std(accuracy_per_fold) * 100:.2f}%\")\n",
    "    print(f\"Average Balanced Accuracy: {np.mean(balanced_accuracy_score_per_fold) * 100:.2f}%\")\n",
    "    print(f\"Standard Deviation of Balanced Accuracy: {np.std(balanced_accuracy_score_per_fold) * 100:.2f}%\")\n",
    "    print(f\"Average F1 Score: {np.mean(f1_score_per_fold) * 100:.2f}%\")\n",
    "    print(f\"Standard Deviation of F1 Score: {np.std(f1_score_per_fold) * 100:.2f}%\")\n",
    "\n",
    "    # Close tensorboard writer after a training\n",
    "    # Log hyperparameters and metrics to TensorBoard\n",
    "    writer.add_hparams(\n",
    "        {\n",
    "            'hparam/bsize': batch_size,\n",
    "            'hparam/hidden size': hidden_size,\n",
    "            'hparam/depth': depth + 2,\n",
    "            'hparam/scheduler': gamma,\n",
    "            'hparam/batch norm': batch_norm\n",
    "        },\n",
    "        {\n",
    "            'Best Loss': np.mean(best_loss_per_fold),\n",
    "            'Avg Accuracy': np.mean(accuracy_per_fold),\n",
    "            'Std Accuracy': np.std(accuracy_per_fold),\n",
    "            'Avg Balanced Accuracy': np.mean(balanced_accuracy_score_per_fold),\n",
    "            'Std Balanced Accuracy': np.std(balanced_accuracy_score_per_fold),\n",
    "            'Avg F1 score': np.mean(f1_score_per_fold),\n",
    "            'Std F1 score': np.std(f1_score_per_fold)\n",
    "        }\n",
    "    )\n",
    "    writer.flush()\n",
    "    print(\"Best loss:\", best_valid_loss)\n",
    "    print(\"Time elapsed:\", time.time() - start)\n",
    "    current_iter += 1\n",
    "    writer.close()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of hyperparameter combinations: 64\n",
      "Iteration 1/64 B512-dim64-dp2-ep100-lr0.01-steplr10-gamma0.5-BNFalse-drop0\n",
      "Epoch [0] - 22.78 seconds - Train Loss: 0.270018 - Val Loss: 0.110735 - Accuracy: 0.960 - Balanced Accuracy: 0.955 - Val F1 Score: 0.963\n",
      "Epoch [1] - 44.69 seconds - Train Loss: 0.141636 - Val Loss: 0.140529 - Accuracy: 0.963 - Balanced Accuracy: 0.953 - Val F1 Score: 0.966\n",
      "Epoch [2] - 67.04 seconds - Train Loss: 0.154846 - Val Loss: 0.139303 - Accuracy: 0.958 - Balanced Accuracy: 0.960 - Val F1 Score: 0.960\n",
      "Epoch [3] - 87.47 seconds - Train Loss: 0.172256 - Val Loss: 0.275106 - Accuracy: 0.958 - Balanced Accuracy: 0.951 - Val F1 Score: 0.961\n",
      "Epoch [4] - 108.43 seconds - Train Loss: 0.137846 - Val Loss: 0.209730 - Accuracy: 0.971 - Balanced Accuracy: 0.966 - Val F1 Score: 0.972\n",
      "Epoch [5] - 130.29 seconds - Train Loss: 0.164581 - Val Loss: 0.103233 - Accuracy: 0.959 - Balanced Accuracy: 0.964 - Val F1 Score: 0.964\n",
      "Epoch [6] - 151.15 seconds - Train Loss: 0.074198 - Val Loss: 0.069991 - Accuracy: 0.972 - Balanced Accuracy: 0.972 - Val F1 Score: 0.974\n",
      "Epoch [7] - 172.86 seconds - Train Loss: 0.059547 - Val Loss: 0.064341 - Accuracy: 0.983 - Balanced Accuracy: 0.977 - Val F1 Score: 0.983\n",
      "Epoch [8] - 196.03 seconds - Train Loss: 0.056855 - Val Loss: 0.044555 - Accuracy: 0.987 - Balanced Accuracy: 0.988 - Val F1 Score: 0.987\n",
      "Epoch [9] - 216.60 seconds - Train Loss: 0.123494 - Val Loss: 0.115026 - Accuracy: 0.979 - Balanced Accuracy: 0.963 - Val F1 Score: 0.980\n"
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "da",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
