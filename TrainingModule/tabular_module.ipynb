{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-16T13:59:40.365732Z",
     "start_time": "2025-01-16T13:59:40.351776Z"
    }
   },
   "source": [
    "import copy\n",
    "import itertools\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold, PredefinedSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight, compute_sample_weight\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "\n",
    "def fix_random(seed: int) -> None:\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True  # slower\n",
    "\n",
    "\n",
    "fix_random(SEED)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "# device = torch.device('cpu')\n",
    "\n",
    "print('Using device:', device)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load the dataset",
   "id": "ec20622f418b9959"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T14:00:19.119092Z",
     "start_time": "2025-01-16T14:00:10.948657Z"
    }
   },
   "cell_type": "code",
   "source": [
    "save_in_test_folder = True\n",
    "if save_in_test_folder:\n",
    "    filepath = \"../TestModule\"\n",
    "else:\n",
    "    filepath = \".\"\n",
    "\n",
    "seed = 42\n",
    "FILENAME = \"dataset/train_dataset.csv\"\n",
    "\n",
    "#Prepare train data\n",
    "df1 = pd.read_csv(FILENAME, sep=\",\", low_memory=False)\n",
    "\n",
    "# get features names\n",
    "features = list(df1.columns)\n",
    "features_to_remove = [\"label\", \"ts\", \"src_ip\", \"dst_ip\", \"dns_query\", \"ssl_subject\", \"ssl_issuer\", \"http_uri\", \"type\", \"http_referrer\", \"http_user_agent\"]\n",
    "features = [feature for feature in features if feature not in features_to_remove]\n",
    "df1 = df1[features + [\"type\"]]\n",
    "\n",
    "# Converte i valori in numeri, sostituendo quelli non validi con NaN\n",
    "df1[\"src_bytes\"] = pd.to_numeric(df1[\"src_bytes\"], errors='coerce')\n",
    "# Filtra le righe con NaN (valori non convertibili)\n",
    "df1 = df1.dropna(subset=[\"src_bytes\"])\n",
    "# Converte i valori rimasti in interi\n",
    "df1.loc[:, \"src_bytes\"] = df1[\"src_bytes\"].astype(int)\n",
    "\n",
    "print(\"#Righe: \" + str(df1.shape[0]) + \" #Colonne: \" + str(df1.shape[1]))\n",
    "df1 = df1.dropna()\n",
    "print(\"#Righe: \" + str(df1.shape[0]) + \" #Colonne: \" + str(df1.shape[1]))\n",
    "\n",
    "X = df1[features]\n",
    "y = df1[\"type\"]\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y)\n",
    "with open(f\"{filepath}/transformer/target_encoder.save\", \"wb\") as f:\n",
    "    pickle.dump(le, f)\n",
    "\n",
    "y = le.transform(y)\n",
    "\n",
    "indices = np.arange(X.shape[0])\n",
    "train_idx, val_idx = train_test_split(indices, test_size=0.2, stratify=y, random_state=seed)\n",
    "\n",
    "# fold = np.zeros(X.shape[0])\n",
    "# fold[train_idx] = -1\n",
    "\n",
    "n_splits = 10\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "fold = np.full(len(y), -1)  # Inizializza tutto con -1 (default: train)\n",
    "\n",
    "# Assegna i fold ai campioni\n",
    "for fold_number, (_, val_idx) in enumerate(skf.split(X, y)):\n",
    "    fold[val_idx] = fold_number  # Assegna il numero del fold ai campioni di validazione\n",
    "\n",
    "ps = PredefinedSplit(fold)\n",
    "ps.get_n_splits()\n",
    "\n",
    "# for i, (train_index, test_index) in enumerate(ps.split()):\n",
    "#     print(f\"Fold {i}:\")\n",
    "#     print(f\"  Train: index={train_index}\")\n",
    "#     print(f\"  Test:  index={test_index}\")\n",
    "\n",
    "# take only x with index in val_idx\n",
    "X_val = X.iloc[val_idx]\n",
    "y_val = y[val_idx]\n",
    "X_train = X.iloc[train_idx]\n",
    "y_train = y[train_idx]"
   ],
   "id": "7d988470b826232a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Righe: 616983 #Colonne: 36\n",
      "#Righe: 616983 #Colonne: 36\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Preprocessing",
   "id": "3e1a8dccdb68878f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T14:00:30.285062Z",
     "start_time": "2025-01-16T14:00:19.135744Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "import pickle\n",
    "\n",
    "categorical_columns = X_train.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "numeric_columns = X_train.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "# boolean_columns = X_train.select_dtypes(include=[\"bool\"]).columns.tolist()\n",
    "\n",
    "ct = ColumnTransformer(\n",
    "    [\n",
    "        (\"cat\", OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1), categorical_columns),  # Trasforma le colonne categoriche\n",
    "        # (\"ordinal\", OneHotEncoder(handle_unknown='infrequent_if_exist', sparse_output=False), categorical_columns),  # Trasforma le colonne categoriche\n",
    "        (\"scale\", StandardScaler(), numeric_columns)  # Normalizza le colonne numeriche\n",
    "    ],\n",
    "    remainder=\"passthrough\"  # Mantieni le altre colonne invariate\n",
    ")\n",
    "ct.set_output(transform=\"pandas\")\n",
    "\n",
    "ct = ct.fit(X_train)\n",
    "with open(f\"{filepath}/transformer/transformer.save\", \"wb\") as f:\n",
    "    pickle.dump(ct, f)\n",
    "\n",
    "# train set\n",
    "X_train = ct.transform(X_train)\n",
    "cat_idxs = [i for i, f in enumerate(X_train.columns) if \"cat__\" in f]\n",
    "cat_dims = [len(X_train[f].unique()) for i, f in enumerate(X_train.columns) if \"cat__\" in f]\n",
    "\n",
    "# validation set\n",
    "X_val = ct.transform(X_val)\n",
    "\n",
    "# X\n",
    "X = ct.transform(X)"
   ],
   "id": "ade119398b7aa630",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### DEFINE WEIGHTS FOR UNBALANCED CLASSES",
   "id": "a7b7e07bd23b4deb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T14:00:30.657827Z",
     "start_time": "2025-01-16T14:00:30.353915Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "sample_weight = compute_sample_weight(class_weight='balanced', y=y_train)\n",
    "print(class_weights)"
   ],
   "id": "c102ea88b85aa69d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 4.069469865611345, 1: 0.3381003918130257, 2: 1.132545546326465, 3: 4.543735616312253, 4: 98.7172, 5: 2.9863625363020327, 6: 1.1966881637007225, 7: 63.85329883570505, 8: 0.28803534018428717, 9: 0.9751965859248429}\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# TabNet",
   "id": "44953d4fd06d388"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T13:21:58.791345Z",
     "start_time": "2025-01-16T13:21:58.748159Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from pytorch_tabnet.pretraining import TabNetPretrainer\n",
    "\n",
    "\n",
    "class TabNet(torch.nn.Module):\n",
    "    '''\n",
    "    Wrapper class for TabNetClassifier\n",
    "    '''\n",
    "\n",
    "    def __init__(self, n_d,\n",
    "                 n_a,\n",
    "                 n_steps,\n",
    "                 gamma,\n",
    "                 optimizer_fn,\n",
    "                 n_independent,\n",
    "                 n_shared,\n",
    "                 epsilon,\n",
    "                 seed,\n",
    "                 lambda_sparse,\n",
    "                 clip_value,\n",
    "                 momentum,\n",
    "                 optimizer_params,\n",
    "                 scheduler_params,\n",
    "                 mask_type,\n",
    "                 scheduler_fn,\n",
    "                 device_name,\n",
    "                 output_dim,\n",
    "                 batch_size,\n",
    "                 num_epochs,\n",
    "                 unsupervised_model,\n",
    "                 cat_idxs=None,\n",
    "                 cat_dims=None,\n",
    "                 verbose=0):\n",
    "        super(TabNet, self).__init__()\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.num_epochs = num_epochs\n",
    "        self.unsupervised_model = unsupervised_model\n",
    "        self.network = TabNetClassifier(n_d=n_d,\n",
    "                                        n_a=n_a,\n",
    "                                        n_steps=n_steps,\n",
    "                                        gamma=gamma,\n",
    "                                        optimizer_fn=optimizer_fn,\n",
    "                                        n_independent=n_independent,\n",
    "                                        n_shared=n_shared,\n",
    "                                        epsilon=epsilon,\n",
    "                                        seed=seed,\n",
    "                                        lambda_sparse=lambda_sparse,\n",
    "                                        clip_value=clip_value,\n",
    "                                        momentum=momentum,\n",
    "                                        optimizer_params=optimizer_params,\n",
    "                                        scheduler_params=scheduler_params,\n",
    "                                        mask_type=mask_type,\n",
    "                                        scheduler_fn=scheduler_fn,\n",
    "                                        device_name=device_name,\n",
    "                                        output_dim=output_dim,\n",
    "                                        verbose=verbose,\n",
    "                                        cat_idxs=cat_idxs,\n",
    "                                        cat_dims=cat_dims)\n",
    "\n",
    "    def fit_model(self, X_train, y_train, X_val, y_val, criterion):\n",
    "        self.network.fit(X_train=X_train,\n",
    "                         y_train=y_train,\n",
    "                         eval_set=[(X_train, y_train), (X_val, y_val)],\n",
    "                         eval_metric=['balanced_accuracy'],\n",
    "                         patience=10,\n",
    "                         batch_size=self.batch_size,\n",
    "                         virtual_batch_size=128,\n",
    "                         num_workers=0,\n",
    "                         drop_last=True,\n",
    "                         max_epochs=self.num_epochs,\n",
    "                         loss_fn=criterion,\n",
    "                         from_unsupervised=self.unsupervised_model)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.network.predict(X)\n",
    "\n",
    "    def explain(self, X):\n",
    "        return self.network.explain(X)\n",
    "\n",
    "    def feature_importances(self):\n",
    "        return self.network.feature_importances_\n",
    "\n",
    "\n",
    "def get_unsupervised_model(n_d_a, n_step, n_independent, n_shared, gamma, lr, cat_idxs, cat_dims):\n",
    "    tabnet_params = dict(n_d=n_d_a,\n",
    "                         n_a=n_d_a,\n",
    "                         n_steps=n_step,\n",
    "                         gamma=gamma,\n",
    "                         n_independent=n_independent,\n",
    "                         n_shared=n_shared,\n",
    "                         lambda_sparse=1e-3,\n",
    "                         optimizer_fn=torch.optim.AdamW,\n",
    "                         optimizer_params=dict(lr=lr),\n",
    "                         mask_type=\"sparsemax\",\n",
    "                         verbose=1,\n",
    "                         cat_idxs=cat_idxs,\n",
    "                         cat_dims=cat_dims\n",
    "                         )\n",
    "    return TabNetPretrainer(**tabnet_params)"
   ],
   "id": "eae20e8645f9b130",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### HYPERPARAMETERS CONFIGURATION",
   "id": "c308cb5ac8d1a397"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T14:03:19.326600Z",
     "start_time": "2025-01-16T14:03:19.318087Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nums_epochs = [1000]\n",
    "batch_sizes = [2048]\n",
    "patience = [20]\n",
    "n_d_a = [64]  # 8, 16, 32, 64\n",
    "n_shared = [1]  # 1, 2, 3, 4, 5\n",
    "n_indipendents = [1]  # 1, 2, 3, 4, 5\n",
    "n_steps = [8]  # 3, 4, 5, 6, 7, 8, 9, 10\n",
    "gamma = [1.1]  # 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0\n",
    "epsilon = [1e-15]\n",
    "learning_rate = [0.001]\n",
    "pretraining_ratio = [0.5]\n",
    "momentum = [0.02]\n",
    "hyperparameters = list(itertools.product(nums_epochs, batch_sizes, patience, n_d_a, n_indipendents, n_shared, n_steps, gamma, epsilon, learning_rate, pretraining_ratio, momentum))\n",
    "n_comb = len(hyperparameters)\n",
    "print(f'Number of hyperparameter combinations: {n_comb}')"
   ],
   "id": "7aa9bb2d5efecc44",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of hyperparameter combinations: 1\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training",
   "id": "bcf2b4b95e934fbc"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-01-16T14:03:20.570084Z"
    }
   },
   "cell_type": "code",
   "source": [
    "current_iter = 0\n",
    "best_acc = 0\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=torch.tensor(list(class_weights.values()), dtype=torch.float32).to(device))\n",
    "\n",
    "unsupervised_model = get_unsupervised_model(n_d_a=64, n_step=8, n_independent=1, n_shared=1, gamma=1.1, lr=0.001, cat_idxs=cat_idxs, cat_dims=cat_dims)\n",
    "\n",
    "# unsupervised_model.fit(\n",
    "#     X_train=X_train.values,\n",
    "#     eval_set=[X_val.values],\n",
    "#     max_epochs=1000,\n",
    "#     patience=20,\n",
    "#     batch_size=2048,\n",
    "#     virtual_batch_size=128,\n",
    "#     drop_last=False,\n",
    "#     num_workers=0,\n",
    "#     pretraining_ratio=0.5,\n",
    "#     # weights=sample_weight\n",
    "# )\n",
    "\n",
    "# with open(f\"{filepath}/models/tabnet_unsupervised_model.save\", \"wb\") as f:\n",
    "#     pickle.dump(unsupervised_model, f)\n",
    "\n",
    "with open(f\"{filepath}/models/tabnet_unsupervised_model.save\", \"rb\") as f:\n",
    "    unsupervised_model = pickle.load(f)\n",
    "\n",
    "for num_epochs, batch_size, patience_, n_d, n_i, n_s, n_steps_, gamma_, epsilon_, lr, pretraining_ratio_, moment in hyperparameters:\n",
    "    print(f'Iteration {current_iter + 1}/{n_comb}')\n",
    "    print(f'Hyperparameters: num_epochs={num_epochs}, batch_size={batch_size}, patience={patience_}, n_d={n_d}, n_indipendent={n_i}, n_shared={n_s}, n_steps={n_steps_}, gamma={gamma_}, epsilon={epsilon_}, lr={lr}, pretraining_ratio={pretraining_ratio_}, momentum={moment}')\n",
    "\n",
    "    model = TabNet(n_d=n_d,\n",
    "                   n_a=n_d,\n",
    "                   n_steps=n_steps_,\n",
    "                   gamma=gamma_,\n",
    "                   optimizer_fn=torch.optim.AdamW,\n",
    "                   n_independent=n_i,\n",
    "                   n_shared=n_s,\n",
    "                   epsilon=epsilon_,\n",
    "                   seed=SEED,\n",
    "                   lambda_sparse=1e-4,\n",
    "                   clip_value=1,\n",
    "                   momentum=moment,\n",
    "                   optimizer_params=dict(lr=lr),\n",
    "                   scheduler_params=dict(step_size=10, gamma=0.9),\n",
    "                   mask_type='sparsemax',\n",
    "                   scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "                   device_name=device,\n",
    "                   output_dim=len(np.unique(y_train)),\n",
    "                   batch_size=batch_size,\n",
    "                   num_epochs=num_epochs,\n",
    "                   unsupervised_model=unsupervised_model,\n",
    "                   verbose=1,\n",
    "                   cat_idxs=cat_idxs,\n",
    "                   cat_dims=cat_dims\n",
    "                   )\n",
    "    model.fit_model(X_train.values, y_train, X_val.values, y_val, criterion)\n",
    "    y_pred = model.predict(X_val)\n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_model = copy.deepcopy(model)\n",
    "        best_hyperparameters = f\"num_epochs={num_epochs}, batch_size={batch_size}, patience={patience_}, n_d={n_d}, n_indipendent={n_i}, n_shared={n_s}, n_steps={n_steps_}, gamma={gamma_}, epsilon={epsilon_}, lr={lr}, pretraining_ratio={pretraining_ratio_}, momentum={moment}\"\n",
    "        with open(f\"{filepath}/models/tabnet_model.save\", \"wb\") as f:\n",
    "            pickle.dump(best_model, f)\n",
    "    current_iter += 1"
   ],
   "id": "57199eab0d8f09f6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miaob\\PycharmProjects\\DataAnalyticsProject\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/1\n",
      "Hyperparameters: num_epochs=1000, batch_size=2048, patience=20, n_d=64, n_indipendent=1, n_shared=1, n_steps=8, gamma=1.1, epsilon=1e-15, lr=0.001, pretraining_ratio=0.5, momentum=0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miaob\\PycharmProjects\\DataAnalyticsProject\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 2.59684 | val_0_balanced_accuracy: 0.2643  | val_1_balanced_accuracy: 0.26719 |  0:01:53s\n",
      "epoch 1  | loss: 1.65057 | val_0_balanced_accuracy: 0.36308 | val_1_balanced_accuracy: 0.34844 |  0:03:46s\n",
      "epoch 2  | loss: 1.43437 | val_0_balanced_accuracy: 0.4243  | val_1_balanced_accuracy: 0.42499 |  0:05:41s\n",
      "epoch 3  | loss: 1.32295 | val_0_balanced_accuracy: 0.4958  | val_1_balanced_accuracy: 0.49579 |  0:07:40s\n",
      "epoch 4  | loss: 1.18483 | val_0_balanced_accuracy: 0.49616 | val_1_balanced_accuracy: 0.48862 |  0:09:34s\n",
      "epoch 5  | loss: 1.10826 | val_0_balanced_accuracy: 0.46841 | val_1_balanced_accuracy: 0.47944 |  0:11:24s\n",
      "epoch 6  | loss: 1.028   | val_0_balanced_accuracy: 0.5589  | val_1_balanced_accuracy: 0.55409 |  0:13:24s\n",
      "epoch 7  | loss: 0.95561 | val_0_balanced_accuracy: 0.56157 | val_1_balanced_accuracy: 0.55485 |  0:15:17s\n",
      "epoch 8  | loss: 0.8926  | val_0_balanced_accuracy: 0.58901 | val_1_balanced_accuracy: 0.58474 |  0:17:12s\n",
      "epoch 9  | loss: 0.85498 | val_0_balanced_accuracy: 0.59932 | val_1_balanced_accuracy: 0.60187 |  0:19:06s\n",
      "epoch 10 | loss: 0.82687 | val_0_balanced_accuracy: 0.54363 | val_1_balanced_accuracy: 0.53706 |  0:20:57s\n",
      "epoch 11 | loss: 0.81172 | val_0_balanced_accuracy: 0.58103 | val_1_balanced_accuracy: 0.595   |  0:22:51s\n",
      "epoch 12 | loss: 0.79278 | val_0_balanced_accuracy: 0.5819  | val_1_balanced_accuracy: 0.59054 |  0:24:47s\n",
      "epoch 13 | loss: 0.79431 | val_0_balanced_accuracy: 0.56859 | val_1_balanced_accuracy: 0.57673 |  0:26:44s\n",
      "epoch 14 | loss: 0.77163 | val_0_balanced_accuracy: 0.63245 | val_1_balanced_accuracy: 0.64563 |  0:28:52s\n",
      "epoch 15 | loss: 0.75237 | val_0_balanced_accuracy: 0.63387 | val_1_balanced_accuracy: 0.63331 |  0:30:55s\n",
      "epoch 16 | loss: 0.70958 | val_0_balanced_accuracy: 0.64954 | val_1_balanced_accuracy: 0.64044 |  0:33:04s\n",
      "epoch 17 | loss: 0.6881  | val_0_balanced_accuracy: 0.63296 | val_1_balanced_accuracy: 0.61513 |  0:34:58s\n",
      "epoch 18 | loss: 0.65858 | val_0_balanced_accuracy: 0.67416 | val_1_balanced_accuracy: 0.678   |  0:37:08s\n",
      "epoch 19 | loss: 0.63559 | val_0_balanced_accuracy: 0.60737 | val_1_balanced_accuracy: 0.60868 |  0:39:19s\n",
      "epoch 20 | loss: 0.58959 | val_0_balanced_accuracy: 0.60176 | val_1_balanced_accuracy: 0.59758 |  0:41:19s\n",
      "epoch 21 | loss: 0.55995 | val_0_balanced_accuracy: 0.69427 | val_1_balanced_accuracy: 0.69863 |  0:43:19s\n",
      "epoch 22 | loss: 0.53908 | val_0_balanced_accuracy: 0.691   | val_1_balanced_accuracy: 0.69456 |  0:45:27s\n",
      "epoch 23 | loss: 0.51365 | val_0_balanced_accuracy: 0.65542 | val_1_balanced_accuracy: 0.65323 |  0:46:47s\n",
      "epoch 24 | loss: 0.49249 | val_0_balanced_accuracy: 0.65635 | val_1_balanced_accuracy: 0.67035 |  0:48:08s\n",
      "epoch 25 | loss: 0.48875 | val_0_balanced_accuracy: 0.69932 | val_1_balanced_accuracy: 0.70706 |  0:49:45s\n",
      "epoch 26 | loss: 0.47183 | val_0_balanced_accuracy: 0.5587  | val_1_balanced_accuracy: 0.57269 |  0:51:08s\n",
      "epoch 27 | loss: 0.45451 | val_0_balanced_accuracy: 0.58065 | val_1_balanced_accuracy: 0.5826  |  0:52:47s\n",
      "epoch 28 | loss: 0.44393 | val_0_balanced_accuracy: 0.60559 | val_1_balanced_accuracy: 0.60689 |  0:54:31s\n",
      "epoch 29 | loss: 0.42889 | val_0_balanced_accuracy: 0.52937 | val_1_balanced_accuracy: 0.51708 |  0:56:23s\n",
      "epoch 30 | loss: 0.42359 | val_0_balanced_accuracy: 0.54599 | val_1_balanced_accuracy: 0.54538 |  0:58:26s\n",
      "epoch 31 | loss: 0.41838 | val_0_balanced_accuracy: 0.68837 | val_1_balanced_accuracy: 0.68269 |  1:00:26s\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## TEST",
   "id": "58901010dfdac7ef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# print(f'Best model hyperparameters: {best_hyperparameters}')\n",
    "# y_pred = best_model.predict(X_test)\n",
    "# acc = accuracy_score(y_test, y_pred)\n",
    "# print(f'Accuracy: {acc}\\n')"
   ],
   "id": "ac0adb2355e7f424"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Explainability",
   "id": "8c52965f90a0a1db"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Assuming `feature_importances` is a numpy array containing the importance values\n",
    "# and `feature_names` is a list of names corresponding to the features\n",
    "feature_importances = best_model.network.feature_importances_\n",
    "feature_names = df1.drop(columns=['type']).columns\n",
    "\n",
    "# Sort the feature importances in descending order and select the top 10\n",
    "indices = np.argsort(feature_importances)[-10:][::-1]\n",
    "top_features = feature_importances[indices]\n",
    "top_feature_names = feature_names[indices]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(top_feature_names, top_features, color='skyblue')\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.title(\"Top 10 Most Important Features\")\n",
    "plt.gca().invert_yaxis()  # To show the most important feature at the top\n",
    "plt.show()"
   ],
   "id": "3d5dc45a10ee28"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
