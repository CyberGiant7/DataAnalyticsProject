{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-15T17:10:31.977132Z",
     "start_time": "2025-01-15T17:10:31.964476Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold, PredefinedSplit\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "import itertools\n",
    "import torch\n",
    "import random\n",
    "import time\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "def fix_random(seed: int) -> None:\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True  # slower\n",
    "fix_random(SEED)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "# device = torch.device('cpu')\n",
    "\n",
    "print('Using device:', device)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load the dataset",
   "id": "ec20622f418b9959"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T17:14:00.509059Z",
     "start_time": "2025-01-15T17:13:55.019320Z"
    }
   },
   "cell_type": "code",
   "source": [
    "save_in_test_folder = True\n",
    "if save_in_test_folder:\n",
    "    filepath = \"../TestModule\"\n",
    "else:\n",
    "    filepath = \".\"\n",
    "\n",
    "seed = 42\n",
    "FILENAME = \"dataset/train_dataset.csv\"\n",
    "\n",
    "#Prepare train data\n",
    "df1 = pd.read_csv(FILENAME, sep=\",\", low_memory=False)\n",
    "\n",
    "# get features names\n",
    "features = list(df1.columns)\n",
    "features_to_remove = [\"label\", \"ts\", \"src_ip\", \"dst_ip\", \"dns_query\", \"ssl_subject\", \"ssl_issuer\", \"http_uri\", \"type\", \"http_referrer\", \"http_user_agent\"]\n",
    "features = [feature for feature in features if feature not in features_to_remove]\n",
    "df1 = df1[features + [\"type\"]]\n",
    "\n",
    "# Converte i valori in numeri, sostituendo quelli non validi con NaN\n",
    "df1[\"src_bytes\"] = pd.to_numeric(df1[\"src_bytes\"], errors='coerce')\n",
    "# Filtra le righe con NaN (valori non convertibili)\n",
    "df1 = df1.dropna(subset=[\"src_bytes\"])\n",
    "# Converte i valori rimasti in interi\n",
    "df1.loc[:, \"src_bytes\"] = df1[\"src_bytes\"].astype(int)\n",
    "\n",
    "print(\"#Righe: \" + str(df1.shape[0]) + \" #Colonne: \" + str(df1.shape[1]))\n",
    "df1 = df1.dropna()\n",
    "print(\"#Righe: \" + str(df1.shape[0]) + \" #Colonne: \" + str(df1.shape[1]))\n",
    "\n",
    "X = df1[features]\n",
    "y = df1[\"type\"]\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y)\n",
    "with open(f\"{filepath}/transformer/target_encoder.save\", \"wb\") as f:\n",
    "    pickle.dump(le, f)\n",
    "\n",
    "y = le.transform(y)\n",
    "\n",
    "indices = np.arange(X.shape[0])\n",
    "train_idx, val_idx = train_test_split(indices, test_size=0.2, stratify=y, random_state=seed)\n",
    "\n",
    "# fold = np.zeros(X.shape[0])\n",
    "# fold[train_idx] = -1\n",
    "\n",
    "n_splits = 10\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "fold = np.full(len(y), -1)  # Inizializza tutto con -1 (default: train)\n",
    "\n",
    "# Assegna i fold ai campioni\n",
    "for fold_number, (_, val_idx) in enumerate(skf.split(X, y)):\n",
    "    fold[val_idx] = fold_number  # Assegna il numero del fold ai campioni di validazione\n",
    "\n",
    "ps = PredefinedSplit(fold)\n",
    "ps.get_n_splits()\n",
    "\n",
    "# for i, (train_index, test_index) in enumerate(ps.split()):\n",
    "#     print(f\"Fold {i}:\")\n",
    "#     print(f\"  Train: index={train_index}\")\n",
    "#     print(f\"  Test:  index={test_index}\")\n",
    "\n",
    "# take only x with index in val_idx\n",
    "X_val = X.iloc[val_idx]\n",
    "y_val = y[val_idx]\n",
    "X_train = X.iloc[train_idx]\n",
    "y_train = y[train_idx]"
   ],
   "id": "7d988470b826232a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Righe: 616983 #Colonne: 36\n",
      "#Righe: 616983 #Colonne: 36\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Preprocessing",
   "id": "3e1a8dccdb68878f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T17:16:07.607242Z",
     "start_time": "2025-01-15T17:16:00.355415Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "import pickle\n",
    "\n",
    "categorical_columns = X_train.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "numeric_columns = X_train.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "# boolean_columns = X_train.select_dtypes(include=[\"bool\"]).columns.tolist()\n",
    "\n",
    "ct = ColumnTransformer(\n",
    "    [\n",
    "        (\"cat\", OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1), categorical_columns),  # Trasforma le colonne categoriche\n",
    "        # (\"ordinal\", OneHotEncoder(handle_unknown='infrequent_if_exist', sparse_output=False), categorical_columns),  # Trasforma le colonne categoriche\n",
    "        (\"scale\", StandardScaler(), numeric_columns)  # Normalizza le colonne numeriche\n",
    "    ],\n",
    "    remainder=\"passthrough\"  # Mantieni le altre colonne invariate\n",
    ")\n",
    "ct.set_output(transform=\"pandas\")\n",
    "\n",
    "ct = ct.fit(X_train)\n",
    "with open(f\"{filepath}/transformer/transformer.save\", \"wb\") as f:\n",
    "    pickle.dump(ct, f)\n",
    "\n",
    "# train set\n",
    "X_train = ct.transform(X_train)\n",
    "\n",
    "# validation set\n",
    "X_val = ct.transform(X_val)\n",
    "\n",
    "# X\n",
    "X = ct.transform(X)"
   ],
   "id": "ade119398b7aa630",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### DEFINE WEIGHTS FOR UNBALANCED CLASSES",
   "id": "a7b7e07bd23b4deb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T17:10:50.063289Z",
     "start_time": "2025-01-15T17:10:49.972027Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "print(class_weights)"
   ],
   "id": "c102ea88b85aa69d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 4.069469865611345, 1: 0.3381003918130257, 2: 1.132545546326465, 3: 4.543735616312253, 4: 98.7172, 5: 2.9863625363020327, 6: 1.1966881637007225, 7: 63.85329883570505, 8: 0.28803534018428717, 9: 0.9751965859248429}\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# TabNet",
   "id": "44953d4fd06d388"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T17:11:22.098810Z",
     "start_time": "2025-01-15T17:11:22.054305Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pytorch_tabnet.tab_model import TabNetClassifier, TabNetRegressor\n",
    "from pytorch_tabnet.pretraining import TabNetPretrainer\n",
    "\n",
    "class TabNet(torch.nn.Module):\n",
    "    '''\n",
    "    Wrapper class for TabNetClassifier\n",
    "    '''\n",
    "    def __init__(self, n_d,\n",
    "                 n_a,\n",
    "                 n_steps,\n",
    "                 gamma,\n",
    "                 optimizer_fn,\n",
    "                 n_independent,\n",
    "                 n_shared,\n",
    "                 epsilon,\n",
    "                 seed,\n",
    "                 lambda_sparse,\n",
    "                 clip_value,\n",
    "                 momentum,\n",
    "                 optimizer_params,\n",
    "                 scheduler_params,\n",
    "                 mask_type,\n",
    "                 scheduler_fn,\n",
    "                 device_name,\n",
    "                 output_dim,\n",
    "                 batch_size,\n",
    "                 num_epochs,\n",
    "                 unsupervised_model,\n",
    "                 verbose=0):\n",
    "        super(TabNet, self).__init__()\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.num_epochs = num_epochs\n",
    "        self.unsupervised_model = unsupervised_model\n",
    "        self.network = TabNetClassifier(n_d=n_d,\n",
    "                                        n_a=n_a,\n",
    "                                        n_steps=n_steps,\n",
    "                                        gamma=gamma,\n",
    "                                        optimizer_fn=optimizer_fn,\n",
    "                                        n_independent=n_independent,\n",
    "                                        n_shared=n_shared,\n",
    "                                        epsilon=epsilon,\n",
    "                                        seed=seed,\n",
    "                                        lambda_sparse=lambda_sparse,\n",
    "                                        clip_value=clip_value,\n",
    "                                        momentum=momentum,\n",
    "                                        optimizer_params=optimizer_params,\n",
    "                                        scheduler_params=scheduler_params,\n",
    "                                        mask_type=mask_type,\n",
    "                                        scheduler_fn=scheduler_fn,\n",
    "                                        device_name=device_name,\n",
    "                                        output_dim=output_dim,\n",
    "                                        verbose=verbose)\n",
    "\n",
    "    def fit_model(self, X_train, y_train, X_val, y_val, criterion):\n",
    "        self.network.fit(X_train=X_train,\n",
    "                         y_train=y_train,\n",
    "                         eval_set=[(X_train,y_train),(X_val, y_val)],\n",
    "                         eval_metric=['accuracy'],\n",
    "                         patience=10,\n",
    "                         batch_size=self.batch_size,\n",
    "                         virtual_batch_size=128,\n",
    "                         num_workers=0,\n",
    "                         drop_last=True,\n",
    "                         max_epochs=self.num_epochs,\n",
    "                         loss_fn=criterion,\n",
    "                         from_unsupervised=self.unsupervised_model)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.network.predict(X)\n",
    "\n",
    "    def explain(self, X):\n",
    "        return self.network.explain(X)\n",
    "\n",
    "    def feature_importances(self):\n",
    "        return self.network.feature_importances_\n",
    "\n",
    "def get_unsupervised_model(n_d_a,n_step,n_independent,n_shared,gamma,lr):\n",
    "    tabnet_params = dict(n_d=n_d_a,\n",
    "                        n_a=n_d_a,\n",
    "                        n_steps=n_step,\n",
    "                        gamma=gamma,\n",
    "                        n_independent=n_independent,\n",
    "                        n_shared=n_shared,\n",
    "                        lambda_sparse=1e-3,\n",
    "                        optimizer_fn=torch.optim.AdamW,\n",
    "                        optimizer_params=dict(lr=lr),\n",
    "                        mask_type=\"sparsemax\",\n",
    "                        verbose=0\n",
    "                        )\n",
    "    unsupervised_model = TabNetPretrainer(**tabnet_params)\n",
    "    return unsupervised_model"
   ],
   "id": "eae20e8645f9b130",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### HYPERPARAMETERS CONFIGURATION",
   "id": "c308cb5ac8d1a397"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T17:11:26.790469Z",
     "start_time": "2025-01-15T17:11:26.783448Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nums_epochs = [1000]\n",
    "batch_sizes = [32]\n",
    "patience = [20]\n",
    "n_d_a = [64] # 8, 16, 32, 64\n",
    "n_shared = [1] # 1, 2, 3, 4, 5\n",
    "n_indipendents = [1] # 1, 2, 3, 4, 5\n",
    "n_steps = [8] # 3, 4, 5, 6, 7, 8, 9, 10\n",
    "gamma = [1.1] # 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0\n",
    "epsilon = [1e-15]\n",
    "learning_rate = [0.02]\n",
    "pretraining_ratio = [0.5]\n",
    "momentum = [0.99]\n",
    "hyperparameters = list(itertools.product(nums_epochs, batch_sizes, patience, n_d_a, n_indipendents, n_shared, n_steps, gamma, epsilon, learning_rate, pretraining_ratio, momentum))\n",
    "n_comb = len(hyperparameters)\n",
    "print (f'Number of hyperparameter combinations: {n_comb}')"
   ],
   "id": "7aa9bb2d5efecc44",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of hyperparameter combinations: 1\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training",
   "id": "bcf2b4b95e934fbc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T17:12:58.816614Z",
     "start_time": "2025-01-15T17:12:58.501928Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_train = X_train.to_numpy()\n",
    "X_val = X_val.to_numpy()"
   ],
   "id": "1872ac9bfcda8751",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-01-15T17:16:53.292420Z"
    }
   },
   "cell_type": "code",
   "source": [
    "current_iter = 0\n",
    "best_acc = 0\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=torch.tensor(list(class_weights.values()), dtype=torch.float32).to(device))\n",
    "for num_epochs, batch_size, patience_, n_d, n_i, n_s, n_steps_, gamma_, epsilon_, lr, pretraining_ratio_, moment in hyperparameters:\n",
    "\n",
    "    print(f'Iteration {current_iter+1}/{n_comb}')\n",
    "    print(f'Hyperparameters: num_epochs={num_epochs}, batch_size={batch_size}, patience={patience_}, n_d={n_d}, n_indipendent={n_i}, n_shared={n_s}, n_steps={n_steps_}, gamma={gamma_}, epsilon={epsilon_}, lr={lr}, pretraining_ratio={pretraining_ratio_}, momentum={moment}')\n",
    "\n",
    "    unsupervised_model = get_unsupervised_model(n_d, n_steps_, n_i, n_s, gamma_, lr)\n",
    "\n",
    "    unsupervised_model.fit(\n",
    "        X_train=X_train.values,\n",
    "        eval_set=[X_val.values],\n",
    "        max_epochs=num_epochs,\n",
    "        patience=patience_,\n",
    "        batch_size=batch_size,\n",
    "        virtual_batch_size=128,\n",
    "        num_workers=0,\n",
    "        drop_last=False,\n",
    "        pretraining_ratio=pretraining_ratio_,\n",
    "    )\n",
    "\n",
    "    model = TabNet(n_d=n_d,\n",
    "                   n_a=n_d,\n",
    "                   n_steps=n_steps_,\n",
    "                   gamma=gamma_,\n",
    "                   optimizer_fn=torch.optim.AdamW,\n",
    "                   n_independent=n_i,\n",
    "                   n_shared=n_s,\n",
    "                   epsilon=epsilon_,\n",
    "                   seed=SEED,\n",
    "                   lambda_sparse=1e-4,\n",
    "                   clip_value=1,\n",
    "                   momentum=moment,\n",
    "                   optimizer_params=dict(lr=lr),\n",
    "                   scheduler_params=dict(step_size=10, gamma=0.9),\n",
    "                   mask_type='sparsemax',\n",
    "                   scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "                   device_name=device,\n",
    "                   output_dim=len(np.unique(y_train)),\n",
    "                   batch_size=batch_size,\n",
    "                   num_epochs=num_epochs,\n",
    "                   unsupervised_model=None,\n",
    "                   verbose=0)\n",
    "    model.fit_model(X_train, y_train, X_val, y_val, criterion)\n",
    "    y_pred = model.predict(X_val)\n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_model = copy.deepcopy(model)\n",
    "        best_hyperparameters = f\"num_epochs={num_epochs}, batch_size={batch_size}, patience={patience_}, n_d={n_d}, n_indipendent={n_i}, n_shared={n_s}, n_steps={n_steps_}, gamma={gamma_}, epsilon={epsilon_}, lr={lr}, pretraining_ratio={pretraining_ratio_}, momentum={moment}\"\n",
    "    current_iter += 1"
   ],
   "id": "57199eab0d8f09f6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/1\n",
      "Hyperparameters: num_epochs=1000, batch_size=32, patience=20, n_d=64, n_indipendent=1, n_shared=1, n_steps=8, gamma=1.1, epsilon=1e-15, lr=0.02, pretraining_ratio=0.5, momentum=0.99\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## TEST",
   "id": "58901010dfdac7ef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# print(f'Best model hyperparameters: {best_hyperparameters}')\n",
    "# y_pred = best_model.predict(X_test)\n",
    "# acc = accuracy_score(y_test, y_pred)\n",
    "# print(f'Accuracy: {acc}\\n')"
   ],
   "id": "ac0adb2355e7f424"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Explainability",
   "id": "8c52965f90a0a1db"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Assuming `feature_importances` is a numpy array containing the importance values\n",
    "# and `feature_names` is a list of names corresponding to the features\n",
    "feature_importances = best_model.network.feature_importances_\n",
    "feature_names = df1.drop(columns=[target]).columns\n",
    "\n",
    "# Sort the feature importances in descending order and select the top 10\n",
    "indices = np.argsort(feature_importances)[-10:][::-1]\n",
    "top_features = feature_importances[indices]\n",
    "top_feature_names = feature_names[indices]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(top_feature_names, top_features, color='skyblue')\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.title(\"Top 10 Most Important Features\")\n",
    "plt.gca().invert_yaxis()  # To show the most important feature at the top\n",
    "plt.show()"
   ],
   "id": "3d5dc45a10ee28"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
