{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "203010d6",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95adbe32",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-03T14:47:00.591443Z",
     "start_time": "2025-01-03T14:46:55.551693Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Righe: 616983 #Colonne: 36\n",
      "#Righe: 616983 #Colonne: 36\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from sklearn import preprocessing\n",
    "\n",
    "save_in_test_folder = True\n",
    "if save_in_test_folder:\n",
    "    filepath = \"../TestModule\"\n",
    "else:\n",
    "    filepath = \".\"\n",
    "\n",
    "seed = 42\n",
    "FILENAME = \"dataset/train_dataset.csv\"\n",
    "\n",
    "#Prepare train data\n",
    "df1 = pd.read_csv(FILENAME, sep=\",\", low_memory=False)\n",
    "\n",
    "# get features names\n",
    "features = list(df1.columns)\n",
    "features_to_remove = [\"label\", \"ts\", \"src_ip\", \"dst_ip\", \"dns_query\", \"ssl_subject\", \"ssl_issuer\", \"http_uri\", \"type\", \"http_referrer\", \"http_user_agent\"]\n",
    "features = [feature for feature in features if feature not in features_to_remove]\n",
    "df1 = df1[features + [\"type\"]]\n",
    "\n",
    "# Converte i valori in numeri, sostituendo quelli non validi con NaN\n",
    "df1[\"src_bytes\"] = pd.to_numeric(df1[\"src_bytes\"], errors='coerce')\n",
    "# Filtra le righe con NaN (valori non convertibili)\n",
    "df1 = df1.dropna(subset=[\"src_bytes\"])\n",
    "# Converte i valori rimasti in interi\n",
    "df1.loc[:, \"src_bytes\"] = df1[\"src_bytes\"].astype(int)\n",
    "\n",
    "print(\"#Righe: \" + str(df1.shape[0]) + \" #Colonne: \" + str(df1.shape[1]))\n",
    "df1 = df1.dropna()\n",
    "print(\"#Righe: \" + str(df1.shape[0]) + \" #Colonne: \" + str(df1.shape[1]))\n",
    "\n",
    "X = df1[features]\n",
    "y = df1[\"type\"]\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y)\n",
    "with open(f\"{filepath}/transformer/target_encoder.save\", \"wb\") as f:\n",
    "    pickle.dump(le, f)\n",
    "\n",
    "y = le.transform(y)\n",
    "\n",
    "indices = np.arange(X.shape[0])\n",
    "train_idx, val_idx = train_test_split(indices, test_size=0.2, stratify=y, random_state=seed)\n",
    "\n",
    "# fold = np.zeros(X.shape[0])\n",
    "# fold[train_idx] = -1\n",
    "\n",
    "n_splits = 10\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "fold = np.full(len(y), -1)  # Inizializza tutto con -1 (default: train)\n",
    "\n",
    "# Assegna i fold ai campioni\n",
    "for fold_number, (_, val_idx) in enumerate(skf.split(X, y)):\n",
    "    fold[val_idx] = fold_number  # Assegna il numero del fold ai campioni di validazione\n",
    "\n",
    "\n",
    "ps = PredefinedSplit(fold)\n",
    "ps.get_n_splits()\n",
    "\n",
    "\n",
    "# for i, (train_index, test_index) in enumerate(ps.split()):\n",
    "#     print(f\"Fold {i}:\")\n",
    "#     print(f\"  Train: index={train_index}\")\n",
    "#     print(f\"  Test:  index={test_index}\")\n",
    "\n",
    "# take only x with index in val_idx\n",
    "X_val = X.iloc[val_idx]\n",
    "y_val = y[val_idx]\n",
    "X_train = X.iloc[train_idx]\n",
    "y_train = y[train_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5bc153025b9757",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-03T10:56:22.425329Z",
     "start_time": "2025-01-03T10:56:22.187322Z"
    }
   },
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5cee74",
   "metadata": {},
   "source": [
    "## Preprocess the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "adbe4df5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-03T14:47:09.222767Z",
     "start_time": "2025-01-03T14:47:03.418602Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder, OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "\n",
    "import pickle\n",
    "\n",
    "categorical_columns = X_train.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "numeric_columns = X_train.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "# boolean_columns = X_train.select_dtypes(include=[\"bool\"]).columns.tolist()\n",
    "\n",
    "ct = ColumnTransformer(\n",
    "    [\n",
    "        (\"cat\", OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1), categorical_columns),  # Trasforma le colonne categoriche\n",
    "        # (\"ordinal\", OneHotEncoder(handle_unknown='infrequent_if_exist', sparse_output=False), categorical_columns),  # Trasforma le colonne categoriche\n",
    "        (\"scale\", StandardScaler(), numeric_columns)  # Normalizza le colonne numeriche\n",
    "    ],\n",
    "    remainder=\"passthrough\"  # Mantieni le altre colonne invariate\n",
    ")\n",
    "ct.set_output(transform=\"pandas\")\n",
    "\n",
    "ct = ct.fit(X_train)\n",
    "with open(f\"{filepath}/transformer/transformer.save\", \"wb\") as f:\n",
    "    pickle.dump(ct, f)    \n",
    "\n",
    "# train set\n",
    "X_train = ct.transform(X_train)\n",
    "\n",
    "# validation set\n",
    "X_val = ct.transform(X_val)\n",
    "\n",
    "# X\n",
    "X = ct.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "988857df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf = RandomForestClassifier(n_estimators=3, random_state=seed)\n",
    "# sfs = SequentialFeatureSelector(estimator=rf, direction=\"backward\", n_features_to_select=\"auto\", scoring=\"balanced_accuracy\")\n",
    "\n",
    "# sfs.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# rename \"scale__\" to all numeric columns in x_train\n",
    "X_train.columns = [col.replace(\"remainder__\", \"scale__\") for col in X_train.columns]\n",
    "X_val.columns = [col.replace(\"remainder__\", \"scale__\") for col in X_val.columns]\n",
    "X.columns = [col.replace(\"remainder__\", \"scale__\") for col in X.columns]\n",
    "\n",
    "\n",
    "with open(\"../TestModule/transformer/sfs.save\", \"rb\") as f:\n",
    "    sfs: SequentialFeatureSelector = pickle.load(f)\n",
    "\n",
    "# train set\n",
    "X_train = sfs.transform(X_train)\n",
    "\n",
    "# validation set\n",
    "X_val = sfs.transform(X_val)\n",
    "\n",
    "# X\n",
    "X = sfs.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42673c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sfs.get_feature_names_out())\n",
    "print(sfs.get_support())\n",
    "with open(f\"{filepath}/transformer/sfs.save\", \"wb\") as f:\n",
    "    pickle.dump(sfs, f)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef57a44f3885f1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "from sklearn import discriminant_analysis\n",
    "\n",
    "# pca = decomposition.PCA(n_components=0.95, random_state=seed)\n",
    "pca = discriminant_analysis.LinearDiscriminantAnalysis(n_components=9)\n",
    "\n",
    "pca.set_output(transform=\"pandas\")\n",
    "\n",
    "pca.fit(X_train, y_train)\n",
    "with open(f\"{filepath}/transformer/pca.save\", \"wb\") as f:\n",
    "    pickle.dump(pca, f)\n",
    "\n",
    "X_train_pca = pca.transform(X_train)\n",
    "X_val_pca = pca.transform(X_val)\n",
    "X_pca = pca.transform(X)\n",
    "\n",
    "# X_train = X_train_pca\n",
    "# X_val = X_val_pca\n",
    "# X = X_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261bb308",
   "metadata": {},
   "source": [
    "## Apply K-Nearest Neighbour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a123521",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-03T11:29:10.264619300Z",
     "start_time": "2025-01-03T11:24:35.562167Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 17\u001b[0m\n\u001b[1;32m      8\u001b[0m param_grid \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_neighbors\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m9\u001b[39m, \u001b[38;5;241m11\u001b[39m],  \u001b[38;5;66;03m# Number of neighbors to consider\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muniform\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistance\u001b[39m\u001b[38;5;124m'\u001b[39m],  \u001b[38;5;66;03m# Weight function\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124malgorithm\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mball_tree\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkd_tree\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbrute\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# Algorithm to compute neighbors\u001b[39;00m\n\u001b[1;32m     14\u001b[0m }\n\u001b[1;32m     16\u001b[0m grid \u001b[38;5;241m=\u001b[39m RandomizedSearchCV(KNeighborsClassifier(), param_grid, cv\u001b[38;5;241m=\u001b[39mps, n_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39mseed)\n\u001b[0;32m---> 17\u001b[0m \u001b[43mgrid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest parameters:\u001b[39m\u001b[38;5;124m\"\u001b[39m, grid\u001b[38;5;241m.\u001b[39mbest_params_)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest score:\u001b[39m\u001b[38;5;124m\"\u001b[39m, grid\u001b[38;5;241m.\u001b[39mbest_score_)\n",
      "File \u001b[0;32m/scratch.hpc/leonardo.dessi/DataAnalyticsProject/.venv/lib/python3.11/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch.hpc/leonardo.dessi/DataAnalyticsProject/.venv/lib/python3.11/site-packages/sklearn/model_selection/_search.py:1023\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1017\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m   1018\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m   1019\u001b[0m     )\n\u001b[1;32m   1021\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m-> 1023\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m   1027\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/scratch.hpc/leonardo.dessi/DataAnalyticsProject/.venv/lib/python3.11/site-packages/sklearn/model_selection/_search.py:1950\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1948\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1949\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1950\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1951\u001b[0m \u001b[43m        \u001b[49m\u001b[43mParameterSampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1952\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_distributions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\n\u001b[1;32m   1953\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1954\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch.hpc/leonardo.dessi/DataAnalyticsProject/.venv/lib/python3.11/site-packages/sklearn/model_selection/_search.py:969\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    961\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    962\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    963\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    964\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    965\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    966\u001b[0m         )\n\u001b[1;32m    967\u001b[0m     )\n\u001b[0;32m--> 969\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    985\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    987\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    988\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    989\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    990\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    991\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    992\u001b[0m     )\n",
      "File \u001b[0;32m/scratch.hpc/leonardo.dessi/DataAnalyticsProject/.venv/lib/python3.11/site-packages/sklearn/utils/parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     76\u001b[0m )\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch.hpc/leonardo.dessi/DataAnalyticsProject/.venv/lib/python3.11/site-packages/joblib/parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m/scratch.hpc/leonardo.dessi/DataAnalyticsProject/.venv/lib/python3.11/site-packages/joblib/parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m/scratch.hpc/leonardo.dessi/DataAnalyticsProject/.venv/lib/python3.11/site-packages/sklearn/utils/parallel.py:139\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch.hpc/leonardo.dessi/DataAnalyticsProject/.venv/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:888\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    885\u001b[0m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_error\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    887\u001b[0m fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[0;32m--> 888\u001b[0m test_scores \u001b[38;5;241m=\u001b[39m \u001b[43m_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscore_params_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_score\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    891\u001b[0m score_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time \u001b[38;5;241m-\u001b[39m fit_time\n\u001b[1;32m    892\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_train_score:\n",
      "File \u001b[0;32m/scratch.hpc/leonardo.dessi/DataAnalyticsProject/.venv/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:949\u001b[0m, in \u001b[0;36m_score\u001b[0;34m(estimator, X_test, y_test, scorer, score_params, error_score)\u001b[0m\n\u001b[1;32m    947\u001b[0m         scores \u001b[38;5;241m=\u001b[39m scorer(estimator, X_test, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mscore_params)\n\u001b[1;32m    948\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 949\u001b[0m         scores \u001b[38;5;241m=\u001b[39m \u001b[43mscorer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mscore_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    951\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(scorer, _MultimetricScorer):\n\u001b[1;32m    952\u001b[0m         \u001b[38;5;66;03m# If `_MultimetricScorer` raises exception, the `error_score`\u001b[39;00m\n\u001b[1;32m    953\u001b[0m         \u001b[38;5;66;03m# parameter is equal to \"raise\".\u001b[39;00m\n",
      "File \u001b[0;32m/scratch.hpc/leonardo.dessi/DataAnalyticsProject/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py:472\u001b[0m, in \u001b[0;36m_PassthroughScorer.__call__\u001b[0;34m(self, estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    471\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Method that wraps estimator.score\"\"\"\u001b[39;00m\n\u001b[0;32m--> 472\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch.hpc/leonardo.dessi/DataAnalyticsProject/.venv/lib/python3.11/site-packages/sklearn/neighbors/_classification.py:446\u001b[0m, in \u001b[0;36mKNeighborsClassifier.score\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mscore\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    418\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;124;03m    Return the mean accuracy on the given test data and labels.\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;124;03m        Mean accuracy of ``self.predict(X)`` w.r.t. `y`.\u001b[39;00m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 446\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch.hpc/leonardo.dessi/DataAnalyticsProject/.venv/lib/python3.11/site-packages/sklearn/base.py:572\u001b[0m, in \u001b[0;36mClassifierMixin.score\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;124;03mReturn the mean accuracy on the given test data and labels.\u001b[39;00m\n\u001b[1;32m    549\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;124;03m    Mean accuracy of ``self.predict(X)`` w.r.t. `y`.\u001b[39;00m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    570\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m accuracy_score\n\u001b[0;32m--> 572\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m accuracy_score(y, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m, sample_weight\u001b[38;5;241m=\u001b[39msample_weight)\n",
      "File \u001b[0;32m/scratch.hpc/leonardo.dessi/DataAnalyticsProject/.venv/lib/python3.11/site-packages/sklearn/neighbors/_classification.py:274\u001b[0m, in \u001b[0;36mKNeighborsClassifier.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_[np\u001b[38;5;241m.\u001b[39margmax(probabilities, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)]\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;66;03m# In that case, we do not need the distances to perform\u001b[39;00m\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;66;03m# the weighting so we do not compute them.\u001b[39;00m\n\u001b[0;32m--> 274\u001b[0m     neigh_ind \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkneighbors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    275\u001b[0m     neigh_dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/scratch.hpc/leonardo.dessi/DataAnalyticsProject/.venv/lib/python3.11/site-packages/sklearn/neighbors/_base.py:922\u001b[0m, in \u001b[0;36mKNeighborsMixin.kneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    917\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m issparse(X):\n\u001b[1;32m    918\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    919\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m does not work with sparse matrices. Densify the data, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    920\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor set algorithm=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbrute\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_method\n\u001b[1;32m    921\u001b[0m         )\n\u001b[0;32m--> 922\u001b[0m     chunked_results \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_neighbors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgen_even_slices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    927\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minternal: _fit_method not recognized\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/scratch.hpc/leonardo.dessi/DataAnalyticsProject/.venv/lib/python3.11/site-packages/sklearn/utils/parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     76\u001b[0m )\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch.hpc/leonardo.dessi/DataAnalyticsProject/.venv/lib/python3.11/site-packages/joblib/parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m/scratch.hpc/leonardo.dessi/DataAnalyticsProject/.venv/lib/python3.11/site-packages/joblib/parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m/scratch.hpc/leonardo.dessi/DataAnalyticsProject/.venv/lib/python3.11/site-packages/sklearn/utils/parallel.py:139\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import os\n",
    "\n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_neighbors': [3, 5, 7, 9, 11],  # Number of neighbors to consider\n",
    "    'weights': ['uniform', 'distance'],  # Weight function\n",
    "    'metric': ['euclidean', 'manhattan', 'minkowski'],  # Distance metrics\n",
    "    'p': [1, 2],  # Minkowski parameter (1 for Manhattan, 2 for Euclidean)\n",
    "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']  # Algorithm to compute neighbors\n",
    "}\n",
    "\n",
    "grid = RandomizedSearchCV(KNeighborsClassifier(), param_grid, cv=ps, n_iter=2, random_state=seed)\n",
    "grid.fit(X, y)\n",
    "\n",
    "print(\"Best parameters:\", grid.best_params_)\n",
    "print(\"Best score:\", grid.best_score_)\n",
    "best_knn = grid.best_estimator_\n",
    "with open(f\"{filepath}/models/knn.save\", \"wb\") as file:\n",
    "    pickle.dump(best_knn, file)\n",
    "\n",
    "pd.DataFrame(grid.cv_results_)\n",
    "\n",
    "\n",
    "# knn = KNeighborsClassifier().fit(X_train, y_train)\n",
    "# print(\"Performance:\", knn.score(X_val, y_val))\n",
    "# with open( f\"{filepath}/models/knn.save\", \"wb\") as file:\n",
    "#     pickle.dump(knn, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcdf539f66957d6",
   "metadata": {},
   "source": [
    "- Performance: 0.9187280941672238 con minmax\n",
    "- Performance: 0.9549626207986386 senza minmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bb0d395d0af24e",
   "metadata": {},
   "source": [
    "## Apply Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecffa88485239bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-03T11:30:44.469531Z",
     "start_time": "2025-01-03T11:30:44.132069Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "[CV] END bootstrap=True, criterion=log_loss, max_depth=50, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time= 1.6min\n",
      "[CV] END bootstrap=True, criterion=log_loss, max_depth=50, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time= 1.7min\n",
      "[CV] END bootstrap=True, criterion=log_loss, max_depth=50, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time= 1.6min\n",
      "[CV] END bootstrap=True, criterion=log_loss, max_depth=50, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time= 1.6min\n",
      "[CV] END bootstrap=True, criterion=log_loss, max_depth=50, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time= 1.6min\n",
      "[CV] END bootstrap=True, criterion=log_loss, max_depth=50, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time= 1.7min\n",
      "[CV] END bootstrap=True, criterion=log_loss, max_depth=50, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time= 1.6min\n",
      "[CV] END bootstrap=True, criterion=log_loss, max_depth=50, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time= 1.6min\n",
      "[CV] END bootstrap=True, criterion=log_loss, max_depth=50, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time= 1.6min\n",
      "[CV] END bootstrap=True, criterion=log_loss, max_depth=50, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time= 1.6min\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "[CV] END bootstrap=False, criterion=log_loss, max_depth=10, max_features=None, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time= 3.6min\n",
      "[CV] END bootstrap=False, criterion=log_loss, max_depth=10, max_features=None, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time= 3.6min\n",
      "[CV] END bootstrap=False, criterion=log_loss, max_depth=10, max_features=None, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time= 3.5min\n",
      "[CV] END bootstrap=False, criterion=log_loss, max_depth=10, max_features=None, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time= 3.5min\n",
      "[CV] END bootstrap=False, criterion=log_loss, max_depth=10, max_features=None, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time= 3.5min\n",
      "[CV] END bootstrap=False, criterion=log_loss, max_depth=10, max_features=None, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time= 3.5min\n",
      "[CV] END bootstrap=False, criterion=log_loss, max_depth=10, max_features=None, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time= 3.6min\n",
      "[CV] END bootstrap=False, criterion=log_loss, max_depth=10, max_features=None, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time= 3.6min\n",
      "[CV] END bootstrap=False, criterion=log_loss, max_depth=10, max_features=None, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time= 3.5min\n",
      "[CV] END bootstrap=False, criterion=log_loss, max_depth=10, max_features=None, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time= 3.6min\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "[CV] END bootstrap=True, criterion=log_loss, max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=  53.2s\n",
      "[CV] END bootstrap=True, criterion=log_loss, max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=  53.6s\n",
      "[CV] END bootstrap=True, criterion=log_loss, max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=  53.3s\n",
      "[CV] END bootstrap=True, criterion=log_loss, max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=  53.3s\n",
      "[CV] END bootstrap=True, criterion=log_loss, max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=  57.6s\n",
      "[CV] END bootstrap=True, criterion=log_loss, max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=  57.6s\n",
      "[CV] END bootstrap=True, criterion=log_loss, max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=  53.6s\n",
      "[CV] END bootstrap=True, criterion=log_loss, max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=  54.1s\n",
      "[CV] END bootstrap=True, criterion=log_loss, max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=  53.7s\n",
      "[CV] END bootstrap=True, criterion=log_loss, max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=  55.0s\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import balanced_accuracy_score, f1_score, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from skopt import BayesSearchCV \n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 500],  # Number of trees in the forest\n",
    "    'max_depth': [None, 10, 20, 30, 50],  # Maximum depth of the tree\n",
    "    'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split a node\n",
    "    'min_samples_leaf': [1, 2, 4],  # Minimum number of samples required at a leaf node\n",
    "    'max_features': ['sqrt', 'log2', None],  # Number of features to consider when looking for the best split\n",
    "    'bootstrap': [True, False],  # Whether bootstrap samples are used when building trees\n",
    "    'criterion': ['gini', 'entropy', 'log_loss'],  # Split quality measure\n",
    "    # 'class_weight': ['balanced', 'balanced_subsample', None]  # Weights associated with classes\n",
    "}\n",
    "\n",
    "grid = BayesSearchCV(RandomForestClassifier(random_state=seed, class_weight='balanced'), param_grid, cv=ps, n_iter=10, random_state=seed, verbose=2, n_jobs=2, scoring=\"balanced_accuracy\")\n",
    "grid.fit(X, y)\n",
    "\n",
    "print(\"Best parameters:\", grid.best_params_)\n",
    "print(\"Best score:\", grid.best_score_)\n",
    "best_rf = grid.best_estimator_\n",
    "with open(f\"{filepath}/models/rf.save\", \"wb\") as file:\n",
    "    pickle.dump(best_rf, file)\n",
    "\n",
    "pd.DataFrame(grid.cv_results_)\n",
    "\n",
    "# rf = RandomForestClassifier(random_state=seed, class_weight=\"balanced\").fit(X_train, y_train)\n",
    "# y_pred = rf.predict(X_val)\n",
    "# print(\"Accuracy:\", accuracy_score(y_val, y_pred))\n",
    "# print(\"Balanced accuracy:\", balanced_accuracy_score(y_val, y_pred))\n",
    "# print(\"F1 score:\", f1_score(y_val, y_pred, average=\"weighted\"))\n",
    "\n",
    "# with open(f\"{filepath}/models/rf.save\", \"wb\") as file:\n",
    "#     pickle.dump(rf, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521b7fab",
   "metadata": {},
   "source": [
    "0.9989123986553292 senza scaling\n",
    "\n",
    "0.9990112715048448 con scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d48c37",
   "metadata": {},
   "source": [
    "## Apply Support Vector Classifier with HP tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5506638",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-03T14:47:41.422787Z",
     "start_time": "2025-01-03T14:47:28.306591Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "param_grid = {'C': [0.1, 1, 10, 100, 1000],\n",
    "              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "              'kernel': ['rbf']}\n",
    "\n",
    "# param_grid = {\n",
    "#     'C': (0.1, 100, 'log-uniform'),\n",
    "#     'gamma': (0.001, 10, 'log-uniform'),\n",
    "#     'kernel': [\"rbf\"]\n",
    "# }\n",
    "\n",
    "grid = BayesSearchCV(\n",
    "    SVC(random_state=seed),\n",
    "    param_grid,\n",
    "    n_iter=1,  # Numero massimo di iterazioni\n",
    "    cv=ps,\n",
    "    verbose=1,\n",
    "    random_state=seed,\n",
    "    n_jobs=6\n",
    ")\n",
    "grid.fit(X, y)\n",
    "\n",
    "# grid = RandomizedSearchCV(SVC(random_state=seed), param_grid, cv=ps)\n",
    "# grid.fit(X, y)\n",
    "#\n",
    "print(\"Best parameters:\", grid.best_params_)\n",
    "print(\"Best score:\", grid.best_score_)\n",
    "best_svm = grid.best_estimator_\n",
    "with open(f\"{filepath}/models/svm.save\", \"wb\") as file:\n",
    "    pickle.dump(best_svm, file)\n",
    "\n",
    "pd.DataFrame(grid.cv_results_)\n",
    "\n",
    "# svm = SVC(random_state=seed, class_weight=\"balanced\", verbose=1, C=100).fit(X_train.head(10000), y_train[:10000])\n",
    "# print(\"Performance:\", svm.score(X_val, y_val))\n",
    "# with open(f\"{filepath}/models/svm.save\", \"wb\") as file:\n",
    "#     pickle.dump(svm, file)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
